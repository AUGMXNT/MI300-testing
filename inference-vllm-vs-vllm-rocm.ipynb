{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ba054c-a0a7-4c65-b250-e0ebc16dd35f",
   "metadata": {},
   "source": [
    "# vLLM Upstream vs vLLM ROCm\n",
    "AMD maintains a vLLM fork: https://github.com/ROCm/vllm\n",
    "Is this any better than the upstream https://github.com/vllm-project/vllm ?\n",
    "\n",
    "```\n",
    "# Clone\n",
    "mamba create --name vllm-rocm --clone vllm\n",
    "mamba activate vllm-rocm\n",
    "python -m ipykernel install --user --name vllm-rocm\n",
    "\n",
    "# Build (leave everything else the same as our previous setup)\n",
    "git clone https://github.com/ROCm/vllm vllm-rocm\n",
    "cd vllm-rocm\n",
    "PYTORCH_ROCM_ARCH=\"gfx942\" python setup.py develop\n",
    "\n",
    "# vllm-rocm requirements\n",
    "pip install -r requirements-rocm.txt\n",
    "pip install msgspec pydantic\n",
    "\n",
    "# without this you will get a `hipbsolidxgemm` error\n",
    "cd gradlib\n",
    "pip install .\n",
    "cd ..\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60cb5cb7-1b62-4f63-b0e9-3a0f2b2fd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Benchmark\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def benchmark_model(model, input_len, output_len, tp):    \n",
    "    # Initialize the DataFrame\n",
    "    df = pd.DataFrame(columns=['Requests per Second', 'Tokens per Second'])\n",
    "    \n",
    "    # Function to run the benchmark command and capture output\n",
    "    def run_benchmark():\n",
    "        command = f\"VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len {input_len} --output-len {output_len} --model {model} -tp {tp}\"\n",
    "        # Run the command and capture the output\n",
    "        start = time.time()\n",
    "        output = get_ipython().getoutput(command)\n",
    "        end = time.time()\n",
    "        total_time = end-start\n",
    "        output_str = ' '.join(output)\n",
    "        print(f\"  Run time: {total_time:.2f} seconds\")\n",
    "        # Use regular expressions to extract the throughput values\n",
    "        matches = re.findall(r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\", output_str)\n",
    "        if matches:\n",
    "            requests_per_sec, tokens_per_sec = map(float, matches[0])\n",
    "            return requests_per_sec, tokens_per_sec\n",
    "        else:\n",
    "            print(f\"No throughput data found for {tuning} tuning.\")\n",
    "            return None, None\n",
    "\n",
    "    # Run benchmarks for no GEMM Tuning\n",
    "    none_rps, none_tps = run_benchmark()\n",
    "    if none_rps is None or none_tps is None:\n",
    "        print(\"Benchmark failed.\")\n",
    "        return None\n",
    "\n",
    "    # Append No GEMM Tuning results to the DataFrame\n",
    "    df.loc[len(df)] = {'Requests per Second': none_rps, 'Tokens per Second': none_tps}\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c24aac-288b-4a8b-a93b-cc20253908ad",
   "metadata": {},
   "source": [
    "## vLLM Upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafaf812-b8bb-404e-82dd-d0a998b98331",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.4.dev9+g5d264f4a\n"
     ]
    }
   ],
   "source": [
    "!VLLM_WORKER_MULTIPROC_METHOD='spawn' python -c 'import vllm; print(vllm.__version__)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecde8ac0-d402-495f-acdd-f02d7e6b2b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "WARNING 10-29 10:22:31 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "PyTorch version: 2.6.0.dev20241015+rocm6.2\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: N/A\n",
      "ROCM used to build PyTorch: 6.2.41133-dd7f95766\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\n",
      "Nvidia driver version: Could not collect\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: 6.2.41133\n",
      "MIOpen runtime version: 3.2.0\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 57 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               208\n",
      "On-line CPU(s) list:                  0-207\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Platinum 8470\n",
      "CPU family:                           6\n",
      "Model:                                143\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   52\n",
      "Socket(s):                            2\n",
      "Stepping:                             8\n",
      "CPU max MHz:                          3800.0000\n",
      "CPU min MHz:                          800.0000\n",
      "BogoMIPS:                             4000.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            4.9 MiB (104 instances)\n",
      "L1i cache:                            3.3 MiB (104 instances)\n",
      "L2 cache:                             208 MiB (104 instances)\n",
      "L3 cache:                             210 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206\n",
      "NUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Not affected\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] lion-pytorch==0.2.2\n",
      "[pip3] mypy-extensions==1.0.0\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "[pip3] pyzmq==26.2.0\n",
      "[pip3] torch==2.6.0.dev20241015+rocm6.2\n",
      "[pip3] torchao==0.5.0\n",
      "[pip3] torchaudio==2.5.0.dev20241015+rocm6.2\n",
      "[pip3] torchtune==0.3.1\n",
      "[pip3] torchvision==0.20.0.dev20241015+rocm6.2\n",
      "[pip3] transformers==4.45.2\n",
      "[pip3] triton==3.1.0\n",
      "[conda] lion-pytorch              0.2.2                    pypi_0    pypi\n",
      "[conda] numpy                     1.26.4                   pypi_0    pypi\n",
      "[conda] pytorch-triton-rocm       3.1.0+cf34004b8a          pypi_0    pypi\n",
      "[conda] pyzmq                     26.2.0          py311h7deb3e3_3    conda-forge\n",
      "[conda] torch                     2.6.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchao                   0.5.0                    pypi_0    pypi\n",
      "[conda] torchaudio                2.5.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchtune                 0.3.1                    pypi_0    pypi\n",
      "[conda] torchvision               0.20.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] transformers              4.45.2                   pypi_0    pypi\n",
      "[conda] triton                    3.1.0                    pypi_0    pypi\n",
      "ROCM Version: 6.2.41134-65d174c3e\n",
      "Neuron SDK Version: N/A\n",
      "vLLM Version: 0.6.4.dev11+gba309422\n",
      "vLLM Build Flags:\n",
      "CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n",
      "GPU Topology:\n",
      "============================ ROCm System Management Interface ============================\n",
      "================================ Weight between two GPUs =================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            15           15           15           15           15           15           15           \n",
      "GPU1   15           0            15           15           15           15           15           15           \n",
      "GPU2   15           15           0            15           15           15           15           15           \n",
      "GPU3   15           15           15           0            15           15           15           15           \n",
      "GPU4   15           15           15           15           0            15           15           15           \n",
      "GPU5   15           15           15           15           15           0            15           15           \n",
      "GPU6   15           15           15           15           15           15           0            15           \n",
      "GPU7   15           15           15           15           15           15           15           0            \n",
      "\n",
      "================================= Hops between two GPUs ==================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            1            1            1            1            1            1            1            \n",
      "GPU1   1            0            1            1            1            1            1            1            \n",
      "GPU2   1            1            0            1            1            1            1            1            \n",
      "GPU3   1            1            1            0            1            1            1            1            \n",
      "GPU4   1            1            1            1            0            1            1            1            \n",
      "GPU5   1            1            1            1            1            0            1            1            \n",
      "GPU6   1            1            1            1            1            1            0            1            \n",
      "GPU7   1            1            1            1            1            1            1            0            \n",
      "\n",
      "=============================== Link Type between two GPUs ===============================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \n",
      "GPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \n",
      "GPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \n",
      "GPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \n",
      "GPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n",
      "\n",
      "======================================= Numa Nodes =======================================\n",
      "GPU[0]\t\t: (Topology) Numa Node: 0\n",
      "GPU[0]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[1]\t\t: (Topology) Numa Node: 0\n",
      "GPU[1]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[2]\t\t: (Topology) Numa Node: 0\n",
      "GPU[2]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[3]\t\t: (Topology) Numa Node: 0\n",
      "GPU[3]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[4]\t\t: (Topology) Numa Node: 1\n",
      "GPU[4]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[5]\t\t: (Topology) Numa Node: 1\n",
      "GPU[5]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[6]\t\t: (Topology) Numa Node: 1\n",
      "GPU[6]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[7]\t\t: (Topology) Numa Node: 1\n",
      "GPU[7]\t\t: (Topology) Numa Affinity: 1\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "# Increase File handles\n",
    "!ulimit -n 131072\n",
    "!python vllm/collect_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8727c4-cec0-44c4-ba1a-25b852378e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "739a116e-095b-460f-b0cd-8ba37ba9f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 74.92 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                91.63            11728.8\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8}\n",
      "  Run time: 88.60 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                43.82           11217.16\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8}\n",
      "  Run time: 107.98 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                22.46           11497.47\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8}\n",
      "  Run time: 159.48 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                10.94           11203.39\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 258.22 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 5.23           10702.26\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8}\n",
      "  Run time: 524.96 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 2.19            8965.82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.63</td>\n",
       "      <td>11728.80</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.82</td>\n",
       "      <td>11217.16</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.46</td>\n",
       "      <td>11497.47</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.94</td>\n",
       "      <td>11203.39</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.23</td>\n",
       "      <td>10702.26</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.19</td>\n",
       "      <td>8965.82</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second                              Config\n",
       "0                91.63           11728.80   input_len=0, output_len=128, tp=8\n",
       "1                43.82           11217.16   input_len=0, output_len=256, tp=8\n",
       "2                22.46           11497.47   input_len=0, output_len=512, tp=8\n",
       "3                10.94           11203.39  input_len=0, output_len=1024, tp=8\n",
       "4                 5.23           10702.26  input_len=0, output_len=2048, tp=8\n",
       "5                 2.19            8965.82  input_len=0, output_len=4096, tp=8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6be3b999-10bd-47a3-bb44-d1ffe7bc6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 76.39 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                83.27           21318.35\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 77.30 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                73.55           28244.18\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 81.28 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                62.58           40052.48\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 86.43 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                45.13           51987.43\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 97.39 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                30.66           66719.98\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 120.45 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                18.16           76695.09\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.27</td>\n",
       "      <td>21318.35</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.55</td>\n",
       "      <td>28244.18</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.58</td>\n",
       "      <td>40052.48</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.13</td>\n",
       "      <td>51987.43</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.66</td>\n",
       "      <td>66719.98</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.16</td>\n",
       "      <td>76695.09</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0                83.27           21318.35   \n",
       "1                73.55           28244.18   \n",
       "2                62.58           40052.48   \n",
       "3                45.13           51987.43   \n",
       "4                30.66           66719.98   \n",
       "5                18.16           76695.09   \n",
       "\n",
       "                                 Config  \n",
       "0   input_len=128, output_len=128, tp=8  \n",
       "1   input_len=256, output_len=128, tp=8  \n",
       "2   input_len=512, output_len=128, tp=8  \n",
       "3  input_len=1024, output_len=128, tp=8  \n",
       "4  input_len=2048, output_len=128, tp=8  \n",
       "5  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40676fe0-1b9c-49e9-af48-ce75e3c3efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8}\n",
      "  Run time: 77.59 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 81.1           21247.29\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8}\n",
      "  Run time: 298.27 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 4.29           17156.23\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 305.56 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 4.15           16997.87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.10</td>\n",
       "      <td>21247.29</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.29</td>\n",
       "      <td>17156.23</td>\n",
       "      <td>input_len=2000, output_len=2000, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.15</td>\n",
       "      <td>16997.87</td>\n",
       "      <td>input_len=2048, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0                81.10           21247.29   \n",
       "1                 4.29           17156.23   \n",
       "2                 4.15           16997.87   \n",
       "\n",
       "                                  Config  \n",
       "0    input_len=131, output_len=131, tp=8  \n",
       "1  input_len=2000, output_len=2000, tp=8  \n",
       "2  input_len=2048, output_len=2048, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6b66a-11d9-49b1-bb8a-316952672833",
   "metadata": {},
   "source": [
    "## vLLM ROCm\n",
    "We switch the kernel here to `vllm-rocm` (in the top right pull-down) and run everything again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b2b51a-a91a-4636-a13b-b72cc57cc635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "0.6.4.dev5+g4bba0922\n",
      "/mnt/nvme1n1p1/MI300-testing\n",
      "popd -> /mnt/nvme1n1p1/MI300-testing\n"
     ]
    }
   ],
   "source": [
    "%pushd /\n",
    "!VLLM_WORKER_MULTIPROC_METHOD='spawn' python -c 'import vllm; print(vllm.__version__)'\n",
    "%popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f52eb61e-ddd6-4739-bc68-e12f0a26c0c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "WARNING 10-29 12:12:16 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "PyTorch version: 2.6.0.dev20241015+rocm6.2\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: N/A\n",
      "ROCM used to build PyTorch: 6.2.41133-dd7f95766\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\n",
      "Nvidia driver version: Could not collect\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: 6.2.41133\n",
      "MIOpen runtime version: 3.2.0\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 57 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               208\n",
      "On-line CPU(s) list:                  0-207\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Platinum 8470\n",
      "CPU family:                           6\n",
      "Model:                                143\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   52\n",
      "Socket(s):                            2\n",
      "Stepping:                             8\n",
      "CPU max MHz:                          3800.0000\n",
      "CPU min MHz:                          800.0000\n",
      "BogoMIPS:                             4000.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            4.9 MiB (104 instances)\n",
      "L1i cache:                            3.3 MiB (104 instances)\n",
      "L2 cache:                             208 MiB (104 instances)\n",
      "L3 cache:                             210 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206\n",
      "NUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Not affected\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] lion-pytorch==0.2.2\n",
      "[pip3] mypy-extensions==1.0.0\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "[pip3] pyzmq==26.2.0\n",
      "[pip3] torch==2.6.0.dev20241015+rocm6.2\n",
      "[pip3] torchao==0.5.0\n",
      "[pip3] torchaudio==2.5.0.dev20241015+rocm6.2\n",
      "[pip3] torchtune==0.3.1\n",
      "[pip3] torchvision==0.20.0.dev20241015+rocm6.2\n",
      "[pip3] transformers==4.45.2\n",
      "[pip3] triton==3.1.0\n",
      "[conda] lion-pytorch              0.2.2                    pypi_0    pypi\n",
      "[conda] numpy                     1.26.4                   pypi_0    pypi\n",
      "[conda] pytorch-triton-rocm       3.1.0+cf34004b8a          pypi_0    pypi\n",
      "[conda] pyzmq                     26.2.0          py311h7deb3e3_3    conda-forge\n",
      "[conda] torch                     2.6.0.dev20241008+rocm6.2          pypi_0    pypi\n",
      "[conda] torchao                   0.5.0                    pypi_0    pypi\n",
      "[conda] torchaudio                2.5.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchtune                 0.3.1                    pypi_0    pypi\n",
      "[conda] torchvision               0.20.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] transformers              4.45.2                   pypi_0    pypi\n",
      "[conda] triton                    3.1.0                    pypi_0    pypi\n",
      "ROCM Version: 6.2.41134-65d174c3e\n",
      "Neuron SDK Version: N/A\n",
      "vLLM Version: 0.6.4.dev5+g4bba0922\n",
      "vLLM Build Flags:\n",
      "CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n",
      "GPU Topology:\n",
      "============================ ROCm System Management Interface ============================\n",
      "================================ Weight between two GPUs =================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            15           15           15           15           15           15           15           \n",
      "GPU1   15           0            15           15           15           15           15           15           \n",
      "GPU2   15           15           0            15           15           15           15           15           \n",
      "GPU3   15           15           15           0            15           15           15           15           \n",
      "GPU4   15           15           15           15           0            15           15           15           \n",
      "GPU5   15           15           15           15           15           0            15           15           \n",
      "GPU6   15           15           15           15           15           15           0            15           \n",
      "GPU7   15           15           15           15           15           15           15           0            \n",
      "\n",
      "================================= Hops between two GPUs ==================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            1            1            1            1            1            1            1            \n",
      "GPU1   1            0            1            1            1            1            1            1            \n",
      "GPU2   1            1            0            1            1            1            1            1            \n",
      "GPU3   1            1            1            0            1            1            1            1            \n",
      "GPU4   1            1            1            1            0            1            1            1            \n",
      "GPU5   1            1            1            1            1            0            1            1            \n",
      "GPU6   1            1            1            1            1            1            0            1            \n",
      "GPU7   1            1            1            1            1            1            1            0            \n",
      "\n",
      "=============================== Link Type between two GPUs ===============================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \n",
      "GPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \n",
      "GPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \n",
      "GPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \n",
      "GPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n",
      "\n",
      "======================================= Numa Nodes =======================================\n",
      "GPU[0]\t\t: (Topology) Numa Node: 0\n",
      "GPU[0]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[1]\t\t: (Topology) Numa Node: 0\n",
      "GPU[1]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[2]\t\t: (Topology) Numa Node: 0\n",
      "GPU[2]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[3]\t\t: (Topology) Numa Node: 0\n",
      "GPU[3]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[4]\t\t: (Topology) Numa Node: 1\n",
      "GPU[4]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[5]\t\t: (Topology) Numa Node: 1\n",
      "GPU[5]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[6]\t\t: (Topology) Numa Node: 1\n",
      "GPU[6]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[7]\t\t: (Topology) Numa Node: 1\n",
      "GPU[7]\t\t: (Topology) Numa Affinity: 1\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "# Increase File handles\n",
    "!ulimit -n 131072\n",
    "!python ~/vllm-rocm/collect_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece3db5f-3c98-489c-8b31-6b303e4b33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e5f306-8d1a-4f3a-8627-5f5d35c1412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Benchmark\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def benchmark_model(model, input_len, output_len, tp):    \n",
    "    # Initialize the DataFrame\n",
    "    df = pd.DataFrame(columns=['Requests per Second', 'Tokens per Second'])\n",
    "    \n",
    "    # Function to run the benchmark command and capture output\n",
    "    def run_benchmark():\n",
    "        command = f\"VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len {input_len} --output-len {output_len} --model {model} -tp {tp}\"\n",
    "        # Run the command and capture the output\n",
    "        start = time.time()\n",
    "        output = get_ipython().getoutput(command)\n",
    "        end = time.time()\n",
    "        total_time = end-start\n",
    "        output_str = ' '.join(output)\n",
    "        print(f\"  Run time: {total_time:.2f} seconds\")\n",
    "        # Use regular expressions to extract the throughput values\n",
    "        matches = re.findall(r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\", output_str)\n",
    "        if matches:\n",
    "            requests_per_sec, tokens_per_sec = map(float, matches[0])\n",
    "            return requests_per_sec, tokens_per_sec\n",
    "        else:\n",
    "            print(f\"No throughput data found for {tuning} tuning.\")\n",
    "            return None, None\n",
    "\n",
    "    # Run benchmarks for no GEMM Tuning\n",
    "    none_rps, none_tps = run_benchmark()\n",
    "    if none_rps is None or none_tps is None:\n",
    "        print(\"Benchmark failed.\")\n",
    "        return None\n",
    "\n",
    "    # Append No GEMM Tuning results to the DataFrame\n",
    "    df.loc[len(df)] = {'Requests per Second': none_rps, 'Tokens per Second': none_tps}\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21c4407e-a067-4727-8dd7-a0f89643cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 81.67 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                85.87           10990.94\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8}\n",
      "  Run time: 91.41 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                44.13           11296.39\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8}\n",
      "  Run time: 112.58 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                22.52           11530.38\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8}\n",
      "  Run time: 161.37 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                10.89           11154.05\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 257.02 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                  5.3           10860.82\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8}\n",
      "  Run time: 523.82 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                  2.2            9026.35\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.87</td>\n",
       "      <td>10990.94</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.13</td>\n",
       "      <td>11296.39</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.52</td>\n",
       "      <td>11530.38</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.89</td>\n",
       "      <td>11154.05</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.30</td>\n",
       "      <td>10860.82</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.20</td>\n",
       "      <td>9026.35</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second                              Config\n",
       "0                85.87           10990.94   input_len=0, output_len=128, tp=8\n",
       "1                44.13           11296.39   input_len=0, output_len=256, tp=8\n",
       "2                22.52           11530.38   input_len=0, output_len=512, tp=8\n",
       "3                10.89           11154.05  input_len=0, output_len=1024, tp=8\n",
       "4                 5.30           10860.82  input_len=0, output_len=2048, tp=8\n",
       "5                 2.20            9026.35  input_len=0, output_len=4096, tp=8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fd4db8a-4065-4ad5-b727-88b4d27a01c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 80.26 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 78.4           20070.07\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 83.16 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                69.67           26753.28\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 84.45 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                60.19           38520.14\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 89.50 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                44.31           51043.84\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 101.74 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                30.22           65750.09\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 123.08 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                18.02           76106.02\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.40</td>\n",
       "      <td>20070.07</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.67</td>\n",
       "      <td>26753.28</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.19</td>\n",
       "      <td>38520.14</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.31</td>\n",
       "      <td>51043.84</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.22</td>\n",
       "      <td>65750.09</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.02</td>\n",
       "      <td>76106.02</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0                78.40           20070.07   \n",
       "1                69.67           26753.28   \n",
       "2                60.19           38520.14   \n",
       "3                44.31           51043.84   \n",
       "4                30.22           65750.09   \n",
       "5                18.02           76106.02   \n",
       "\n",
       "                                 Config  \n",
       "0   input_len=128, output_len=128, tp=8  \n",
       "1   input_len=256, output_len=128, tp=8  \n",
       "2   input_len=512, output_len=128, tp=8  \n",
       "3  input_len=1024, output_len=128, tp=8  \n",
       "4  input_len=2048, output_len=128, tp=8  \n",
       "5  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d405a8-6ea3-490e-86af-5276e12962ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8}\n",
      "  Run time: 82.60 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 71.3            18679.5\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8}\n",
      "  Run time: 300.54 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 4.31           17234.79\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 307.76 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 4.21           17257.16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.30</td>\n",
       "      <td>18679.50</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.31</td>\n",
       "      <td>17234.79</td>\n",
       "      <td>input_len=2000, output_len=2000, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.21</td>\n",
       "      <td>17257.16</td>\n",
       "      <td>input_len=2048, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0                71.30           18679.50   \n",
       "1                 4.31           17234.79   \n",
       "2                 4.21           17257.16   \n",
       "\n",
       "                                  Config  \n",
       "0    input_len=131, output_len=131, tp=8  \n",
       "1  input_len=2000, output_len=2000, tp=8  \n",
       "2  input_len=2048, output_len=2048, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6df112-4365-49cf-924b-ed6c41fee146",
   "metadata": {},
   "source": [
    "# 2024-10-30 Update\n",
    "EmbeddedLM just published their article on MI300X best practices on the vLLM blog and at they suggested that I give the ROCm fork another try with all the tuning they did to see if the potentially more optimized ROCm kernels could overcome the CPU overhead issues:\n",
    "- https://blog.vllm.ai/2024/10/23/vllm-serving-amd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78aac431-9584-4051-bc8e-838081c56620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "  # disable automatic NUMA balancing\n",
    "  !sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "  # check if NUMA balancing is disabled (returns 0 if disabled)\n",
    "  !cat /proc/sys/kernel/numa_balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe0669b6-b1c0-4ea5-9794-19541382cdfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 14:10:50 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 14:11:01 config.py:946] Defaulting to use mp for distributed inference\n",
      "WARNING 10-30 14:11:01 arg_utils.py:982] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 14:11:01 llm_engine.py:238] Initializing an LLM engine (v0.6.4.dev5+g4bba0922) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 14:11:01 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 14:11:01 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 14:11:01 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:07 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:07 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:07 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:07 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:07 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:07 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:07 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:07 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:07 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:11:08 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x765de405a210>, local_subscribe_port=37183, remote_subscribe_port=None)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:08 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:08 selector.py:119] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 14:11:08 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:08 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:09 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:09 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:09 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:09 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:09 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:09 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.11it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.98it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.81it/s]\n",
      "\n",
      "INFO 10-30 14:11:10 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:10 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:10 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:10 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:11 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:11 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:11 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:11 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.73GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.92Gib non_torch_memory=7.00GiB kv_cache_size=158.55GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.65GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.76Gib non_torch_memory=6.83GiB kv_cache_size=158.71GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.71GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.87Gib non_torch_memory=6.94GiB kv_cache_size=158.60GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.73GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.91Gib non_torch_memory=6.99GiB kv_cache_size=158.56GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.72GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.89Gib non_torch_memory=6.96GiB kv_cache_size=158.58GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.65GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.76Gib non_torch_memory=6.83GiB kv_cache_size=158.71GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.63GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.72Gib non_torch_memory=6.79GiB kv_cache_size=158.75GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:11:48 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.55GiB peak_torch_memory=7.24GiB memory_usage_post_profile=9.94Gib non_torch_memory=8.02GiB kv_cache_size=157.53GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:11:48 distributed_gpu_executor.py:57] # GPU blocks: 645227, # CPU blocks: 16384\n",
      "INFO 10-30 14:11:48 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 78.76x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:11:50 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:12:08 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:12:08 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:14<00:00, 68.35it/s, est. speed input: \n",
      "INFO 10-30 14:12:28 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829919)\u001b[0;0m INFO 10-30 14:12:28 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829920)\u001b[0;0m INFO 10-30 14:12:28 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829924)\u001b[0;0m INFO 10-30 14:12:28 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829925)\u001b[0;0m INFO 10-30 14:12:28 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829923)\u001b[0;0m INFO 10-30 14:12:28 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829922)\u001b[0;0m INFO 10-30 14:12:28 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2829921)\u001b[0;0m INFO 10-30 14:12:28 multiproc_worker_utils.py:240] Worker exiting\n",
      "Throughput: 67.59 requests/s, 17301.93 tokens/s\n",
      "[rank0]:[W1030 14:12:30.522208128 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "/home/hotaisle/miniforge3/envs/vllm-rocm/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "\n",
      "real\t1m44.130s\n",
      "user\t8m28.913s\n",
      "sys\t1m47.294s\n"
     ]
    }
   ],
   "source": [
    "# Default\n",
    "!time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a9e74d5-e037-4e47-bb89-31f9d3ae3a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 14:09:26 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 14:09:37 config.py:946] Defaulting to use mp for distributed inference\n",
      "WARNING 10-30 14:09:37 arg_utils.py:982] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 14:09:37 llm_engine.py:238] Initializing an LLM engine (v0.6.4.dev5+g4bba0922) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 14:09:37 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 14:09:37 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 14:09:37 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:43 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:09:45 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7e30899b8250>, local_subscribe_port=40571, remote_subscribe_port=None)\n",
      "INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:45 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 14:09:45 selector.py:119] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.05it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:09:46 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.99it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:09:47 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:09:47 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:09:47 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.88it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:09:47 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 14:09:47 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:09:47 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:09:47 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.65GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.74Gib non_torch_memory=6.82GiB kv_cache_size=158.73GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.63GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.70Gib non_torch_memory=6.78GiB kv_cache_size=158.77GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.73GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.90Gib non_torch_memory=6.97GiB kv_cache_size=158.57GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.73GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.91Gib non_torch_memory=6.98GiB kv_cache_size=158.56GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.71GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.85Gib non_torch_memory=6.93GiB kv_cache_size=158.62GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.65GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.74Gib non_torch_memory=6.82GiB kv_cache_size=158.73GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=4.72GiB peak_torch_memory=7.24GiB memory_usage_post_profile=8.87Gib non_torch_memory=6.94GiB kv_cache_size=158.60GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:10:02 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.55GiB peak_torch_memory=7.24GiB memory_usage_post_profile=9.93Gib non_torch_memory=8.00GiB kv_cache_size=157.54GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:10:02 distributed_gpu_executor.py:57] # GPU blocks: 645291, # CPU blocks: 16384\n",
      "INFO 10-30 14:10:02 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 78.77x\n",
      "INFO 10-30 14:10:03 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 14:10:03 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:10:04 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:10:22 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:10:22 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:12<00:00, 81.73it/s, est. speed input: \n",
      "INFO 10-30 14:10:40 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827173)\u001b[0;0m INFO 10-30 14:10:40 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827177)\u001b[0;0m INFO 10-30 14:10:40 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827178)\u001b[0;0m INFO 10-30 14:10:40 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827175)\u001b[0;0m INFO 10-30 14:10:40 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827176)\u001b[0;0m INFO 10-30 14:10:40 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827174)\u001b[0;0m INFO 10-30 14:10:40 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2827179)\u001b[0;0m INFO 10-30 14:10:40 multiproc_worker_utils.py:240] Worker exiting\n",
      "Throughput: 80.63 requests/s, 20640.56 tokens/s\n",
      "[rank0]:[W1030 14:10:42.794587314 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "321.18/home/hotaisle/miniforge3/envs/vllm-rocm/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "user 95.33system 1:23.36elapsed 499%CPU (0avgtext+0avgdata 13125640maxresident)k\n",
      "0inputs+384432outputs (101major+34979564minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# CK Flash Attention\n",
    "!VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a38e0004-8490-407d-9460-f06974629f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 14:07:47 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 14:07:57 config.py:946] Defaulting to use mp for distributed inference\n",
      "WARNING 10-30 14:07:57 arg_utils.py:982] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 14:07:57 llm_engine.py:238] Initializing an LLM engine (v0.6.4.dev5+g4bba0922) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 14:07:58 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 14:07:58 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 14:07:58 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:03 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:03 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:03 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:03 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:03 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:03 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:03 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:03 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:03 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:03 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:03 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:03 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:03 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:03 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:04 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:08:05 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7af8eb2b6b90>, local_subscribe_port=54261, remote_subscribe_port=None)\n",
      "INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:05 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:05 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.17it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:07 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:07 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.10it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:07 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.73it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.96it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:07 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 14:08:07 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:07 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:08 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:08 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.61GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.66Gib non_torch_memory=8.73GiB kv_cache_size=156.81GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.45GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.33Gib non_torch_memory=8.40GiB kv_cache_size=157.14GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.46GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.37Gib non_torch_memory=8.44GiB kv_cache_size=157.10GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.55GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.54Gib non_torch_memory=8.61GiB kv_cache_size=156.93GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.46GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.37Gib non_torch_memory=8.44GiB kv_cache_size=157.10GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.59GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.62Gib non_torch_memory=8.70GiB kv_cache_size=156.85GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.61GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.65Gib non_torch_memory=8.72GiB kv_cache_size=156.82GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:08:23 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=6.21GiB peak_torch_memory=7.24GiB memory_usage_post_profile=11.24Gib non_torch_memory=9.32GiB kv_cache_size=156.23GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:08:23 distributed_gpu_executor.py:57] # GPU blocks: 639907, # CPU blocks: 16384\n",
      "INFO 10-30 14:08:23 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 78.11x\n",
      "INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:24 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:42 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:42 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:12<00:00, 81.64it/s, est. speed input: \n",
      "INFO 10-30 14:08:59 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824408)\u001b[0;0m INFO 10-30 14:08:59 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824409)\u001b[0;0m INFO 10-30 14:08:59 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824412)\u001b[0;0m INFO 10-30 14:08:59 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824413)\u001b[0;0m INFO 10-30 14:08:59 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824414)\u001b[0;0m INFO 10-30 14:08:59 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824411)\u001b[0;0m INFO 10-30 14:08:59 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2824410)\u001b[0;0m INFO 10-30 14:08:59 multiproc_worker_utils.py:240] Worker exiting\n",
      "Throughput: 80.49 requests/s, 20606.10 tokens/s\n",
      "[rank0]:[W1030 14:09:01.588296667 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "329.86user 94.63system 1:22.12elapsed 516%CPU (0avgtext+0avgdata 13122544maxresident)k\n",
      "0inputs+384432outputs (96major+35144965minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# AMD Network Tuning\n",
    "!NCCL_MIN_NCHANNELS=112 VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adf5bda6-66e8-4282-b1f4-72451b994101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 14:15:38 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=True, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 14:15:49 config.py:946] Defaulting to use mp for distributed inference\n",
      "INFO 10-30 14:15:49 config.py:1063] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 10-30 14:15:49 llm_engine.py:238] Initializing an LLM engine (v0.6.4.dev5+g4bba0922) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 14:15:49 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 14:15:49 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 14:15:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:55 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:55 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:55 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:55 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:55 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:55 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:55 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:55 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:55 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:15:56 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f6ae4409010>, local_subscribe_port=44765, remote_subscribe_port=None)\n",
      "INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:56 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:57 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.65it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:15:58 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.22it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:15:59 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:15:59 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  3.75it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  4.03it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:15:59 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 14:15:59 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:15:59 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:15:59 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:15:59 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.61GiB peak_torch_memory=1.95GiB memory_usage_post_profile=10.66Gib non_torch_memory=8.73GiB kv_cache_size=162.10GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.55GiB peak_torch_memory=1.95GiB memory_usage_post_profile=10.54Gib non_torch_memory=8.62GiB kv_cache_size=162.22GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.46GiB peak_torch_memory=1.95GiB memory_usage_post_profile=10.37Gib non_torch_memory=8.44GiB kv_cache_size=162.39GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.46GiB peak_torch_memory=1.95GiB memory_usage_post_profile=10.37Gib non_torch_memory=8.44GiB kv_cache_size=162.39GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.45GiB peak_torch_memory=1.95GiB memory_usage_post_profile=10.33Gib non_torch_memory=8.41GiB kv_cache_size=162.43GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.59GiB peak_torch_memory=1.95GiB memory_usage_post_profile=10.62Gib non_torch_memory=8.70GiB kv_cache_size=162.14GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.61GiB peak_torch_memory=1.95GiB memory_usage_post_profile=10.65Gib non_torch_memory=8.73GiB kv_cache_size=162.11GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:16:13 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=6.21GiB peak_torch_memory=3.09GiB memory_usage_post_profile=11.24Gib non_torch_memory=9.32GiB kv_cache_size=160.37GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:16:13 distributed_gpu_executor.py:57] # GPU blocks: 656892, # CPU blocks: 16384\n",
      "INFO 10-30 14:16:13 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 80.19x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 14:16:14 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 14:16:14 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:16:14 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:16:15 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:16:15 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:16:15 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:16:15 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:16:15 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:16:15 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:16:32 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:16:32 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:26<00:00, 37.21it/s, est. speed input: \n",
      "INFO 10-30 14:17:05 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838605)\u001b[0;0m INFO 10-30 14:17:05 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838611)\u001b[0;0m INFO 10-30 14:17:05 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838610)\u001b[0;0m INFO 10-30 14:17:05 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838608)\u001b[0;0m INFO 10-30 14:17:05 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838607)\u001b[0;0m INFO 10-30 14:17:05 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838609)\u001b[0;0m INFO 10-30 14:17:05 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2838606)\u001b[0;0m INFO 10-30 14:17:05 multiproc_worker_utils.py:240] Worker exiting\n",
      "Throughput: 36.97 requests/s, 9464.32 tokens/s\n",
      "[rank0]:[W1030 14:17:07.089341284 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "477.23user 89.10system 1:35.98elapsed 590%CPU (0avgtext+0avgdata 13172208maxresident)k\n",
      "0inputs+421240outputs (108major+35042017minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# Holy Hell, definitely leave Chunked Prefill Disabled\n",
    "!NCCL_MIN_NCHANNELS=112 VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8 --enable-chunked-prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "445d4dda-2552-4f94-8fed-0c87d574f3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 14:17:55 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "usage: benchmark_throughput.py [-h] [--backend {vllm,hf,mii}]\n",
      "                               [--dataset DATASET] [--input-len INPUT_LEN]\n",
      "                               [--output-len OUTPUT_LEN] [--model MODEL]\n",
      "                               [--tokenizer TOKENIZER]\n",
      "                               [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,ipex,None}]\n",
      "                               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                               [--n N] [--num-prompts NUM_PROMPTS]\n",
      "                               [--seed SEED]\n",
      "                               [--hf-max-batch-size HF_MAX_BATCH_SIZE]\n",
      "                               [--trust-remote-code]\n",
      "                               [--max-model-len MAX_MODEL_LEN]\n",
      "                               [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                               [--enforce-eager]\n",
      "                               [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]\n",
      "                               [--quantization-param-path QUANTIZATION_PARAM_PATH]\n",
      "                               [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu}]\n",
      "                               [--num-scheduler-steps NUM_SCHEDULER_STEPS]\n",
      "                               [--use-v2-block-manager]\n",
      "                               [--enable-prefix-caching]\n",
      "                               [--enable-chunked-prefill]\n",
      "                               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                               [--download-dir DOWNLOAD_DIR]\n",
      "                               [--output-json OUTPUT_JSON]\n",
      "                               [--distributed-executor-backend {ray,mp}]\n",
      "                               [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,bitsandbytes}]\n",
      "                               [--disable-async-output-proc] [--async-engine]\n",
      "                               [--disable-frontend-multiprocessing]\n",
      "benchmark_throughput.py: error: unrecognized arguments: --max-seq-len-to-capture 16384\n",
      "Command exited with non-zero status 2\n",
      "7.64user 7.44system 0:06.63elapsed 227%CPU (0avgtext+0avgdata 2186320maxresident)k\n",
      "0inputs+12264outputs (1major+153274minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# Graph Capture size # Default 8192 - shouldn't make a difference for short context?\n",
    "!NCCL_MIN_NCHANNELS=112 VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8 --max-seq-len-to-capture 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0086db5b-45b6-433c-ba0a-558b30a425fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Capture size # Default 8192 - shouldn't make a difference for short context?\n",
    "# Not a valid option? --max-seq-len-to-capture 16384\n",
    "# !NCCL_MIN_NCHANNELS=112 VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ba45b52-93f5-4aac-8a64-7a402f9e5c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 14:21:32 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=15, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 14:21:42 config.py:946] Defaulting to use mp for distributed inference\n",
      "WARNING 10-30 14:21:42 arg_utils.py:982] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 14:21:42 llm_engine.py:238] Initializing an LLM engine (v0.6.4.dev5+g4bba0922) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=15, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 14:21:43 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 14:21:43 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 14:21:43 selector.py:119] Using ROCmFlashAttention backend.\n",
      "WARNING 10-30 14:21:43 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:48 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m WARNING 10-30 14:21:48 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:48 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:48 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m WARNING 10-30 14:21:48 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m WARNING 10-30 14:21:48 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:48 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:48 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m WARNING 10-30 14:21:48 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m WARNING 10-30 14:21:48 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:48 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:48 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m WARNING 10-30 14:21:48 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m WARNING 10-30 14:21:48 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:49 utils.py:1129] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:21:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 14:21:50 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x77638d05e550>, local_subscribe_port=48109, remote_subscribe_port=None)\n",
      "INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:50 model_runner.py:1055] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:50 selector.py:119] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-30 14:21:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.14it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.06it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:21:52 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.72it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:21:52 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.94it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:21:52 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 14:21:52 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:21:52 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:21:52 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:21:53 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:21:53 model_runner.py:1066] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.46GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.37Gib non_torch_memory=8.44GiB kv_cache_size=157.10GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.61GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.66Gib non_torch_memory=8.73GiB kv_cache_size=156.81GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.61GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.65Gib non_torch_memory=8.72GiB kv_cache_size=156.82GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.55GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.54Gib non_torch_memory=8.61GiB kv_cache_size=156.93GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.46GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.37Gib non_torch_memory=8.44GiB kv_cache_size=157.10GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.59GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.62Gib non_torch_memory=8.70GiB kv_cache_size=156.85GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=5.45GiB peak_torch_memory=7.24GiB memory_usage_post_profile=10.33Gib non_torch_memory=8.40GiB kv_cache_size=157.14GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:22:08 worker.py:288] Memory profiling results: total_gpu_memory=191.98GiB initial_memory_usage=6.21GiB peak_torch_memory=7.24GiB memory_usage_post_profile=11.24Gib non_torch_memory=9.32GiB kv_cache_size=156.23GiB gpu_memory_utilization=0.90\n",
      "INFO 10-30 14:22:08 distributed_gpu_executor.py:57] # GPU blocks: 639907, # CPU blocks: 16384\n",
      "INFO 10-30 14:22:08 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 78.11x\n",
      "INFO 10-30 14:22:09 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 14:22:09 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:22:09 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:22:09 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:22:09 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:22:09 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:22:09 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:22:09 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:22:10 model_runner.py:1398] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:22:27 custom_all_reduce.py:260] Registering 2275 cuda graph addresses\n",
      "INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847874)\u001b[0;0m INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847872)\u001b[0;0m INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847871)\u001b[0;0m INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847877)\u001b[0;0m INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847873)\u001b[0;0m INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847876)\u001b[0;0m INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2847875)\u001b[0;0m INFO 10-30 14:22:27 model_runner.py:1522] Graph capturing finished in 17 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:06<00:00, 150.84it/s, est. speed input:\n",
      "INFO 10-30 14:22:39 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "Throughput: 147.05 requests/s, 37644.16 tokens/s\n",
      "[rank0]:[W1030 14:22:41.070300821 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "343.11user 94.39system 1:16.72elapsed 570%CPU (0avgtext+0avgdata 13096604maxresident)k\n",
      "0inputs+384544outputs (65major+34876957minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# Multi-step scheduling - should reduce GPU idle time\n",
    "!NCCL_MIN_NCHANNELS=112 VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8 --num-scheduler-steps 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c0ae329-8156-4387-815b-cff9521c8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 14:26:50 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=15, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=1024, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 14:27:01 config.py:946] Defaulting to use mp for distributed inference\n",
      "WARNING 10-30 14:27:01 arg_utils.py:982] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/nvme1n1p1/MI300-testing/vllm/benchmarks/benchmark_throughput.py\", line 572, in <module>\n",
      "    main(args)\n",
      "  File \"/mnt/nvme1n1p1/MI300-testing/vllm/benchmarks/benchmark_throughput.py\", line 352, in main\n",
      "    elapsed_time = run_vllm(*run_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/nvme1n1p1/MI300-testing/vllm/benchmarks/benchmark_throughput.py\", line 95, in run_vllm\n",
      "    llm = LLM(\n",
      "          ^^^^\n",
      "  File \"/home/hotaisle/vllm-rocm/vllm/utils.py\", line 1193, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hotaisle/vllm-rocm/vllm/entrypoints/llm.py\", line 193, in __init__\n",
      "    self.llm_engine = LLMEngine.from_engine_args(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hotaisle/vllm-rocm/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n",
      "    engine_config = engine_args.create_engine_config()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hotaisle/vllm-rocm/vllm/engine/arg_utils.py\", line 1043, in create_engine_config\n",
      "    scheduler_config = SchedulerConfig(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hotaisle/vllm-rocm/vllm/config.py\", line 1078, in __init__\n",
      "    self._verify_args()\n",
      "  File \"/home/hotaisle/vllm-rocm/vllm/config.py\", line 1083, in _verify_args\n",
      "    raise ValueError(\n",
      "ValueError: max_num_batched_tokens (1024) is smaller than max_model_len (131072). This effectively limits the maximum sequence length to max_num_batched_tokens and makes vLLM reject longer sequences. Please increase max_num_batched_tokens or decrease max_model_len.\n",
      "Command exited with non-zero status 1\n",
      "15.42user 14.91system 0:13.74elapsed 220%CPU (0avgtext+0avgdata 2274140maxresident)k\n",
      "0inputs+24536outputs (2major+344175minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# Is this like --max-num-seqs? Let's see 1024 - nope.\n",
    "!NCCL_MIN_NCHANNELS=112 VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 128 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8 --num-scheduler-steps 15 --max-num-batched-tokens 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b427dd4-cd39-435e-bedf-6bd32b1dde94",
   "metadata": {},
   "source": [
    "# OK, let's see how vLLM-ROCm does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d309ffb9-16a2-4e5a-bced-59fa59a589ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Benchmark\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def benchmark_model(model, input_len, output_len, tp):    \n",
    "    # Initialize the DataFrame\n",
    "    df = pd.DataFrame(columns=['Requests per Second', 'Tokens per Second'])\n",
    "    \n",
    "    # Function to run the benchmark command and capture output\n",
    "    def run_benchmark():\n",
    "        command = f\"NCCL_MIN_NCHANNELS=112 VLLM_USE_TRITON_FLASH_ATTN=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm  --num-scheduler-steps 15 --input-len {input_len} --output-len {output_len} --model {model} -tp {tp}\"\n",
    "        # Run the command and capture the output\n",
    "        start = time.time()\n",
    "        output = get_ipython().getoutput(command)\n",
    "        end = time.time()\n",
    "        total_time = end-start\n",
    "        output_str = ' '.join(output)\n",
    "        print(f\"  Run time: {total_time:.2f} seconds\")\n",
    "        # Use regular expressions to extract the throughput values\n",
    "        matches = re.findall(r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\", output_str)\n",
    "        if matches:\n",
    "            requests_per_sec, tokens_per_sec = map(float, matches[0])\n",
    "            return requests_per_sec, tokens_per_sec\n",
    "        else:\n",
    "            print(f\"No throughput data found for {tuning} tuning.\")\n",
    "            return None, None\n",
    "\n",
    "    # Run benchmarks for no GEMM Tuning\n",
    "    none_rps, none_tps = run_benchmark()\n",
    "    if none_rps is None or none_tps is None:\n",
    "        print(\"Benchmark failed.\")\n",
    "        return None\n",
    "\n",
    "    # Append No GEMM Tuning results to the DataFrame\n",
    "    df.loc[len(df)] = {'Requests per Second': none_rps, 'Tokens per Second': none_tps}\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1103eac4-b26d-4b6b-861a-c5db002cf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 75.56 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               174.39           22321.68\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8}\n",
      "  Run time: 82.09 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                89.95           23026.23\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8}\n",
      "  Run time: 90.89 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                45.38           23236.72\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8}\n",
      "  Run time: 114.40 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                22.32           22856.67\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 167.86 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                10.14           20763.64\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8}\n",
      "  Run time: 356.66 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 3.51           14394.31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174.39</td>\n",
       "      <td>22321.68</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.95</td>\n",
       "      <td>23026.23</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.38</td>\n",
       "      <td>23236.72</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.32</td>\n",
       "      <td>22856.67</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.14</td>\n",
       "      <td>20763.64</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.51</td>\n",
       "      <td>14394.31</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second                              Config\n",
       "0               174.39           22321.68   input_len=0, output_len=128, tp=8\n",
       "1                89.95           23026.23   input_len=0, output_len=256, tp=8\n",
       "2                45.38           23236.72   input_len=0, output_len=512, tp=8\n",
       "3                22.32           22856.67  input_len=0, output_len=1024, tp=8\n",
       "4                10.14           20763.64  input_len=0, output_len=2048, tp=8\n",
       "5                 3.51           14394.31  input_len=0, output_len=4096, tp=8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28dc6f7f-23b5-4134-919c-5dedb0ccbb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 77.26 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               146.46           37494.08\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 78.68 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               122.85           47174.45\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 78.70 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                94.51           60489.33\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 85.56 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                64.38           74161.48\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 94.52 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                38.88           84612.11\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 116.55 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                21.06           88950.39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146.46</td>\n",
       "      <td>37494.08</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122.85</td>\n",
       "      <td>47174.45</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.51</td>\n",
       "      <td>60489.33</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.38</td>\n",
       "      <td>74161.48</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.88</td>\n",
       "      <td>84612.11</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.06</td>\n",
       "      <td>88950.39</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0               146.46           37494.08   \n",
       "1               122.85           47174.45   \n",
       "2                94.51           60489.33   \n",
       "3                64.38           74161.48   \n",
       "4                38.88           84612.11   \n",
       "5                21.06           88950.39   \n",
       "\n",
       "                                 Config  \n",
       "0   input_len=128, output_len=128, tp=8  \n",
       "1   input_len=256, output_len=128, tp=8  \n",
       "2   input_len=512, output_len=128, tp=8  \n",
       "3  input_len=1024, output_len=128, tp=8  \n",
       "4  input_len=2048, output_len=128, tp=8  \n",
       "5  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77788911-2dfa-42ec-afb3-8ce2827b5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8}\n",
      "  Run time: 76.49 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               144.87           37957.05\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8}\n",
      "  Run time: 202.68 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 7.53           30123.15\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 205.52 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 7.39           30269.69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144.87</td>\n",
       "      <td>37957.05</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.53</td>\n",
       "      <td>30123.15</td>\n",
       "      <td>input_len=2000, output_len=2000, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.39</td>\n",
       "      <td>30269.69</td>\n",
       "      <td>input_len=2048, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0               144.87           37957.05   \n",
       "1                 7.53           30123.15   \n",
       "2                 7.39           30269.69   \n",
       "\n",
       "                                  Config  \n",
       "0    input_len=131, output_len=131, tp=8  \n",
       "1  input_len=2000, output_len=2000, tp=8  \n",
       "2  input_len=2048, output_len=2048, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad37dd-ba88-46cf-887a-6467b7f7ae4e",
   "metadata": {},
   "source": [
    "# Now Let's try vLLM again\n",
    "- We switch back the the vllm kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c281d3-df68-4fc6-a27e-4d8f5951c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "  # disable automatic NUMA balancing\n",
    "  !sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "  # check if NUMA balancing is disabled (returns 0 if disabled)\n",
    "  !cat /proc/sys/kernel/numa_balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b53a31f-6508-4b83-a909-0938f6399f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Benchmark\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def benchmark_model(model, input_len, output_len, tp):    \n",
    "    # Initialize the DataFrame\n",
    "    df = pd.DataFrame(columns=['Requests per Second', 'Tokens per Second'])\n",
    "    \n",
    "    # Function to run the benchmark command and capture output\n",
    "    def run_benchmark():\n",
    "        # Run the command and capture the output\n",
    "        start = time.time()\n",
    "        output = get_ipython().getoutput(command)\n",
    "        end = time.time()\n",
    "        total_time = end-start\n",
    "        output_str = ' '.join(output)\n",
    "        print(f\"  Run time: {total_time:.2f} seconds\")\n",
    "        # Use regular expressions to extract the throughput values\n",
    "        matches = re.findall(r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\", output_str)\n",
    "        if matches:\n",
    "            requests_per_sec, tokens_per_sec = map(float, matches[0])\n",
    "            return requests_per_sec, tokens_per_sec\n",
    "        else:\n",
    "            print(f\"No throughput data found for {tuning} tuning.\")\n",
    "            return None, None\n",
    "\n",
    "    # Run benchmarks for no GEMM Tuning\n",
    "    none_rps, none_tps = run_benchmark()\n",
    "    if none_rps is None or none_tps is None:\n",
    "        print(\"Benchmark failed.\")\n",
    "        return None\n",
    "\n",
    "    # Append No GEMM Tuning results to the DataFrame\n",
    "    df.loc[len(df)] = {'Requests per Second': none_rps, 'Tokens per Second': none_tps}\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fceb59e4-899b-424d-a3b1-f7d7e36c9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 71.67 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               167.16            21396.5\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8}\n",
      "  Run time: 79.76 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                84.53           21638.64\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8}\n",
      "  Run time: 89.27 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                42.96           21993.74\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8}\n",
      "  Run time: 112.99 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                21.39           21907.37\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 169.98 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 9.75           19976.14\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8}\n",
      "  Run time: 349.46 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 3.54           14485.69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167.16</td>\n",
       "      <td>21396.50</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.53</td>\n",
       "      <td>21638.64</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.96</td>\n",
       "      <td>21993.74</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.39</td>\n",
       "      <td>21907.37</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.75</td>\n",
       "      <td>19976.14</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.54</td>\n",
       "      <td>14485.69</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second                              Config\n",
       "0               167.16           21396.50   input_len=0, output_len=128, tp=8\n",
       "1                84.53           21638.64   input_len=0, output_len=256, tp=8\n",
       "2                42.96           21993.74   input_len=0, output_len=512, tp=8\n",
       "3                21.39           21907.37  input_len=0, output_len=1024, tp=8\n",
       "4                 9.75           19976.14  input_len=0, output_len=2048, tp=8\n",
       "5                 3.54           14485.69  input_len=0, output_len=4096, tp=8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74d6625-2bf9-44a2-a5af-1ce24ddd4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 72.80 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               139.63           35745.19\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 74.33 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               119.27           45797.94\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 76.69 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                92.25           59037.91\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 82.70 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                62.74           72281.77\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 91.09 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                38.23           83190.57\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "  Run time: 111.40 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                21.15           89324.95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139.63</td>\n",
       "      <td>35745.19</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119.27</td>\n",
       "      <td>45797.94</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.25</td>\n",
       "      <td>59037.91</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.74</td>\n",
       "      <td>72281.77</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.23</td>\n",
       "      <td>83190.57</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.15</td>\n",
       "      <td>89324.95</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0               139.63           35745.19   \n",
       "1               119.27           45797.94   \n",
       "2                92.25           59037.91   \n",
       "3                62.74           72281.77   \n",
       "4                38.23           83190.57   \n",
       "5                21.15           89324.95   \n",
       "\n",
       "                                 Config  \n",
       "0   input_len=128, output_len=128, tp=8  \n",
       "1   input_len=256, output_len=128, tp=8  \n",
       "2   input_len=512, output_len=128, tp=8  \n",
       "3  input_len=1024, output_len=128, tp=8  \n",
       "4  input_len=2048, output_len=128, tp=8  \n",
       "5  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658bdb8b-090f-454c-a0b3-8de7985d6445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8}\n",
      "  Run time: 73.34 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0               138.15           36195.75\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8}\n",
      "  Run time: 201.68 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 7.34           29374.21\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8}\n",
      "  Run time: 205.29 seconds\n",
      "   Requests per Second  Tokens per Second\n",
      "0                 7.17           29378.71\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.15</td>\n",
       "      <td>36195.75</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.34</td>\n",
       "      <td>29374.21</td>\n",
       "      <td>input_len=2000, output_len=2000, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.17</td>\n",
       "      <td>29378.71</td>\n",
       "      <td>input_len=2048, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requests per Second  Tokens per Second  \\\n",
       "0               138.15           36195.75   \n",
       "1                 7.34           29374.21   \n",
       "2                 7.17           29378.71   \n",
       "\n",
       "                                  Config  \n",
       "0    input_len=131, output_len=131, tp=8  \n",
       "1  input_len=2000, output_len=2000, tp=8  \n",
       "2  input_len=2048, output_len=2048, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vllm]",
   "language": "python",
   "name": "conda-env-vllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
