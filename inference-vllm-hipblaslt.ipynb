{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e7c027-a089-4402-8c53-3ceb2b0f5854",
   "metadata": {},
   "source": [
    "# vLLM: hipBLAS vs hipBLASLt\n",
    "In newer ROCm PyTorch versions (2.4+), it defaults to using [hipBLASLt](https://github.com/ROCm/hipBLASLt) vs regular [hipBLAS](https://github.com/ROCm/hipBLAS). To use hipBLAS, you need to set `TORCH_BLAS_PREFER_HIPBLASLT=0`.\n",
    "- https://github.com/pytorch/pytorch/issues/119081\n",
    "\n",
    "There are two main reasons to use this setting. 1) the hipBLASLt included with PyTorch does not have support for all hipBLASLt (not to mention hipBLAS) supported platforms like RDNA3 gfx1100 and 2) if you don't increase your file handle limit, it will fail on `-tp 8`.\n",
    "\n",
    "Anyway, let's run some tests and see if it makes any performance difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c24aac-288b-4a8b-a93b-cc20253908ad",
   "metadata": {},
   "source": [
    "## File Handles and -TP8\n",
    "When using hipBLASlt (which is the default for ROCm with PyTorch 2.4+), it will have problems loading above `-tp4` due to exhausted file handles. You can read more about it here: https://github.com/pytorch/pytorch/issues/137695\n",
    "\n",
    "It can be solved by increasing the file handles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecde8ac0-d402-495f-acdd-f02d7e6b2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase File handles\n",
    "!ulimit -n 131072"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9058d-a9a1-44a9-8d2f-759cd6d846af",
   "metadata": {},
   "source": [
    "## Environment\n",
    "For replicability, here are the versions used and some of the more relevant system information using the `vllm/collect_env.py` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a183eaf6-8590-492b-acfd-91f4ba6bc137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "WARNING 10-16 16:38:03 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "PyTorch version: 2.6.0.dev20241015+rocm6.2\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: N/A\n",
      "ROCM used to build PyTorch: 6.2.41133-dd7f95766\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\n",
      "Nvidia driver version: Could not collect\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: 6.2.41133\n",
      "MIOpen runtime version: 3.2.0\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 57 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               208\n",
      "On-line CPU(s) list:                  0-207\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Platinum 8470\n",
      "CPU family:                           6\n",
      "Model:                                143\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   52\n",
      "Socket(s):                            2\n",
      "Stepping:                             8\n",
      "CPU max MHz:                          3800.0000\n",
      "CPU min MHz:                          800.0000\n",
      "BogoMIPS:                             4000.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            4.9 MiB (104 instances)\n",
      "L1i cache:                            3.3 MiB (104 instances)\n",
      "L2 cache:                             208 MiB (104 instances)\n",
      "L3 cache:                             210 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206\n",
      "NUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Not affected\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] lion-pytorch==0.2.2\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "[pip3] pyzmq==26.2.0\n",
      "[pip3] torch==2.6.0.dev20241015+rocm6.2\n",
      "[pip3] torchao==0.5.0\n",
      "[pip3] torchaudio==2.5.0.dev20241015+rocm6.2\n",
      "[pip3] torchtune==0.3.1\n",
      "[pip3] torchvision==0.20.0.dev20241015+rocm6.2\n",
      "[pip3] transformers==4.45.2\n",
      "[pip3] triton==3.1.0\n",
      "[conda] lion-pytorch              0.2.2                    pypi_0    pypi\n",
      "[conda] numpy                     1.26.4                   pypi_0    pypi\n",
      "[conda] pytorch-triton-rocm       3.1.0+cf34004b8a          pypi_0    pypi\n",
      "[conda] pyzmq                     26.2.0          py311h7deb3e3_3    conda-forge\n",
      "[conda] torch                     2.6.0.dev20241008+rocm6.2          pypi_0    pypi\n",
      "[conda] torchao                   0.5.0                    pypi_0    pypi\n",
      "[conda] torchaudio                2.5.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchtune                 0.3.1                    pypi_0    pypi\n",
      "[conda] torchvision               0.20.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] transformers              4.45.2                   pypi_0    pypi\n",
      "[conda] triton                    3.1.0                    pypi_0    pypi\n",
      "ROCM Version: 6.2.41134-65d174c3e\n",
      "Neuron SDK Version: N/A\n",
      "vLLM Version: 0.6.4.dev11+gba309422\n",
      "vLLM Build Flags:\n",
      "CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n",
      "GPU Topology:\n",
      "============================ ROCm System Management Interface ============================\n",
      "================================ Weight between two GPUs =================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            15           15           15           15           15           15           15           \n",
      "GPU1   15           0            15           15           15           15           15           15           \n",
      "GPU2   15           15           0            15           15           15           15           15           \n",
      "GPU3   15           15           15           0            15           15           15           15           \n",
      "GPU4   15           15           15           15           0            15           15           15           \n",
      "GPU5   15           15           15           15           15           0            15           15           \n",
      "GPU6   15           15           15           15           15           15           0            15           \n",
      "GPU7   15           15           15           15           15           15           15           0            \n",
      "\n",
      "================================= Hops between two GPUs ==================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            1            1            1            1            1            1            1            \n",
      "GPU1   1            0            1            1            1            1            1            1            \n",
      "GPU2   1            1            0            1            1            1            1            1            \n",
      "GPU3   1            1            1            0            1            1            1            1            \n",
      "GPU4   1            1            1            1            0            1            1            1            \n",
      "GPU5   1            1            1            1            1            0            1            1            \n",
      "GPU6   1            1            1            1            1            1            0            1            \n",
      "GPU7   1            1            1            1            1            1            1            0            \n",
      "\n",
      "=============================== Link Type between two GPUs ===============================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \n",
      "GPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \n",
      "GPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \n",
      "GPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \n",
      "GPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n",
      "\n",
      "======================================= Numa Nodes =======================================\n",
      "GPU[0]\t\t: (Topology) Numa Node: 0\n",
      "GPU[0]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[1]\t\t: (Topology) Numa Node: 0\n",
      "GPU[1]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[2]\t\t: (Topology) Numa Node: 0\n",
      "GPU[2]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[3]\t\t: (Topology) Numa Node: 0\n",
      "GPU[3]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[4]\t\t: (Topology) Numa Node: 1\n",
      "GPU[4]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[5]\t\t: (Topology) Numa Node: 1\n",
      "GPU[5]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[6]\t\t: (Topology) Numa Node: 1\n",
      "GPU[6]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[7]\t\t: (Topology) Numa Node: 1\n",
      "GPU[7]\t\t: (Topology) Numa Affinity: 1\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!python vllm/collect_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2369d7d-2a2b-4c6a-980f-e883d268a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nbformat in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.20.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8727c4-cec0-44c4-ba1a-25b852378e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebeec3f-9838-45af-9cda-747e4ac8eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, input_len, output_len, tp):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import time\n",
    "    \n",
    "    # Initialize the DataFrame\n",
    "    df = pd.DataFrame(columns=['BLAS', 'Requests per Second', 'Tokens per Second'])\n",
    "    \n",
    "    # Function to run the benchmark command and capture output\n",
    "    def run_benchmark(use_hipblaslt):\n",
    "        start = time.time()\n",
    "        # Set the environment variable\n",
    "        TORCH_BLAS_PREFER_HIPBLASLT = '1' if use_hipblaslt else '0'\n",
    "        # Construct the command\n",
    "        command = f\"TORCH_BLAS_PREFER_HIPBLASLT={TORCH_BLAS_PREFER_HIPBLASLT} VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len {input_len} --output-len {output_len} --model {model} -tp {tp}\"\n",
    "        # Run the command and capture the output\n",
    "        output = get_ipython().getoutput(command)\n",
    "        output_str = ' '.join(output)\n",
    "        # Use regular expressions to extract the throughput values\n",
    "        matches = re.findall(r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\", output_str)\n",
    "        if matches:\n",
    "            requests_per_sec, tokens_per_sec = map(float, matches[0])\n",
    "            return requests_per_sec, tokens_per_sec\n",
    "        else:\n",
    "            print(f\"No throughput data found for {'hipBLASLt' if use_hipblaslt else 'hipBLAS'}.\")\n",
    "            return None, None\n",
    "        duration = time.time() - start\n",
    "        print(f\"Took {duration:.3f} seconds\")\n",
    "\n",
    "    # Run benchmarks for hipBLAS (use_hipblaslt=False)\n",
    "    hb_rps, hb_tps = run_benchmark(use_hipblaslt=False)\n",
    "    if hb_rps is None or hb_tps is None:\n",
    "        print(\"Benchmark failed for hipBLAS.\")\n",
    "        return None\n",
    "\n",
    "    # Append hipBLAS results to the DataFrame\n",
    "    df.loc[len(df)] = {'BLAS': 'hipBLAS', 'Requests per Second': hb_rps, 'Tokens per Second': hb_tps}\n",
    "\n",
    "    # Run benchmarks for Triton FA (use_triton=True)\n",
    "    hblt_rps, hblt_tps = run_benchmark(use_hipblaslt=True)\n",
    "    if hblt_rps is None or hblt_tps is None:\n",
    "        print(\"Benchmark failed for hipBLASLt.\")\n",
    "        return None\n",
    "\n",
    "    # Append Triton FA results to the DataFrame\n",
    "    df.loc[len(df)] = {'BLAS': 'hipBLASLt', 'Requests per Second': hblt_rps, 'Tokens per Second': hblt_tps}\n",
    "\n",
    "    # Calculate percentage differences (hipBLAS is baseline)\n",
    "    percent_diff_rps = ((hblt_rps - hb_rps) / hb_rps) * 100\n",
    "    percent_diff_tps = ((hblt_tps - hb_tps) / hb_tps) * 100\n",
    "    avg_percent_diff = (percent_diff_rps + percent_diff_tps) / 2\n",
    "\n",
    "    # Add percentage differences to the DataFrame\n",
    "    df['% Difference RPS'] = [0, percent_diff_rps]\n",
    "    df['% Difference TPS'] = [0, percent_diff_tps]\n",
    "    df['% Difference Avg'] = [0, avg_percent_diff]\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311201e8-bc19-4b6c-bf6d-d23000c0ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                73.02           18693.78            0.0000   \n",
      "1  hipBLASLt                81.61           20893.19           11.7639   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1         11.765464         11.764682  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLAS</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>73.02</td>\n",
       "      <td>18693.78</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>81.61</td>\n",
       "      <td>20893.19</td>\n",
       "      <td>11.7639</td>\n",
       "      <td>11.765464</td>\n",
       "      <td>11.764682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0    hipBLAS                73.02           18693.78            0.0000   \n",
       "1  hipBLASLt                81.61           20893.19           11.7639   \n",
       "\n",
       "   % Difference TPS  % Difference Avg  \n",
       "0          0.000000          0.000000  \n",
       "1         11.765464         11.764682  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function with your parameters\n",
    "df_results = benchmark_model(\n",
    "    model='meta-llama/Llama-2-7b-chat-hf',\n",
    "    input_len=128,\n",
    "    output_len=128,\n",
    "    tp=8\n",
    ")\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e10ba-0813-49cd-86ba-2a4a125c7065",
   "metadata": {},
   "source": [
    "## Llama2-7B in:128 out:128\n",
    "Just an initial test to make sure everything is working hunky dory. There's a surprising amount of variance... hipBLASLt has been as little as 2% faster, 5% faster, and in this run, almost 12% faster on each run. Any real testing may require 5-10 runs (drop high/low, and mean) or something to reduce std deviation/get better numbers.\n",
    "\n",
    "OK, lets run our Llama3-8B sweeps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739a116e-095b-460f-b0cd-8ba37ba9f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                82.74           10591.08          0.000000   \n",
      "1  hipBLASLt                92.54           11845.65         11.844332   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1         11.845534         11.844933  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                43.16           11048.96          0.000000   \n",
      "1  hipBLASLt                43.32           11088.96          0.370714   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          0.362025          0.366369  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                21.73           11125.29          0.000000   \n",
      "1  hipBLASLt                22.12           11325.74          1.794754   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          1.801751          1.798252  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                10.65           10903.10          0.000000   \n",
      "1  hipBLASLt                10.92           11180.66          2.535211   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          2.545698          2.540455  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                 5.10           10441.79          0.000000   \n",
      "1  hipBLASLt                 5.24           10733.39          2.745098   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          2.792625          2.768861  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                 2.16            8849.35          0.000000   \n",
      "1  hipBLASLt                 2.19            8960.27          1.388889   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          1.253425          1.321157  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLAS</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>82.74</td>\n",
       "      <td>10591.08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>92.54</td>\n",
       "      <td>11845.65</td>\n",
       "      <td>11.844332</td>\n",
       "      <td>11.845534</td>\n",
       "      <td>11.844933</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>43.16</td>\n",
       "      <td>11048.96</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>43.32</td>\n",
       "      <td>11088.96</td>\n",
       "      <td>0.370714</td>\n",
       "      <td>0.362025</td>\n",
       "      <td>0.366369</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>21.73</td>\n",
       "      <td>11125.29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>22.12</td>\n",
       "      <td>11325.74</td>\n",
       "      <td>1.794754</td>\n",
       "      <td>1.801751</td>\n",
       "      <td>1.798252</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10903.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>10.92</td>\n",
       "      <td>11180.66</td>\n",
       "      <td>2.535211</td>\n",
       "      <td>2.545698</td>\n",
       "      <td>2.540455</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>5.10</td>\n",
       "      <td>10441.79</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>5.24</td>\n",
       "      <td>10733.39</td>\n",
       "      <td>2.745098</td>\n",
       "      <td>2.792625</td>\n",
       "      <td>2.768861</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>2.16</td>\n",
       "      <td>8849.35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>2.19</td>\n",
       "      <td>8960.27</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>1.253425</td>\n",
       "      <td>1.321157</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0     hipBLAS                82.74           10591.08          0.000000   \n",
       "1   hipBLASLt                92.54           11845.65         11.844332   \n",
       "2     hipBLAS                43.16           11048.96          0.000000   \n",
       "3   hipBLASLt                43.32           11088.96          0.370714   \n",
       "4     hipBLAS                21.73           11125.29          0.000000   \n",
       "5   hipBLASLt                22.12           11325.74          1.794754   \n",
       "6     hipBLAS                10.65           10903.10          0.000000   \n",
       "7   hipBLASLt                10.92           11180.66          2.535211   \n",
       "8     hipBLAS                 5.10           10441.79          0.000000   \n",
       "9   hipBLASLt                 5.24           10733.39          2.745098   \n",
       "10    hipBLAS                 2.16            8849.35          0.000000   \n",
       "11  hipBLASLt                 2.19            8960.27          1.388889   \n",
       "\n",
       "    % Difference TPS  % Difference Avg                              Config  \n",
       "0           0.000000          0.000000   input_len=0, output_len=128, tp=8  \n",
       "1          11.845534         11.844933   input_len=0, output_len=128, tp=8  \n",
       "2           0.000000          0.000000   input_len=0, output_len=256, tp=8  \n",
       "3           0.362025          0.366369   input_len=0, output_len=256, tp=8  \n",
       "4           0.000000          0.000000   input_len=0, output_len=512, tp=8  \n",
       "5           1.801751          1.798252   input_len=0, output_len=512, tp=8  \n",
       "6           0.000000          0.000000  input_len=0, output_len=1024, tp=8  \n",
       "7           2.545698          2.540455  input_len=0, output_len=1024, tp=8  \n",
       "8           0.000000          0.000000  input_len=0, output_len=2048, tp=8  \n",
       "9           2.792625          2.768861  input_len=0, output_len=2048, tp=8  \n",
       "10          0.000000          0.000000  input_len=0, output_len=4096, tp=8  \n",
       "11          1.253425          1.321157  input_len=0, output_len=4096, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b787f27-8a6c-4004-8b49-89380f4bca07",
   "metadata": {},
   "source": [
    "## Llama3-8B in:0 out:128-4096\n",
    "We see one close to 12% improvement, but largely a 1-3% gain for hipBLASLt vs hipBLAS.\n",
    "\n",
    "Next we test longer inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6be3b999-10bd-47a3-bb44-d1ffe7bc6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                80.50           20608.97           0.00000   \n",
      "1  hipBLASLt                83.38           21345.87           3.57764   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          3.575628          3.576634  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                72.28           27755.17          0.000000   \n",
      "1  hipBLASLt                73.62           28269.26          1.853901   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          1.852231          1.853066  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                58.03           37137.53          0.000000   \n",
      "1  hipBLASLt                61.46           39335.04          5.910736   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          5.917222          5.913979  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                42.58           49055.79          0.000000   \n",
      "1  hipBLASLt                46.82           53931.77          9.957727   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          9.939663          9.948695  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                28.46           61932.65          0.000000   \n",
      "1  hipBLASLt                31.28           68066.89          9.908644   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          9.904695          9.906669  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                17.07           72107.91          0.000000   \n",
      "1  hipBLASLt                18.45           77925.47          8.084359   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          8.067853          8.076106  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLAS</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>80.50</td>\n",
       "      <td>20608.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>83.38</td>\n",
       "      <td>21345.87</td>\n",
       "      <td>3.577640</td>\n",
       "      <td>3.575628</td>\n",
       "      <td>3.576634</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>72.28</td>\n",
       "      <td>27755.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>73.62</td>\n",
       "      <td>28269.26</td>\n",
       "      <td>1.853901</td>\n",
       "      <td>1.852231</td>\n",
       "      <td>1.853066</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>58.03</td>\n",
       "      <td>37137.53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>61.46</td>\n",
       "      <td>39335.04</td>\n",
       "      <td>5.910736</td>\n",
       "      <td>5.917222</td>\n",
       "      <td>5.913979</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>42.58</td>\n",
       "      <td>49055.79</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>46.82</td>\n",
       "      <td>53931.77</td>\n",
       "      <td>9.957727</td>\n",
       "      <td>9.939663</td>\n",
       "      <td>9.948695</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>28.46</td>\n",
       "      <td>61932.65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>31.28</td>\n",
       "      <td>68066.89</td>\n",
       "      <td>9.908644</td>\n",
       "      <td>9.904695</td>\n",
       "      <td>9.906669</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>17.07</td>\n",
       "      <td>72107.91</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>18.45</td>\n",
       "      <td>77925.47</td>\n",
       "      <td>8.084359</td>\n",
       "      <td>8.067853</td>\n",
       "      <td>8.076106</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0     hipBLAS                80.50           20608.97          0.000000   \n",
       "1   hipBLASLt                83.38           21345.87          3.577640   \n",
       "2     hipBLAS                72.28           27755.17          0.000000   \n",
       "3   hipBLASLt                73.62           28269.26          1.853901   \n",
       "4     hipBLAS                58.03           37137.53          0.000000   \n",
       "5   hipBLASLt                61.46           39335.04          5.910736   \n",
       "6     hipBLAS                42.58           49055.79          0.000000   \n",
       "7   hipBLASLt                46.82           53931.77          9.957727   \n",
       "8     hipBLAS                28.46           61932.65          0.000000   \n",
       "9   hipBLASLt                31.28           68066.89          9.908644   \n",
       "10    hipBLAS                17.07           72107.91          0.000000   \n",
       "11  hipBLASLt                18.45           77925.47          8.084359   \n",
       "\n",
       "    % Difference TPS  % Difference Avg                                Config  \n",
       "0           0.000000          0.000000   input_len=128, output_len=128, tp=8  \n",
       "1           3.575628          3.576634   input_len=128, output_len=128, tp=8  \n",
       "2           0.000000          0.000000   input_len=256, output_len=128, tp=8  \n",
       "3           1.852231          1.853066   input_len=256, output_len=128, tp=8  \n",
       "4           0.000000          0.000000   input_len=512, output_len=128, tp=8  \n",
       "5           5.917222          5.913979   input_len=512, output_len=128, tp=8  \n",
       "6           0.000000          0.000000  input_len=1024, output_len=128, tp=8  \n",
       "7           9.939663          9.948695  input_len=1024, output_len=128, tp=8  \n",
       "8           0.000000          0.000000  input_len=2048, output_len=128, tp=8  \n",
       "9           9.904695          9.906669  input_len=2048, output_len=128, tp=8  \n",
       "10          0.000000          0.000000  input_len=4096, output_len=128, tp=8  \n",
       "11          8.067853          8.076106  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c54bbe-bae6-497a-91eb-b121675e48f1",
   "metadata": {},
   "source": [
    "## Llama3-8B in:128-4096 out:128\n",
    "With longer context, we see on average a bigger perf boost, with 5-10% possible. It looks like on longer context, hipBLASlt has better performance.\n",
    "\n",
    "We'll do just a couple more tests now:\n",
    "- in:131 out:131 - let's see how a non power of 2 (prime no less) number works\n",
    "- in:4000 out:4000 - and what a medium context looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40676fe0-1b9c-49e9-af48-ce75e3c3efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8}\n",
      "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0    hipBLAS                76.94           20159.24          0.000000   \n",
      "1  hipBLASLt                77.93           20417.96          1.286717   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          1.283382          1.285049  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8}\n",
      "No throughput data found for hipBLAS.\n",
      "Benchmark failed for hipBLAS.\n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8}\n",
      "No throughput data found for hipBLAS.\n",
      "Benchmark failed for hipBLAS.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLAS</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hipBLAS</td>\n",
       "      <td>76.94</td>\n",
       "      <td>20159.24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hipBLASLt</td>\n",
       "      <td>77.93</td>\n",
       "      <td>20417.96</td>\n",
       "      <td>1.286717</td>\n",
       "      <td>1.283382</td>\n",
       "      <td>1.285049</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        BLAS  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0    hipBLAS                76.94           20159.24          0.000000   \n",
       "1  hipBLASLt                77.93           20417.96          1.286717   \n",
       "\n",
       "   % Difference TPS  % Difference Avg                               Config  \n",
       "0          0.000000          0.000000  input_len=131, output_len=131, tp=8  \n",
       "1          1.283382          1.285049  input_len=131, output_len=131, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0548742-a8f4-4918-b04d-4d46d062d070",
   "metadata": {},
   "source": [
    "Hmm, it looks like there werer some failures. Oh well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vllm]",
   "language": "python",
   "name": "conda-env-vllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
