{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ba054c-a0a7-4c65-b250-e0ebc16dd35f",
   "metadata": {},
   "source": [
    "# vLLM: GEMM TUning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c24aac-288b-4a8b-a93b-cc20253908ad",
   "metadata": {},
   "source": [
    "## File Handles and -TP8\n",
    "When using hipBLASlt (which is the default for ROCm with PyTorch 2.4+), it will have problems loading above `-tp4` due to exhausted file handles. You can read more about it here: https://github.com/pytorch/pytorch/issues/137695\n",
    "\n",
    "It can be solved by increasing the file handles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecde8ac0-d402-495f-acdd-f02d7e6b2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase File handles\n",
    "!ulimit -n 131072"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9058d-a9a1-44a9-8d2f-759cd6d846af",
   "metadata": {},
   "source": [
    "## Environment\n",
    "For replicability, here are the versions used and some of the more relevant system information using the `vllm/collect_env.py` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a183eaf6-8590-492b-acfd-91f4ba6bc137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "WARNING 10-28 17:18:23 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "PyTorch version: 2.6.0.dev20241015+rocm6.2\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: N/A\n",
      "ROCM used to build PyTorch: 6.2.41133-dd7f95766\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\n",
      "Nvidia driver version: Could not collect\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: 6.2.41133\n",
      "MIOpen runtime version: 3.2.0\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 57 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               208\n",
      "On-line CPU(s) list:                  0-207\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Platinum 8470\n",
      "CPU family:                           6\n",
      "Model:                                143\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   52\n",
      "Socket(s):                            2\n",
      "Stepping:                             8\n",
      "CPU max MHz:                          3800.0000\n",
      "CPU min MHz:                          800.0000\n",
      "BogoMIPS:                             4000.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            4.9 MiB (104 instances)\n",
      "L1i cache:                            3.3 MiB (104 instances)\n",
      "L2 cache:                             208 MiB (104 instances)\n",
      "L3 cache:                             210 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206\n",
      "NUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Not affected\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] lion-pytorch==0.2.2\n",
      "[pip3] mypy-extensions==1.0.0\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "[pip3] pyzmq==26.2.0\n",
      "[pip3] torch==2.6.0.dev20241015+rocm6.2\n",
      "[pip3] torchao==0.5.0\n",
      "[pip3] torchaudio==2.5.0.dev20241015+rocm6.2\n",
      "[pip3] torchtune==0.3.1\n",
      "[pip3] torchvision==0.20.0.dev20241015+rocm6.2\n",
      "[pip3] transformers==4.45.2\n",
      "[pip3] triton==3.1.0\n",
      "[conda] lion-pytorch              0.2.2                    pypi_0    pypi\n",
      "[conda] numpy                     1.26.4                   pypi_0    pypi\n",
      "[conda] pytorch-triton-rocm       3.1.0+cf34004b8a          pypi_0    pypi\n",
      "[conda] pyzmq                     26.2.0          py311h7deb3e3_3    conda-forge\n",
      "[conda] torch                     2.6.0.dev20241008+rocm6.2          pypi_0    pypi\n",
      "[conda] torchao                   0.5.0                    pypi_0    pypi\n",
      "[conda] torchaudio                2.5.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchtune                 0.3.1                    pypi_0    pypi\n",
      "[conda] torchvision               0.20.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] transformers              4.45.2                   pypi_0    pypi\n",
      "[conda] triton                    3.1.0                    pypi_0    pypi\n",
      "ROCM Version: 6.2.41134-65d174c3e\n",
      "Neuron SDK Version: N/A\n",
      "vLLM Version: 0.6.4.dev11+gba309422\n",
      "vLLM Build Flags:\n",
      "CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n",
      "GPU Topology:\n",
      "============================ ROCm System Management Interface ============================\n",
      "================================ Weight between two GPUs =================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            15           15           15           15           15           15           15           \n",
      "GPU1   15           0            15           15           15           15           15           15           \n",
      "GPU2   15           15           0            15           15           15           15           15           \n",
      "GPU3   15           15           15           0            15           15           15           15           \n",
      "GPU4   15           15           15           15           0            15           15           15           \n",
      "GPU5   15           15           15           15           15           0            15           15           \n",
      "GPU6   15           15           15           15           15           15           0            15           \n",
      "GPU7   15           15           15           15           15           15           15           0            \n",
      "\n",
      "================================= Hops between two GPUs ==================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            1            1            1            1            1            1            1            \n",
      "GPU1   1            0            1            1            1            1            1            1            \n",
      "GPU2   1            1            0            1            1            1            1            1            \n",
      "GPU3   1            1            1            0            1            1            1            1            \n",
      "GPU4   1            1            1            1            0            1            1            1            \n",
      "GPU5   1            1            1            1            1            0            1            1            \n",
      "GPU6   1            1            1            1            1            1            0            1            \n",
      "GPU7   1            1            1            1            1            1            1            0            \n",
      "\n",
      "=============================== Link Type between two GPUs ===============================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \n",
      "GPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \n",
      "GPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \n",
      "GPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \n",
      "GPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n",
      "\n",
      "======================================= Numa Nodes =======================================\n",
      "GPU[0]\t\t: (Topology) Numa Node: 0\n",
      "GPU[0]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[1]\t\t: (Topology) Numa Node: 0\n",
      "GPU[1]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[2]\t\t: (Topology) Numa Node: 0\n",
      "GPU[2]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[3]\t\t: (Topology) Numa Node: 0\n",
      "GPU[3]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[4]\t\t: (Topology) Numa Node: 1\n",
      "GPU[4]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[5]\t\t: (Topology) Numa Node: 1\n",
      "GPU[5]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[6]\t\t: (Topology) Numa Node: 1\n",
      "GPU[6]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[7]\t\t: (Topology) Numa Node: 1\n",
      "GPU[7]\t\t: (Topology) Numa Affinity: 1\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!python vllm/collect_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2369d7d-2a2b-4c6a-980f-e883d268a4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nbformat in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.20.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8727c4-cec0-44c4-ba1a-25b852378e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305c7c6-33a9-4de9-b783-860d0c9ff438",
   "metadata": {},
   "source": [
    "# PyTorch GEMM Tuning\n",
    "PyTorch has new feature called [TunableOp](https://pytorch.org/docs/main/cuda.tunable.html) which allows GEMM tuning.\n",
    "I'm using this ROCm blog [guide to run the GEMM Tuning](https://rocm.blogs.amd.com/artificial-intelligence/vllm-optimize/README.html#gemm-tuning). Here is a replication of their example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb0cd89-1655-4a28-810d-6fc99c0061f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-28 17:18:29 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=512, output_len=512, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=10, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\n",
      "INFO 10-28 17:18:37 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-28 17:18:37 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-28 17:18:38 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-28 17:18:38 model_runner.py:1060] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "INFO 10-28 17:18:38 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-28 17:18:39 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.68it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:02,  2.17s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.12s/it]\n",
      "\n",
      "INFO 10-28 17:18:47 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "reading tuning results from tunableop_results0.csv\n",
      "key=\"PT_VERSION\" is not provided for validation. \n",
      "results validator check failed\n",
      "INFO 10-28 17:21:21 gpu_executor.py:122] # GPU blocks: 10207, # CPU blocks: 2048\n",
      "INFO 10-28 17:21:21 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 19.94x\n",
      "INFO 10-28 17:21:21 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-28 17:21:21 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-28 17:32:00 model_runner.py:1530] Graph capturing finished in 639 secs.\n",
      "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None\n",
      "Warming up...\n",
      "Warmup iterations: 100%|████████████████████████| 10/10 [02:50<00:00, 17.01s/it]\n",
      "Profiling iterations: 100%|█████████████████████| 10/10 [00:46<00:00,  4.62s/it]\n",
      "Avg latency: 4.622690040399993 seconds\n",
      "10% percentile latency: 4.60895177479988 seconds\n",
      "25% percentile latency: 4.609407359750094 seconds\n",
      "50% percentile latency: 4.6099951174999205 seconds\n",
      "75% percentile latency: 4.610802000499916 seconds\n",
      "90% percentile latency: 4.6242288758001 seconds\n",
      "99% percentile latency: 4.728618280280079 seconds\n",
      "[rank0]:[W1028 17:35:37.979163799 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "writing file tunableop_results0.csv\n",
      "1890.76user 37.86system 17:15.59elapsed 186%CPU (0avgtext+0avgdata 12518240maxresident)k\n",
      "35957488inputs+24576outputs (2047major+9519222minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# With PyTorch GEMM Tuning\n",
    "!VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=1 PYTORCH_TUNABLEOP_VERBOSE=1 time python3 vllm/benchmarks/benchmark_latency.py --input-len 512 --output-len 512 --num-iters 10 --model meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662d5397-a056-4c7a-8754-ea47c1ffeba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-28 17:38:45 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=512, output_len=512, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=10, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\n",
      "INFO 10-28 17:38:53 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-28 17:38:53 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-28 17:38:53 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-28 17:38:54 model_runner.py:1060] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "INFO 10-28 17:38:54 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-28 17:38:54 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.20s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.62s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.46s/it]\n",
      "\n",
      "INFO 10-28 17:39:00 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "reading tuning results from tunableop_results0.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "INFO 10-28 17:39:13 gpu_executor.py:122] # GPU blocks: 78839, # CPU blocks: 2048\n",
      "INFO 10-28 17:39:13 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 153.98x\n",
      "INFO 10-28 17:39:14 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-28 17:39:14 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-28 17:39:21 model_runner.py:1530] Graph capturing finished in 7 secs.\n",
      "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None\n",
      "Warming up...\n",
      "Warmup iterations: 100%|████████████████████████| 10/10 [00:46<00:00,  4.65s/it]\n",
      "Profiling iterations: 100%|█████████████████████| 10/10 [00:46<00:00,  4.67s/it]\n",
      "Avg latency: 4.672779997499902 seconds\n",
      "10% percentile latency: 4.668842132199893 seconds\n",
      "25% percentile latency: 4.669351155499953 seconds\n",
      "50% percentile latency: 4.672088821499983 seconds\n",
      "75% percentile latency: 4.675085914749843 seconds\n",
      "90% percentile latency: 4.678566259499826 seconds\n",
      "99% percentile latency: 4.678785352349908 seconds\n",
      "[rank0]:[W1028 17:40:55.248832170 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "185.59user 27.01system 2:18.20elapsed 153%CPU (0avgtext+0avgdata 11833984maxresident)k\n",
      "0inputs+24552outputs (2major+8387959minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# Run again to see if we skip tuning time\n",
    "!VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=1 PYTORCH_TUNABLEOP_VERBOSE=1 time python3 vllm/benchmarks/benchmark_latency.py --input-len 512 --output-len 512 --num-iters 10 --model meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b74ef6-501b-42ad-bed2-38012a066491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-28 17:35:45 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=512, output_len=512, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=10, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\n",
      "INFO 10-28 17:35:56 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-28 17:35:56 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-28 17:35:57 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-28 17:35:57 model_runner.py:1060] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "INFO 10-28 17:35:57 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-28 17:35:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.36it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.20s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.51s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.65s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.47s/it]\n",
      "\n",
      "INFO 10-28 17:36:04 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "INFO 10-28 17:36:17 gpu_executor.py:122] # GPU blocks: 78921, # CPU blocks: 2048\n",
      "INFO 10-28 17:36:17 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 154.14x\n",
      "INFO 10-28 17:36:18 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-28 17:36:18 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-28 17:36:25 model_runner.py:1530] Graph capturing finished in 7 secs.\n",
      "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None\n",
      "Warming up...\n",
      "Warmup iterations: 100%|████████████████████████| 10/10 [00:49<00:00,  4.91s/it]\n",
      "Profiling iterations: 100%|█████████████████████| 10/10 [00:49<00:00,  4.93s/it]\n",
      "Avg latency: 4.934604864800031 seconds\n",
      "10% percentile latency: 4.9272411277000625 seconds\n",
      "25% percentile latency: 4.932319527249888 seconds\n",
      "50% percentile latency: 4.936575772999959 seconds\n",
      "75% percentile latency: 4.938140879500054 seconds\n",
      "90% percentile latency: 4.938273840200168 seconds\n",
      "99% percentile latency: 4.938347138720189 seconds\n",
      "[rank0]:[W1028 17:38:04.482224713 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "192.31user 27.27system 2:26.84elapsed 149%CPU (0avgtext+0avgdata 11729892maxresident)k\n",
      "0inputs+24552outputs (2major+8568527minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# Without PyTorch GEMM Tuning - ROCR_VISIBLE_DEVICES=7 if you can run at same time\n",
    "!VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=0 time python3 vllm/benchmarks/benchmark_latency.py --input-len 512 --output-len 512 --num-iters 10 --model meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674ead6-b0a0-4a80-a13e-6ba85ca592d1",
   "metadata": {},
   "source": [
    "# GEMM Tuning Replication Results\n",
    "While our raw numbers are a bit higher (avg 4.669s, 4.935s - 5.4% lower latency) vs their results (avg 4.30s, 4.60s - 6.5% lower latency) the latency decrease with GEMM tuning was directionally similar and within the same ballpark.\n",
    "\n",
    "Replication is good!\n",
    "\n",
    "Now lets see how this affects bechmark throughput..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b3da44-6f23-4d12-822b-fddf67c3f513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 03:04:47 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=0, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-29 03:04:56 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-29 03:04:56 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-29 03:04:56 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-29 03:04:56 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-29 03:04:57 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-29 03:04:57 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-29 03:04:57 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:01 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:01 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:01 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:01 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:02 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:02 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:02 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:02 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:02 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 03:05:03 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7cc544d52890>, local_subscribe_port=52909, remote_subscribe_port=None)\n",
      "INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:03 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-29 03:05:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:05 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.58it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:05 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:05 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:05 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.01it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:05 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:05 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  3.79it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  4.02it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:05 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-29 03:05:06 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "reading tuning results from tunableop_results6.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results4.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results3.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results7.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results2.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results1.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results0.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results5.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "INFO 10-29 03:05:21 distributed_gpu_executor.py:57] # GPU blocks: 646723, # CPU blocks: 16384\n",
      "INFO 10-29 03:05:21 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 78.95x\n",
      "INFO 10-29 03:05:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-29 03:05:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:05:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:05:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:05:23 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 165 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 165 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 166 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 165 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 165 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 165 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 166 secs.\n",
      "INFO 10-29 03:08:08 model_runner.py:1530] Graph capturing finished in 166 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:18<00:00, 55.54it/s, est. speed input: \n",
      "INFO 10-29 03:08:31 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880434)\u001b[0;0m INFO 10-29 03:08:31 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880435)\u001b[0;0m INFO 10-29 03:08:31 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880436)\u001b[0;0m INFO 10-29 03:08:31 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880437)\u001b[0;0m INFO 10-29 03:08:31 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880438)\u001b[0;0m INFO 10-29 03:08:31 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880440)\u001b[0;0m INFO 10-29 03:08:31 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=880439)\u001b[0;0m INFO 10-29 03:08:31 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 55.26 requests/s, 7073.90 tokens/s\n",
      "[rank0]:[W1029 03:08:34.638668299 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "2104.98user 383.24system 3:54.55elapsed 1060%CPU (0avgtext+0avgdata 13056156maxresident)k\n",
      "0inputs+384440outputs (104major+41411740minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "!VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=1 PYTORCH_TUNABLEOP_VERBOSE=1 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 0 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebde419-6993-4c92-88a4-8b95bee6fa14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 03:18:09 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=0, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-29 03:18:18 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-29 03:18:18 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-29 03:18:18 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-29 03:18:18 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-29 03:18:19 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-29 03:18:19 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-29 03:18:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 03:18:25 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7e6c6094e510>, local_subscribe_port=48645, remote_subscribe_port=None)\n",
      "INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-29 03:18:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.94it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.44it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:27 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:27 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  3.91it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  4.22it/s]\n",
      "\n",
      "INFO 10-29 03:18:27 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:27 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "reading tuning results from tunableop_results0.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results5.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results1.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results3.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results7.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results6.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results4.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "reading tuning results from tunableop_results2.csv\n",
      "Validator PT_VERSION=2.6.0\n",
      "Validator ROCM_VERSION=6.2.0.0-66-98a50c5\n",
      "Validator HIPBLASLT_VERSION=800-50015815\n",
      "Validator GCN_ARCH_NAME=gfx942:sramecc+:xnack-\n",
      "Validator ROCBLAS_VERSION=4.2.0-54f305c1-dirty\n",
      "ROCBLAS_VERSION validation: expect 4.2.0-54f305c1-dirty to match 4.2.0-54f305c1-dirty\n",
      "GCN_ARCH_NAME validation: expect gfx942:sramecc+:xnack- to match gfx942:sramecc+:xnack-\n",
      "HIPBLASLT_VERSION validation: expect 800-50015815 to match 800-50015815\n",
      "ROCM_VERSION validation: expect 6.2.0.0-66-98a50c5 to match 6.2.0.0-66-98a50c5\n",
      "PT_VERSION validation: expect 2.6.0 to match 2.6.0\n",
      "Loading results\n",
      "INFO 10-29 03:18:43 distributed_gpu_executor.py:57] # GPU blocks: 646723, # CPU blocks: 16384\n",
      "INFO 10-29 03:18:43 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 78.95x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:18:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-29 03:18:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-29 03:18:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:18:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:18:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:18:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "INFO 10-29 03:21:36 model_runner.py:1530] Graph capturing finished in 171 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:17<00:00, 57.14it/s, est. speed input: \n",
      "INFO 10-29 03:21:58 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902662)\u001b[0;0m INFO 10-29 03:21:58 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902665)\u001b[0;0m INFO 10-29 03:21:58 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902663)\u001b[0;0m INFO 10-29 03:21:58 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902666)\u001b[0;0m INFO 10-29 03:21:58 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902667)\u001b[0;0m INFO 10-29 03:21:58 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902664)\u001b[0;0m INFO 10-29 03:21:58 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=902668)\u001b[0;0m INFO 10-29 03:21:58 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 56.85 requests/s, 7276.72 tokens/s\n",
      "[rank0]:[W1029 03:22:00.444736217 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "2145.59user 373.45system 3:58.77elapsed 1054%CPU (0avgtext+0avgdata 13060508maxresident)k\n",
      "0inputs+384432outputs (180major+41508482minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "# Run again to see what it's like after compile...\n",
    "!VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=1 PYTORCH_TUNABLEOP_VERBOSE=1 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 0 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b708fb6-d2e3-4dc4-8563-14640adc52fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 03:08:57 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=0, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-29 03:09:06 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-29 03:09:06 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-29 03:09:06 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-29 03:09:06 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-29 03:09:06 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-29 03:09:06 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-29 03:09:07 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:11 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:12 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:12 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 03:09:13 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x789f37e46350>, local_subscribe_port=50937, remote_subscribe_port=None)\n",
      "INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:13 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:13 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  6.04it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.55it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:14 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:14 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  4.04it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  4.34it/s]\n",
      "\n",
      "INFO 10-29 03:09:15 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:15 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:15 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:15 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:15 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:15 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-29 03:09:30 distributed_gpu_executor.py:57] # GPU blocks: 647379, # CPU blocks: 16384\n",
      "INFO 10-29 03:09:30 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 79.03x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:31 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:31 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:31 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:31 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-29 03:09:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-29 03:09:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "INFO 10-29 03:09:45 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:10<00:00, 90.99it/s, est. speed input: \n",
      "INFO 10-29 03:10:01 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887348)\u001b[0;0m INFO 10-29 03:10:01 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887349)\u001b[0;0m INFO 10-29 03:10:01 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887352)\u001b[0;0m INFO 10-29 03:10:01 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887354)\u001b[0;0m INFO 10-29 03:10:01 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887351)\u001b[0;0m INFO 10-29 03:10:01 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887350)\u001b[0;0m INFO 10-29 03:10:01 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=887353)\u001b[0;0m INFO 10-29 03:10:01 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 90.25 requests/s, 11552.54 tokens/s\n",
      "[rank0]:[W1029 03:10:03.150654107 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "288.93user 98.15system 1:13.83elapsed 524%CPU (0avgtext+0avgdata 12907984maxresident)k\n",
      "0inputs+384432outputs (153major+37456105minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "!VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 0 --output-len 128 --model meta-llama/Llama-3.1-8B-Instruct -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebeec3f-9838-45af-9cda-747e4ac8eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def benchmark_model(model, input_len, output_len, tp):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    # Initialize the DataFrame\n",
    "    df = pd.DataFrame(columns=['Tuning', 'Requests per Second', 'Tokens per Second'])\n",
    "    \n",
    "    # Function to run the benchmark command and capture output\n",
    "    def run_benchmark(tuning):\n",
    "        # Set the environment variable\n",
    "        if tuning == 'pytorch':\n",
    "            command = f\"VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=1 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len {input_len} --output-len {output_len} --model {model} -tp {tp}\"\n",
    "        else:\n",
    "            command = f\"VLLM_USE_TRITON_FLASH_ATTN=0 PYTORCH_TUNABLEOP_ENABLED=0 time python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len {input_len} --output-len {output_len} --model {model} -tp {tp}\"\n",
    "        # Run the command and capture the output\n",
    "        start = time.time()\n",
    "        output = get_ipython().getoutput(command)\n",
    "        end = time.time()\n",
    "        output_str = ' '.join(output)\n",
    "        print(f\"  {tuning} Run time: {end-start:.2f} seconds\")\n",
    "        # Use regular expressions to extract the throughput values\n",
    "        matches = re.findall(r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\", output_str)\n",
    "        if matches:\n",
    "            requests_per_sec, tokens_per_sec = map(float, matches[0])\n",
    "            return requests_per_sec, tokens_per_sec\n",
    "        else:\n",
    "            print(f\"No throughput data found for {tuning} tuning.\")\n",
    "            return None, None\n",
    "        \n",
    "\n",
    "    # Run benchmarks for no GEMM Tuning\n",
    "    none_rps, none_tps = run_benchmark(tuning='none')\n",
    "    if none_rps is None or none_tps is None:\n",
    "        print(\"Benchmark failed for no GEMM tuning.\")\n",
    "        return None\n",
    "\n",
    "    # Append No GEMM Tuning results to the DataFrame\n",
    "    df.loc[len(df)] = {'Tuning': 'none', 'Requests per Second': none_rps, 'Tokens per Second': none_tps}\n",
    "\n",
    "    # Run benchmarks for Pytorch GEMM Tuning (tunable ops)\n",
    "    pytorch_rps, pytorch_tps = run_benchmark(tuning='pytorch')\n",
    "    if pytorch_rps is None or pytorch_tps is None:\n",
    "        print(\"Benchmark failed for Pytorch TunableOp GEMM tuning.\")\n",
    "        return None\n",
    "\n",
    "    # Append Triton FA results to the DataFrame\n",
    "    df.loc[len(df)] = {'Tuning': 'pytorch', 'Requests per Second': pytorch_rps, 'Tokens per Second': pytorch_tps}\n",
    "\n",
    "    # Calculate percentage differences (None is baseline)\n",
    "    percent_diff_rps = ((pytorch_rps - none_rps) / none_rps) * 100\n",
    "    percent_diff_tps = ((pytorch_tps - none_tps) / none_tps) * 100\n",
    "    avg_percent_diff = (percent_diff_rps + percent_diff_tps) / 2\n",
    "\n",
    "    # Add percentage differences to the DataFrame\n",
    "    df['% Difference RPS'] = [0, percent_diff_rps]\n",
    "    df['% Difference TPS'] = [0, percent_diff_tps]\n",
    "    df['% Difference Avg'] = [0, avg_percent_diff]\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "739a116e-095b-460f-b0cd-8ba37ba9f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 75.02 seconds\n",
      "  pytorch Run time: 213.28 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                92.71           11866.28          0.000000   \n",
      "1  pytorch                55.85            7148.72        -39.758386   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000            0.0000  \n",
      "1        -39.756015          -39.7572  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8}\n",
      "  none Run time: 85.67 seconds\n",
      "  pytorch Run time: 225.29 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                44.84           11480.29          0.000000   \n",
      "1  pytorch                34.30            8781.02        -23.505798   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -23.512211        -23.509005  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8}\n",
      "  none Run time: 107.68 seconds\n",
      "  pytorch Run time: 243.89 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                22.60           11570.40          0.000000   \n",
      "1  pytorch                19.74           10106.81        -12.654867   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000           0.00000  \n",
      "1        -12.649433         -12.65215  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8}\n",
      "  none Run time: 154.74 seconds\n",
      "  pytorch Run time: 273.51 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                11.03           11299.32          0.000000   \n",
      "1  pytorch                10.30           10548.75         -6.618314   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1         -6.642612         -6.630463  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8}\n",
      "  none Run time: 253.97 seconds\n",
      "  pytorch Run time: 385.98 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                 5.26           10779.30          0.000000   \n",
      "1  pytorch                 5.17           10588.03         -1.711027   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1         -1.774419         -1.742723  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8}\n",
      "  none Run time: 525.30 seconds\n",
      "  pytorch Run time: 650.01 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                 2.18            8932.60          0.000000   \n",
      "1  pytorch                 2.20            8992.21          0.917431   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1          0.667331          0.792381  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuning</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>92.71</td>\n",
       "      <td>11866.28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>55.85</td>\n",
       "      <td>7148.72</td>\n",
       "      <td>-39.758386</td>\n",
       "      <td>-39.756015</td>\n",
       "      <td>-39.757200</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>44.84</td>\n",
       "      <td>11480.29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>34.30</td>\n",
       "      <td>8781.02</td>\n",
       "      <td>-23.505798</td>\n",
       "      <td>-23.512211</td>\n",
       "      <td>-23.509005</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>22.60</td>\n",
       "      <td>11570.40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>19.74</td>\n",
       "      <td>10106.81</td>\n",
       "      <td>-12.654867</td>\n",
       "      <td>-12.649433</td>\n",
       "      <td>-12.652150</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>none</td>\n",
       "      <td>11.03</td>\n",
       "      <td>11299.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>10.30</td>\n",
       "      <td>10548.75</td>\n",
       "      <td>-6.618314</td>\n",
       "      <td>-6.642612</td>\n",
       "      <td>-6.630463</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>none</td>\n",
       "      <td>5.26</td>\n",
       "      <td>10779.30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.17</td>\n",
       "      <td>10588.03</td>\n",
       "      <td>-1.711027</td>\n",
       "      <td>-1.774419</td>\n",
       "      <td>-1.742723</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>none</td>\n",
       "      <td>2.18</td>\n",
       "      <td>8932.60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>2.20</td>\n",
       "      <td>8992.21</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>0.667331</td>\n",
       "      <td>0.792381</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0      none                92.71           11866.28          0.000000   \n",
       "1   pytorch                55.85            7148.72        -39.758386   \n",
       "2      none                44.84           11480.29          0.000000   \n",
       "3   pytorch                34.30            8781.02        -23.505798   \n",
       "4      none                22.60           11570.40          0.000000   \n",
       "5   pytorch                19.74           10106.81        -12.654867   \n",
       "6      none                11.03           11299.32          0.000000   \n",
       "7   pytorch                10.30           10548.75         -6.618314   \n",
       "8      none                 5.26           10779.30          0.000000   \n",
       "9   pytorch                 5.17           10588.03         -1.711027   \n",
       "10     none                 2.18            8932.60          0.000000   \n",
       "11  pytorch                 2.20            8992.21          0.917431   \n",
       "\n",
       "    % Difference TPS  % Difference Avg                              Config  \n",
       "0           0.000000          0.000000   input_len=0, output_len=128, tp=8  \n",
       "1         -39.756015        -39.757200   input_len=0, output_len=128, tp=8  \n",
       "2           0.000000          0.000000   input_len=0, output_len=256, tp=8  \n",
       "3         -23.512211        -23.509005   input_len=0, output_len=256, tp=8  \n",
       "4           0.000000          0.000000   input_len=0, output_len=512, tp=8  \n",
       "5         -12.649433        -12.652150   input_len=0, output_len=512, tp=8  \n",
       "6           0.000000          0.000000  input_len=0, output_len=1024, tp=8  \n",
       "7          -6.642612         -6.630463  input_len=0, output_len=1024, tp=8  \n",
       "8           0.000000          0.000000  input_len=0, output_len=2048, tp=8  \n",
       "9          -1.774419         -1.742723  input_len=0, output_len=2048, tp=8  \n",
       "10          0.000000          0.000000  input_len=0, output_len=4096, tp=8  \n",
       "11          0.667331          0.792381  input_len=0, output_len=4096, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6be3b999-10bd-47a3-bb44-d1ffe7bc6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 75.49 seconds\n",
      "  pytorch Run time: 409.22 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                83.43           21357.74          0.000000   \n",
      "1  pytorch                 4.64            1187.13        -94.438451   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -94.441687        -94.440069  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 77.80 seconds\n",
      "  pytorch Run time: 454.51 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                74.77           28710.44          0.000000   \n",
      "1  pytorch                 3.82            1465.84        -94.890999   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000            0.0000  \n",
      "1        -94.894401          -94.8927  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 81.04 seconds\n",
      "  pytorch Run time: 357.45 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                60.11           38472.05           0.00000   \n",
      "1  pytorch                 5.48            3505.06         -90.88338   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -90.889334        -90.886357  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 83.97 seconds\n",
      "  pytorch Run time: 382.80 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                47.30           54486.56          0.000000   \n",
      "1  pytorch                 5.27            6074.70        -88.858351   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -88.851012        -88.854682  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 95.70 seconds\n",
      "  pytorch Run time: 375.76 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                31.34           68196.24          0.000000   \n",
      "1  pytorch                 5.43           11815.98        -82.673899   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000           0.00000  \n",
      "1        -82.673561         -82.67373  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 117.67 seconds\n",
      "  pytorch Run time: 359.42 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                18.58           78467.08          0.000000   \n",
      "1  pytorch                 5.92           24995.44        -68.137783   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000           0.00000  \n",
      "1        -68.145316         -68.14155  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuning</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>83.43</td>\n",
       "      <td>21357.74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>4.64</td>\n",
       "      <td>1187.13</td>\n",
       "      <td>-94.438451</td>\n",
       "      <td>-94.441687</td>\n",
       "      <td>-94.440069</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>74.77</td>\n",
       "      <td>28710.44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>3.82</td>\n",
       "      <td>1465.84</td>\n",
       "      <td>-94.890999</td>\n",
       "      <td>-94.894401</td>\n",
       "      <td>-94.892700</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>60.11</td>\n",
       "      <td>38472.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.48</td>\n",
       "      <td>3505.06</td>\n",
       "      <td>-90.883380</td>\n",
       "      <td>-90.889334</td>\n",
       "      <td>-90.886357</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>none</td>\n",
       "      <td>47.30</td>\n",
       "      <td>54486.56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.27</td>\n",
       "      <td>6074.70</td>\n",
       "      <td>-88.858351</td>\n",
       "      <td>-88.851012</td>\n",
       "      <td>-88.854682</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>none</td>\n",
       "      <td>31.34</td>\n",
       "      <td>68196.24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.43</td>\n",
       "      <td>11815.98</td>\n",
       "      <td>-82.673899</td>\n",
       "      <td>-82.673561</td>\n",
       "      <td>-82.673730</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>none</td>\n",
       "      <td>18.58</td>\n",
       "      <td>78467.08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.92</td>\n",
       "      <td>24995.44</td>\n",
       "      <td>-68.137783</td>\n",
       "      <td>-68.145316</td>\n",
       "      <td>-68.141550</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0      none                83.43           21357.74          0.000000   \n",
       "1   pytorch                 4.64            1187.13        -94.438451   \n",
       "2      none                74.77           28710.44          0.000000   \n",
       "3   pytorch                 3.82            1465.84        -94.890999   \n",
       "4      none                60.11           38472.05          0.000000   \n",
       "5   pytorch                 5.48            3505.06        -90.883380   \n",
       "6      none                47.30           54486.56          0.000000   \n",
       "7   pytorch                 5.27            6074.70        -88.858351   \n",
       "8      none                31.34           68196.24          0.000000   \n",
       "9   pytorch                 5.43           11815.98        -82.673899   \n",
       "10     none                18.58           78467.08          0.000000   \n",
       "11  pytorch                 5.92           24995.44        -68.137783   \n",
       "\n",
       "    % Difference TPS  % Difference Avg                                Config  \n",
       "0           0.000000          0.000000   input_len=128, output_len=128, tp=8  \n",
       "1         -94.441687        -94.440069   input_len=128, output_len=128, tp=8  \n",
       "2           0.000000          0.000000   input_len=256, output_len=128, tp=8  \n",
       "3         -94.894401        -94.892700   input_len=256, output_len=128, tp=8  \n",
       "4           0.000000          0.000000   input_len=512, output_len=128, tp=8  \n",
       "5         -90.889334        -90.886357   input_len=512, output_len=128, tp=8  \n",
       "6           0.000000          0.000000  input_len=1024, output_len=128, tp=8  \n",
       "7         -88.851012        -88.854682  input_len=1024, output_len=128, tp=8  \n",
       "8           0.000000          0.000000  input_len=2048, output_len=128, tp=8  \n",
       "9         -82.673561        -82.673730  input_len=2048, output_len=128, tp=8  \n",
       "10          0.000000          0.000000  input_len=4096, output_len=128, tp=8  \n",
       "11        -68.145316        -68.141550  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fb3e9ac-3718-47e4-ad58-321311e428bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 76.46 seconds\n",
      "  pytorch Run time: 406.58 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                82.44           21103.54          0.000000   \n",
      "1  pytorch                 4.64            1187.83        -94.371664   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -94.371418        -94.371541  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 82.36 seconds\n",
      "  pytorch Run time: 456.11 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                74.92           28769.30          0.000000   \n",
      "1  pytorch                 3.80            1457.97        -94.927923   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -94.932202        -94.930063  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 80.87 seconds\n",
      "  pytorch Run time: 379.09 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                61.66           39460.56          0.000000   \n",
      "1  pytorch                 5.39            3449.60        -91.258514   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -91.258107        -91.258311  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 85.11 seconds\n",
      "  pytorch Run time: 376.16 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                47.14           54304.69          0.000000   \n",
      "1  pytorch                 5.27            6070.88        -88.820535   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -88.820708        -88.820621  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 95.16 seconds\n",
      "  pytorch Run time: 357.90 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                31.38           68279.47          0.000000   \n",
      "1  pytorch                 5.52           12021.28        -82.409178   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -82.394005        -82.401591  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "  none Run time: 120.61 seconds\n",
      "  pytorch Run time: 362.14 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                18.41           77744.75          0.000000   \n",
      "1  pytorch                 5.88           24854.66        -68.060837   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -68.030433        -68.045635  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuning</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>82.44</td>\n",
       "      <td>21103.54</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>4.64</td>\n",
       "      <td>1187.83</td>\n",
       "      <td>-94.371664</td>\n",
       "      <td>-94.371418</td>\n",
       "      <td>-94.371541</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>74.92</td>\n",
       "      <td>28769.30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1457.97</td>\n",
       "      <td>-94.927923</td>\n",
       "      <td>-94.932202</td>\n",
       "      <td>-94.930063</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>61.66</td>\n",
       "      <td>39460.56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3449.60</td>\n",
       "      <td>-91.258514</td>\n",
       "      <td>-91.258107</td>\n",
       "      <td>-91.258311</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>none</td>\n",
       "      <td>47.14</td>\n",
       "      <td>54304.69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.27</td>\n",
       "      <td>6070.88</td>\n",
       "      <td>-88.820535</td>\n",
       "      <td>-88.820708</td>\n",
       "      <td>-88.820621</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>none</td>\n",
       "      <td>31.38</td>\n",
       "      <td>68279.47</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.52</td>\n",
       "      <td>12021.28</td>\n",
       "      <td>-82.409178</td>\n",
       "      <td>-82.394005</td>\n",
       "      <td>-82.401591</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>none</td>\n",
       "      <td>18.41</td>\n",
       "      <td>77744.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>5.88</td>\n",
       "      <td>24854.66</td>\n",
       "      <td>-68.060837</td>\n",
       "      <td>-68.030433</td>\n",
       "      <td>-68.045635</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0      none                82.44           21103.54          0.000000   \n",
       "1   pytorch                 4.64            1187.83        -94.371664   \n",
       "2      none                74.92           28769.30          0.000000   \n",
       "3   pytorch                 3.80            1457.97        -94.927923   \n",
       "4      none                61.66           39460.56          0.000000   \n",
       "5   pytorch                 5.39            3449.60        -91.258514   \n",
       "6      none                47.14           54304.69          0.000000   \n",
       "7   pytorch                 5.27            6070.88        -88.820535   \n",
       "8      none                31.38           68279.47          0.000000   \n",
       "9   pytorch                 5.52           12021.28        -82.409178   \n",
       "10     none                18.41           77744.75          0.000000   \n",
       "11  pytorch                 5.88           24854.66        -68.060837   \n",
       "\n",
       "    % Difference TPS  % Difference Avg                                Config  \n",
       "0           0.000000          0.000000   input_len=128, output_len=128, tp=8  \n",
       "1         -94.371418        -94.371541   input_len=128, output_len=128, tp=8  \n",
       "2           0.000000          0.000000   input_len=256, output_len=128, tp=8  \n",
       "3         -94.932202        -94.930063   input_len=256, output_len=128, tp=8  \n",
       "4           0.000000          0.000000   input_len=512, output_len=128, tp=8  \n",
       "5         -91.258107        -91.258311   input_len=512, output_len=128, tp=8  \n",
       "6           0.000000          0.000000  input_len=1024, output_len=128, tp=8  \n",
       "7         -88.820708        -88.820621  input_len=1024, output_len=128, tp=8  \n",
       "8           0.000000          0.000000  input_len=2048, output_len=128, tp=8  \n",
       "9         -82.394005        -82.401591  input_len=2048, output_len=128, tp=8  \n",
       "10          0.000000          0.000000  input_len=4096, output_len=128, tp=8  \n",
       "11        -68.030433        -68.045635  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Second time...\n",
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c54bbe-bae6-497a-91eb-b121675e48f1",
   "metadata": {},
   "source": [
    "Random outputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40676fe0-1b9c-49e9-af48-ce75e3c3efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8}\n",
      "  none Run time: 78.27 seconds\n",
      "  pytorch Run time: 411.14 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                80.99           21218.48          0.000000   \n",
      "1  pytorch                 4.57            1196.45        -94.357328   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -94.361283        -94.359306  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8}\n",
      "  none Run time: 298.03 seconds\n",
      "  pytorch Run time: 893.04 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                 4.28           17104.94          0.000000   \n",
      "1  pytorch                 1.43            5726.42        -66.588785   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000           0.00000  \n",
      "1        -66.521835         -66.55531  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8}\n",
      "  none Run time: 304.83 seconds\n",
      "  pytorch Run time: 558.05 seconds\n",
      "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0     none                 4.19           17153.80          0.000000   \n",
      "1  pytorch                 2.62           10723.66        -37.470167   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.000000          0.000000  \n",
      "1        -37.485222        -37.477694  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuning</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>80.99</td>\n",
       "      <td>21218.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>4.57</td>\n",
       "      <td>1196.45</td>\n",
       "      <td>-94.357328</td>\n",
       "      <td>-94.361283</td>\n",
       "      <td>-94.359306</td>\n",
       "      <td>input_len=131, output_len=131, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>4.28</td>\n",
       "      <td>17104.94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=2000, output_len=2000, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>1.43</td>\n",
       "      <td>5726.42</td>\n",
       "      <td>-66.588785</td>\n",
       "      <td>-66.521835</td>\n",
       "      <td>-66.555310</td>\n",
       "      <td>input_len=2000, output_len=2000, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>4.19</td>\n",
       "      <td>17153.80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=2048, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>2.62</td>\n",
       "      <td>10723.66</td>\n",
       "      <td>-37.470167</td>\n",
       "      <td>-37.485222</td>\n",
       "      <td>-37.477694</td>\n",
       "      <td>input_len=2048, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tuning  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0     none                80.99           21218.48          0.000000   \n",
       "1  pytorch                 4.57            1196.45        -94.357328   \n",
       "2     none                 4.28           17104.94          0.000000   \n",
       "3  pytorch                 1.43            5726.42        -66.588785   \n",
       "4     none                 4.19           17153.80          0.000000   \n",
       "5  pytorch                 2.62           10723.66        -37.470167   \n",
       "\n",
       "   % Difference TPS  % Difference Avg                                 Config  \n",
       "0          0.000000          0.000000    input_len=131, output_len=131, tp=8  \n",
       "1        -94.361283        -94.359306    input_len=131, output_len=131, tp=8  \n",
       "2          0.000000          0.000000  input_len=2000, output_len=2000, tp=8  \n",
       "3        -66.521835        -66.555310  input_len=2000, output_len=2000, tp=8  \n",
       "4          0.000000          0.000000  input_len=2048, output_len=2048, tp=8  \n",
       "5        -37.485222        -37.477694  input_len=2048, output_len=2048, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 131, 'output_len': 131, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2000, 'output_len': 2000, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 2048, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0548742-a8f4-4918-b04d-4d46d062d070",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Based on these results we are seeing huge overhead, no improvements at all when turning on TunableOps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65103228-8f57-4d08-8744-3636c32d8bea",
   "metadata": {},
   "source": [
    "# gradlib GEMM Tuning\n",
    "- https://rocm.blogs.amd.com/artificial-intelligence/vllm-optimize/README.html#gemm-tuning\n",
    "- https://rocm.blogs.amd.com/artificial-intelligence/pytorch-tunableop/README.html\n",
    "- https://www.nscale.com/blog/nscale-benchmarks-amd-mi300x-gpus-with-gemm-tuning-improves-throughput-and-latency-by-up-to-7-2x\n",
    "\n",
    "There's surprisingly little documentation on using gradlib (part of ROCm/vllm)\n",
    "- https://rocm.docs.amd.com/en/latest/how-to/tuning-guides/mi300x/workload.html#gemm-tuning-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5952df-ab14-4777-8c56-033453eb0a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#VLLM_UNTUNE_FILE=\"untuned-in128-out128.csv\" VLLM_TUNE_GEMM=1 VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.1-8B-Instruct -tp 8 --input-len 128 --output-len 128\n",
    "#python ~/vllm-rocm/gradlib/gradlib/gemm_tuner.py --input untuned-in128-out128.csv --tuned_file tuned-in128-out128.csv --tp 8  --nsets 1 \n",
    "\n",
    "# && for tp in 1 1 2 2 4 4 8 8 128 128 256 256; do VLLM_TUNE_FILE=\"tuned_llama3_8B_B${tp}.csv\" VLLM_TUNE_GEMM=0 HIP_VISIBLE_DEVICES=2,3,4,5,6,7 VLLM_USE_TRITON_FLASH_ATTN=0 python benchmark_throughput_prompt.py --model /models/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/ -tp 1 --num-prompts $tp --input-len 1024 --output-len 128 --prompt \"Write a poem about a black cat\" > ${tp}PromptsL3_8BGEMM; done 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fac50a6-8c95-4a9b-87c5-d8530734bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLLM variables\n",
    "# export VLLM_UNTUNE_FILE=\"/tmp/vllm_untuned.csv\"\n",
    "# export VLLM_TUNE_FILE=\"$(pwd)/tuned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d9476d3-8194-4bda-a195-83fe7d623182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 19:01:05 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=15, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 19:01:14 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-30 19:01:14 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-30 19:01:14 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 19:01:14 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=15, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 19:01:15 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 19:01:15 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 19:01:15 selector.py:120] Using ROCmFlashAttention backend.\n",
      "WARNING 10-30 19:01:15 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m WARNING 10-30 19:01:20 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:20 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m WARNING 10-30 19:01:20 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:20 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m WARNING 10-30 19:01:20 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:20 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m WARNING 10-30 19:01:20 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:20 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m WARNING 10-30 19:01:20 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:20 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m WARNING 10-30 19:01:20 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:20 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m WARNING 10-30 19:01:20 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:20 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:20 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 19:01:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 19:01:21 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f1ab16df610>, local_subscribe_port=51969, remote_subscribe_port=None)\n",
      "INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:21 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 19:01:21 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 19:01:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:22 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:22 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:22 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:22 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.62it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.05it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.50it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.81it/s]\n",
      "\n",
      "INFO 10-30 19:01:23 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:23 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:23 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:23 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:23 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:24 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:24 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:24 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 19:01:39 distributed_gpu_executor.py:57] # GPU blocks: 647379, # CPU blocks: 16384\n",
      "INFO 10-30 19:01:39 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 79.03x\n",
      "INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:01:56 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:07<00:00, 142.85it/s, est. speed input:\n",
      "INFO 10-30 19:02:09 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213295)\u001b[0;0m INFO 10-30 19:02:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213294)\u001b[0;0m INFO 10-30 19:02:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213297)\u001b[0;0m INFO 10-30 19:02:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213299)\u001b[0;0m INFO 10-30 19:02:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213296)\u001b[0;0m INFO 10-30 19:02:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213293)\u001b[0;0m INFO 10-30 19:02:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3213298)\u001b[0;0m INFO 10-30 19:02:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 139.54 requests/s, 35722.79 tokens/s\n",
      "[rank0]:[W1030 19:02:11.224264932 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Generate vLLM untuned files\n",
    "!VLLM_UNTUNE_FILE=\"/tmp/vllm_untuned.csv\" VLLM_TUNE_GEMM=1 VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --num-scheduler-steps 15 --model meta-llama/Llama-3.1-8B-Instruct --input-len 128 --output-len 128 -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11fbbb6c-1466-451f-bb39-50705233d3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading /tmp/vllm_untuned.csv\n",
      "M N K dtype 768 131072 4096 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 768 131072 4096 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283192  1.529309\n",
      "621283627  1.540934\n",
      "621283196  1.543761\n",
      "621286477  1.544263\n",
      "621283639  1.548452\n",
      "621283198  1.554225\n",
      "621283177  1.568658\n",
      "621283552  1.570802\n",
      "621286458  1.571945\n",
      "621283554  1.572868\n",
      "621283625  1.577277\n",
      "621283180  1.577778\n",
      "621283200  1.582008\n",
      "621283637  1.588503\n",
      "621283634  1.590768\n",
      "621283553  1.593213\n",
      "621283442  1.596040\n",
      "621283628  1.596080\n",
      "621283444  1.597884\n",
      "621286461  1.599066\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13908  1.478093\n",
      "67575  1.561321\n",
      "13828  1.568498\n",
      "13737  1.577077\n",
      "13829  1.578520\n",
      "68900  1.586218\n",
      "67582  1.600169\n",
      "13887  1.605301\n",
      "67589  1.614321\n",
      "67224  1.631921\n",
      "13911  1.634808\n",
      "13792  1.650984\n",
      "13820  1.654533\n",
      "13862  1.655875\n",
      "67621  1.660607\n",
      "13819  1.661909\n",
      "13824  1.677765\n",
      "13825  1.679569\n",
      "67543  1.681173\n",
      "13888  1.682496\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283627  1.549446\n",
      "621283192  1.555808\n",
      "621286477  1.557805\n",
      "621283196  1.576377\n",
      "621283639  1.583650\n",
      "621283554  1.586273\n",
      "621283625  1.587615\n",
      "621286458  1.588027\n",
      "621283552  1.590738\n",
      "621283200  1.595653\n",
      "621283198  1.602921\n",
      "621283177  1.605062\n",
      "621283637  1.606594\n",
      "621283628  1.606700\n",
      "621283634  1.610232\n",
      "621283553  1.613247\n",
      "621283444  1.616177\n",
      "621286461  1.620639\n",
      "621283180  1.628343\n",
      "621283442  1.630957\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13911  1.568357\n",
      "13908  1.569664\n",
      "68900  1.571620\n",
      "13828  1.573368\n",
      "67575  1.589220\n",
      "13888  1.589527\n",
      "13887  1.594973\n",
      "13862  1.596134\n",
      "13829  1.599081\n",
      "67589  1.604078\n",
      "67582  1.613862\n",
      "67621  1.638588\n",
      "67224  1.642545\n",
      "13820  1.652867\n",
      "13792  1.655440\n",
      "13824  1.671820\n",
      "13819  1.676923\n",
      "13737  1.678569\n",
      "67543  1.681490\n",
      "13825  1.691408\n",
      ">>> Fastest Solution is rocblas 621283627 1.5494460105895995\n",
      "M N K dtype 4096 131072 512 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 4096 131072 512 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283628  1.222573\n",
      "621283525  1.222633\n",
      "621283520  1.225419\n",
      "621283265  1.225479\n",
      "621283560  1.227945\n",
      "621283521  1.230090\n",
      "621283627  1.231232\n",
      "621283284  1.231934\n",
      "621283634  1.233738\n",
      "621283192  1.235643\n",
      "621283285  1.237286\n",
      "621283519  1.238208\n",
      "621283637  1.241114\n",
      "621286478  1.241476\n",
      "621286477  1.241536\n",
      "621283517  1.242177\n",
      "621283433  1.243961\n",
      "621283522  1.244743\n",
      "621283613  1.245906\n",
      "621283553  1.247529\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13828  1.235803\n",
      "13837  1.268216\n",
      "13869  1.270702\n",
      "13829  1.284173\n",
      "13887  1.285040\n",
      "13836  1.286562\n",
      "13874  1.289725\n",
      "13823  1.290346\n",
      "13888  1.299547\n",
      "13824  1.300509\n",
      "13826  1.315583\n",
      "13908  1.319407\n",
      "13820  1.325547\n",
      "13758  1.339177\n",
      "13819  1.342966\n",
      "13760  1.343688\n",
      "13827  1.358842\n",
      "13862  1.359443\n",
      "68900  1.367321\n",
      "13903  1.369927\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283521  1.227480\n",
      "621286477  1.231112\n",
      "621283628  1.231405\n",
      "621283637  1.231475\n",
      "621283519  1.231874\n",
      "621283285  1.232022\n",
      "621283284  1.232549\n",
      "621283192  1.236394\n",
      "621283634  1.236480\n",
      "621283525  1.236957\n",
      "621283560  1.241193\n",
      "621283517  1.247988\n",
      "621283613  1.251290\n",
      "621286478  1.251562\n",
      "621283520  1.255714\n",
      "621283627  1.267839\n",
      "621283433  1.267857\n",
      "621283522  1.280727\n",
      "621283553  1.292281\n",
      "621283265  1.304274\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13837  1.251879\n",
      "13828  1.252178\n",
      "13874  1.255379\n",
      "13869  1.257935\n",
      "13824  1.262860\n",
      "13908  1.268559\n",
      "13829  1.272153\n",
      "13836  1.273099\n",
      "13888  1.275044\n",
      "13826  1.292078\n",
      "13823  1.303252\n",
      "13758  1.312449\n",
      "13760  1.316017\n",
      "13887  1.322602\n",
      "13862  1.323762\n",
      "13820  1.341402\n",
      "13819  1.344622\n",
      "13903  1.344996\n",
      "68900  1.375606\n",
      "13827  1.381621\n",
      ">>> Fastest Solution is rocblas 621283521 1.2274799346923828\n",
      "M N K dtype 3584 131072 4096 torch.bfloat16 >>> Total rocb solutions 390\n",
      "M N K bias dtype 3584 131072 4096 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283284  6.725453\n",
      "621283445  6.739986\n",
      "621283439  6.760532\n",
      "621283556  6.760553\n",
      "621283449  6.777150\n",
      "621286477  6.782904\n",
      "621283636  6.786451\n",
      "621283192  6.790700\n",
      "621283632  6.803650\n",
      "621283627  6.808902\n",
      "621283652  6.814916\n",
      "621283200  6.818023\n",
      "621283634  6.831273\n",
      "621283452  6.838690\n",
      "621283196  6.839652\n",
      "621283554  6.842298\n",
      "621283628  6.842919\n",
      "621283644  6.845386\n",
      "621283557  6.845445\n",
      "621283198  6.845946\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13828  6.911996\n",
      "13829  6.912397\n",
      "13840  6.983538\n",
      "13824  7.020964\n",
      "13869  7.039746\n",
      "13823  7.059090\n",
      "13822  7.101526\n",
      "13821  7.106497\n",
      "13780  7.232303\n",
      "13758  7.235569\n",
      "13874  7.310340\n",
      "13819  7.394471\n",
      "13820  7.400544\n",
      "13888  7.401446\n",
      "13837  7.401987\n",
      "13792  7.410427\n",
      "13908  7.461041\n",
      "13836  7.485497\n",
      "13900  7.501193\n",
      "13825  7.550464\n",
      ">>> rocblas Solidx 621283284 FAILED reference test\n",
      ">>> rocblas Solidx 621283439 FAILED reference test\n",
      ">>> rocblas Solidx 621283556 FAILED reference test\n",
      ">>> rocblas Solidx 621283449 FAILED reference test\n",
      ">>> rocblas Solidx 621286477 FAILED reference test\n",
      ">>> rocblas Solidx 621283636 FAILED reference test\n",
      ">>> rocblas Solidx 621283192 FAILED reference test\n",
      ">>> rocblas Solidx 621283632 FAILED reference test\n",
      ">>> rocblas Solidx 621283627 FAILED reference test\n",
      ">>> rocblas Solidx 621283200 FAILED reference test\n",
      ">>> rocblas Solidx 621283634 FAILED reference test\n",
      ">>> rocblas Solidx 621283452 FAILED reference test\n",
      ">>> rocblas Solidx 621283196 FAILED reference test\n",
      ">>> rocblas Solidx 621283628 FAILED reference test\n",
      ">>> rocblas Solidx 621283557 FAILED reference test\n",
      ">>> rocblas Solidx 621283198 FAILED reference test\n",
      ">>> hipblaslt Solidx 13828 FAILED reference test\n",
      ">>> hipblaslt Solidx 13829 FAILED reference test\n",
      ">>> hipblaslt Solidx 13840 FAILED reference test\n",
      ">>> hipblaslt Solidx 13824 FAILED reference test\n",
      ">>> hipblaslt Solidx 13869 FAILED reference test\n",
      ">>> hipblaslt Solidx 13823 FAILED reference test\n",
      ">>> hipblaslt Solidx 13822 FAILED reference test\n",
      ">>> hipblaslt Solidx 13821 FAILED reference test\n",
      ">>> hipblaslt Solidx 13780 FAILED reference test\n",
      ">>> hipblaslt Solidx 13758 FAILED reference test\n",
      ">>> hipblaslt Solidx 13874 FAILED reference test\n",
      ">>> hipblaslt Solidx 13819 FAILED reference test\n",
      ">>> hipblaslt Solidx 13820 FAILED reference test\n",
      ">>> hipblaslt Solidx 13888 FAILED reference test\n",
      ">>> hipblaslt Solidx 13837 FAILED reference test\n",
      ">>> hipblaslt Solidx 13792 FAILED reference test\n",
      ">>> hipblaslt Solidx 13908 FAILED reference test\n",
      ">>> hipblaslt Solidx 13900 FAILED reference test\n",
      ">>> hipblaslt Solidx 13825 FAILED reference test\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283445  6.910030\n",
      "621283652  6.918910\n",
      "621283554  6.982835\n",
      "621283644  6.999799\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13836  7.573629\n",
      ">>> Fastest Solution is rocblas 621283445 6.910029602050781\n",
      "M N K dtype 4096 131072 1792 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 4096 131072 1792 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283284  3.578055\n",
      "621283197  3.581162\n",
      "621283593  3.603353\n",
      "621283634  3.609566\n",
      "621283488  3.614197\n",
      "621283286  3.619308\n",
      "621283192  3.623879\n",
      "621283642  3.625082\n",
      "621283628  3.628349\n",
      "621283627  3.629291\n",
      "621283643  3.634062\n",
      "621283602  3.652844\n",
      "621283641  3.665633\n",
      "621283614  3.673371\n",
      "621286477  3.675496\n",
      "621283200  3.684637\n",
      "621283285  3.686641\n",
      "621283198  3.688847\n",
      "621283177  3.694740\n",
      "621283637  3.694860\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13837  3.629251\n",
      "13836  3.645087\n",
      "13874  3.645247\n",
      "13792  3.725430\n",
      "13828  3.731483\n",
      "13829  3.754856\n",
      "13726  3.756078\n",
      "13737  3.763114\n",
      "13758  3.768126\n",
      "68900  3.774941\n",
      "13824  3.775242\n",
      "13900  3.798695\n",
      "13862  3.802925\n",
      "67589  3.806533\n",
      "13869  3.810903\n",
      "13832  3.849490\n",
      "13908  3.863924\n",
      "13820  3.864866\n",
      "67342  3.885332\n",
      "13819  3.887938\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283284  3.532712\n",
      "621283593  3.544291\n",
      "621283197  3.549713\n",
      "621283602  3.551465\n",
      "621283643  3.561273\n",
      "621286477  3.572591\n",
      "621283488  3.574186\n",
      "621283634  3.574471\n",
      "621283637  3.579703\n",
      "621283286  3.583385\n",
      "621283642  3.584512\n",
      "621283628  3.590904\n",
      "621283192  3.591177\n",
      "621283627  3.592197\n",
      "621283641  3.600097\n",
      "621283285  3.608422\n",
      "621283200  3.611311\n",
      "621283198  3.629404\n",
      "621283614  3.653630\n",
      "621283177  3.696624\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13837  3.573178\n",
      "13874  3.580125\n",
      "13869  3.595918\n",
      "13836  3.612182\n",
      "13792  3.640918\n",
      "13828  3.651798\n",
      "13726  3.687717\n",
      "13832  3.689233\n",
      "13824  3.705566\n",
      "13829  3.710221\n",
      "13758  3.712193\n",
      "13862  3.720333\n",
      "13737  3.725407\n",
      "13900  3.768214\n",
      "67589  3.782533\n",
      "68900  3.804392\n",
      "67342  3.837766\n",
      "13820  3.859592\n",
      "13819  3.869294\n",
      "13908  3.915434\n",
      ">>> Fastest Solution is rocblas 621283284 3.5327121734619142\n",
      "M N K dtype 16032 256 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 16032 256 4096 False torch.bfloat16 >>> Total hipb solutions 989\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283374  0.103690\n",
      "621283411  0.108607\n",
      "621283409  0.109694\n",
      "621283240  0.109694\n",
      "621283410  0.109990\n",
      "621283540  0.110210\n",
      "621283537  0.112751\n",
      "621283412  0.113057\n",
      "621283377  0.113322\n",
      "621283375  0.113808\n",
      "621283408  0.115642\n",
      "621283405  0.116544\n",
      "621283379  0.116585\n",
      "621283539  0.117767\n",
      "621283378  0.117867\n",
      "621283376  0.118028\n",
      "621283538  0.119115\n",
      "621283536  0.119310\n",
      "621283418  0.120674\n",
      "621283407  0.120784\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13928  0.109087\n",
      "13885  0.110580\n",
      "13901  0.113558\n",
      "13854  0.114860\n",
      "13891  0.114901\n",
      "67142  0.115672\n",
      "13784  0.117386\n",
      "13783  0.117572\n",
      "67527  0.118148\n",
      "67602  0.118559\n",
      "13902  0.118799\n",
      "13756  0.121035\n",
      "67594  0.121180\n",
      "13856  0.121330\n",
      "67544  0.121476\n",
      "13903  0.124362\n",
      "13851  0.125139\n",
      "67919  0.125264\n",
      "67505  0.125490\n",
      "13772  0.125711\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283537  0.094783\n",
      "621283377  0.097236\n",
      "621283376  0.097934\n",
      "621283407  0.100823\n",
      "621283408  0.101212\n",
      "621283374  0.101444\n",
      "621283538  0.101883\n",
      "621283405  0.102515\n",
      "621283536  0.103256\n",
      "621283240  0.106544\n",
      "621283409  0.107460\n",
      "621283375  0.107606\n",
      "621283540  0.110132\n",
      "621283411  0.111268\n",
      "621283410  0.116280\n",
      "621283378  0.118717\n",
      "621283412  0.120281\n",
      "621283379  0.121030\n",
      "621283418  0.124957\n",
      "621283539  0.126218\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13783  0.095503\n",
      "13902  0.095788\n",
      "13885  0.096984\n",
      "13901  0.100458\n",
      "67594  0.105670\n",
      "13784  0.106226\n",
      "67505  0.108977\n",
      "67142  0.114303\n",
      "67919  0.114422\n",
      "13928  0.119942\n",
      "67527  0.119974\n",
      "13851  0.120220\n",
      "13756  0.122646\n",
      "13854  0.122817\n",
      "67602  0.122859\n",
      "13772  0.122879\n",
      "67544  0.124845\n",
      "13903  0.128295\n",
      "13856  0.130622\n",
      "13891  0.135165\n",
      ">>> Fastest Solution is rocblas 621283537 0.09478300213813781\n",
      "M N K dtype 768 256 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 256 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283244  0.018623\n",
      "621283393  0.019705\n",
      "621283392  0.019930\n",
      "621283530  0.019946\n",
      "621283246  0.020186\n",
      "621283319  0.021008\n",
      "621283321  0.021128\n",
      "621283323  0.021349\n",
      "621283328  0.021368\n",
      "621283325  0.021609\n",
      "621283245  0.021629\n",
      "621283330  0.021669\n",
      "621283334  0.021750\n",
      "621283320  0.021930\n",
      "621283395  0.022091\n",
      "621283322  0.022130\n",
      "621283257  0.022411\n",
      "621283329  0.022411\n",
      "621283324  0.022932\n",
      "621283327  0.023053\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13884  0.023674\n",
      "67757  0.023794\n",
      "13852  0.023834\n",
      "13740  0.024135\n",
      "13882  0.024295\n",
      "13912  0.024531\n",
      "13841  0.024696\n",
      "13896  0.024696\n",
      "13907  0.024857\n",
      "13746  0.025177\n",
      "36974  0.025257\n",
      "13892  0.025277\n",
      "13883  0.025398\n",
      "13894  0.025398\n",
      "13793  0.025428\n",
      "13845  0.025518\n",
      "13886  0.025578\n",
      "67694  0.025583\n",
      "13848  0.025798\n",
      "66941  0.025879\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.015259\n",
      "621283246  0.015629\n",
      "621283323  0.016177\n",
      "621283334  0.016189\n",
      "621283322  0.016880\n",
      "621283321  0.016944\n",
      "621283320  0.017053\n",
      "621283319  0.017444\n",
      "621283530  0.017580\n",
      "621283392  0.017700\n",
      "621283328  0.018730\n",
      "621283330  0.018827\n",
      "621283325  0.018867\n",
      "621283329  0.019286\n",
      "621283395  0.019552\n",
      "621283257  0.020128\n",
      "621283324  0.020452\n",
      "621283327  0.020761\n",
      "621283393  0.024886\n",
      "621283244  0.026284\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13845  0.020388\n",
      "13896  0.020723\n",
      "13907  0.020818\n",
      "13841  0.021020\n",
      "13912  0.021101\n",
      "13746  0.021202\n",
      "13883  0.021469\n",
      "13894  0.021803\n",
      "13848  0.021886\n",
      "13793  0.021888\n",
      "13882  0.022054\n",
      "13852  0.022054\n",
      "13892  0.022200\n",
      "13886  0.022345\n",
      "13740  0.022655\n",
      "67694  0.023341\n",
      "66941  0.023926\n",
      "36974  0.023984\n",
      "67757  0.025931\n",
      "13884  0.034745\n",
      ">>> Fastest Solution is rocblas 621283245 0.015258599817752839\n",
      "M N K dtype 4096 256 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 256 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283379  0.018101\n",
      "621283564  0.018181\n",
      "621283411  0.018261\n",
      "621283541  0.018321\n",
      "621283255  0.018462\n",
      "621283418  0.018522\n",
      "621283410  0.018623\n",
      "621283417  0.018642\n",
      "621283261  0.018823\n",
      "621283423  0.018843\n",
      "621283482  0.018863\n",
      "621283266  0.018943\n",
      "621283561  0.018983\n",
      "621283415  0.019083\n",
      "621283264  0.019103\n",
      "621283620  0.019264\n",
      "621283256  0.019264\n",
      "621283263  0.019304\n",
      "621283413  0.019384\n",
      "621283540  0.019404\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67602  0.022271\n",
      "67410  0.022672\n",
      "67536  0.023093\n",
      "68873  0.023213\n",
      "66899  0.023253\n",
      "67609  0.023454\n",
      "68893  0.023454\n",
      "67162  0.023553\n",
      "67361  0.023594\n",
      "67170  0.023754\n",
      "13918  0.023935\n",
      "66880  0.023994\n",
      "67280  0.024035\n",
      "66849  0.024195\n",
      "67244  0.024255\n",
      "68102  0.024335\n",
      "13772  0.024475\n",
      "66933  0.024516\n",
      "66848  0.024556\n",
      "13793  0.024556\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283417  0.015349\n",
      "621283263  0.015495\n",
      "621283411  0.015740\n",
      "621283418  0.015872\n",
      "621283415  0.016417\n",
      "621283264  0.016459\n",
      "621283410  0.016467\n",
      "621283620  0.016544\n",
      "621283482  0.016618\n",
      "621283256  0.016720\n",
      "621283561  0.016756\n",
      "621283266  0.016766\n",
      "621283423  0.016834\n",
      "621283255  0.017013\n",
      "621283261  0.017067\n",
      "621283541  0.017339\n",
      "621283413  0.017359\n",
      "621283540  0.018057\n",
      "621283564  0.020897\n",
      "621283379  0.025147\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67162  0.020462\n",
      "66899  0.020962\n",
      "68102  0.021409\n",
      "13918  0.021427\n",
      "67536  0.021457\n",
      "13793  0.021743\n",
      "66880  0.022196\n",
      "68873  0.022204\n",
      "67244  0.022228\n",
      "66933  0.022431\n",
      "67361  0.022455\n",
      "67410  0.022607\n",
      "67280  0.022685\n",
      "67170  0.022814\n",
      "68893  0.022976\n",
      "66848  0.023000\n",
      "67609  0.023112\n",
      "66849  0.023297\n",
      "13772  0.023389\n",
      "67602  0.030806\n",
      ">>> Fastest Solution is rocblas 621283417 0.015348799526691437\n",
      "M N K dtype 3584 256 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 256 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283329  0.033030\n",
      "621283330  0.035351\n",
      "621283328  0.035480\n",
      "621283333  0.037806\n",
      "621283332  0.038357\n",
      "621283365  0.039770\n",
      "621283398  0.039790\n",
      "621283364  0.039970\n",
      "621283342  0.040011\n",
      "621283390  0.040031\n",
      "621283397  0.040131\n",
      "621283327  0.040362\n",
      "621283363  0.040792\n",
      "621283331  0.041088\n",
      "621283339  0.041795\n",
      "621283334  0.041870\n",
      "621283325  0.042285\n",
      "621283366  0.042537\n",
      "621283388  0.042632\n",
      "621283396  0.042998\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13873  0.037404\n",
      "13863  0.039540\n",
      "13852  0.040346\n",
      "13849  0.040392\n",
      "13876  0.040612\n",
      "13896  0.040848\n",
      "13875  0.041249\n",
      "13853  0.042176\n",
      "13844  0.042657\n",
      "13884  0.042666\n",
      "13759  0.042857\n",
      "13883  0.042872\n",
      "13851  0.042918\n",
      "13860  0.042957\n",
      "13897  0.043514\n",
      "67869  0.044060\n",
      "13856  0.044400\n",
      "67919  0.044486\n",
      "67903  0.044561\n",
      "13779  0.045123\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.029707\n",
      "621283332  0.029721\n",
      "621283330  0.031960\n",
      "621283327  0.032232\n",
      "621283325  0.034278\n",
      "621283388  0.034747\n",
      "621283329  0.035599\n",
      "621283331  0.035904\n",
      "621283333  0.036619\n",
      "621283390  0.038507\n",
      "621283364  0.038834\n",
      "621283363  0.039580\n",
      "621283366  0.040135\n",
      "621283339  0.041221\n",
      "621283342  0.041749\n",
      "621283397  0.041751\n",
      "621283365  0.042346\n",
      "621283396  0.042470\n",
      "621283398  0.042727\n",
      "621283334  0.042731\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13852  0.031438\n",
      "13883  0.033604\n",
      "13897  0.033726\n",
      "13884  0.035085\n",
      "13853  0.035731\n",
      "13863  0.036306\n",
      "13896  0.036937\n",
      "13851  0.039770\n",
      "13779  0.040250\n",
      "13860  0.040546\n",
      "67869  0.040706\n",
      "13875  0.040839\n",
      "13856  0.040913\n",
      "67903  0.041119\n",
      "13849  0.041358\n",
      "13844  0.041402\n",
      "13876  0.041510\n",
      "67919  0.042228\n",
      "13759  0.043302\n",
      "13873  0.048015\n",
      ">>> Fastest Solution is rocblas 621283328 0.029707351326942445\n",
      "M N K dtype 4096 256 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 256 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283365  0.020687\n",
      "621283329  0.020938\n",
      "621283335  0.020987\n",
      "621283333  0.021078\n",
      "621283341  0.021269\n",
      "621283325  0.021454\n",
      "621283324  0.021599\n",
      "621283339  0.021809\n",
      "621283340  0.022231\n",
      "621283532  0.022676\n",
      "621283338  0.022857\n",
      "621283617  0.023117\n",
      "621283337  0.023132\n",
      "621283391  0.023434\n",
      "621283330  0.023558\n",
      "621283328  0.023573\n",
      "621283388  0.023729\n",
      "621283366  0.023734\n",
      "621283332  0.023804\n",
      "621283342  0.023874\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13856  0.023779\n",
      "13791  0.024741\n",
      "13849  0.024912\n",
      "67895  0.025102\n",
      "13897  0.025192\n",
      "13860  0.025242\n",
      "13892  0.025313\n",
      "13746  0.025448\n",
      "13779  0.025639\n",
      "13759  0.026119\n",
      "13790  0.026184\n",
      "13781  0.026601\n",
      "13771  0.026741\n",
      "13876  0.026741\n",
      "67903  0.026746\n",
      "13894  0.026956\n",
      "67916  0.026966\n",
      "13864  0.027317\n",
      "13915  0.027422\n",
      "13916  0.027502\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283329  0.016630\n",
      "621283330  0.017408\n",
      "621283328  0.017443\n",
      "621283388  0.017794\n",
      "621283332  0.018205\n",
      "621283339  0.019053\n",
      "621283324  0.019183\n",
      "621283333  0.019528\n",
      "621283338  0.019759\n",
      "621283340  0.019849\n",
      "621283366  0.019935\n",
      "621283337  0.020186\n",
      "621283391  0.020222\n",
      "621283325  0.020445\n",
      "621283342  0.020639\n",
      "621283341  0.020795\n",
      "621283617  0.020925\n",
      "621283532  0.020985\n",
      "621283335  0.021274\n",
      "621283365  0.027749\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13864  0.021877\n",
      "13860  0.021958\n",
      "13849  0.022208\n",
      "13876  0.022389\n",
      "13915  0.022467\n",
      "13897  0.022545\n",
      "67895  0.023053\n",
      "67903  0.023175\n",
      "67916  0.023245\n",
      "13894  0.023340\n",
      "13779  0.023400\n",
      "13892  0.023614\n",
      "13746  0.023927\n",
      "13916  0.024016\n",
      "13791  0.024692\n",
      "13759  0.024808\n",
      "13790  0.025308\n",
      "13856  0.027935\n",
      "13771  0.029535\n",
      "13781  0.030106\n",
      ">>> Fastest Solution is rocblas 621283329 0.016629700362682343\n",
      "M N K dtype 768 248 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 248 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.019905\n",
      "621283246  0.020086\n",
      "621283244  0.020165\n",
      "621283392  0.020281\n",
      "621283393  0.020386\n",
      "621283322  0.020808\n",
      "621283334  0.021027\n",
      "621283238  0.021629\n",
      "621283321  0.021709\n",
      "621283328  0.021729\n",
      "621283325  0.021850\n",
      "621283388  0.021909\n",
      "621283330  0.022070\n",
      "621283323  0.022170\n",
      "621283329  0.022331\n",
      "621283395  0.022331\n",
      "621283257  0.022371\n",
      "621283531  0.022491\n",
      "621283320  0.022572\n",
      "621283332  0.023153\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.023713\n",
      "13907  0.024030\n",
      "13841  0.024135\n",
      "13917  0.024155\n",
      "13884  0.024295\n",
      "67865  0.024616\n",
      "67895  0.024636\n",
      "67746  0.024696\n",
      "13847  0.024706\n",
      "13850  0.024776\n",
      "13896  0.024816\n",
      "67678  0.024836\n",
      "13899  0.024916\n",
      "13912  0.024986\n",
      "13913  0.024997\n",
      "13790  0.025017\n",
      "67837  0.025017\n",
      "13740  0.025207\n",
      "36974  0.025217\n",
      "13746  0.025257\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283322  0.016463\n",
      "621283392  0.016687\n",
      "621283321  0.016864\n",
      "621283323  0.017041\n",
      "621283334  0.017065\n",
      "621283320  0.017556\n",
      "621283393  0.017568\n",
      "621283244  0.018309\n",
      "621283328  0.018957\n",
      "621283388  0.019149\n",
      "621283330  0.019286\n",
      "621283238  0.019454\n",
      "621283325  0.019649\n",
      "621283395  0.019759\n",
      "621283329  0.019981\n",
      "621283332  0.020276\n",
      "621283531  0.020701\n",
      "621283257  0.021028\n",
      "621283246  0.022146\n",
      "621283530  0.028334\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13850  0.020286\n",
      "13917  0.020422\n",
      "13896  0.020489\n",
      "13841  0.020698\n",
      "13912  0.020879\n",
      "13884  0.021100\n",
      "13907  0.021144\n",
      "13847  0.021195\n",
      "67837  0.021268\n",
      "67746  0.021296\n",
      "13899  0.021789\n",
      "13740  0.021978\n",
      "67678  0.022132\n",
      "13913  0.022149\n",
      "13790  0.022242\n",
      "13746  0.022393\n",
      "67895  0.022627\n",
      "67865  0.022724\n",
      "36974  0.024469\n",
      "13897  0.031716\n",
      ">>> Fastest Solution is rocblas 621283322 0.016463349759578704\n",
      "M N K dtype 4096 248 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 248 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283411  0.017520\n",
      "621283417  0.018683\n",
      "621283620  0.018843\n",
      "621283379  0.018902\n",
      "621283656  0.019264\n",
      "621283533  0.019645\n",
      "621283416  0.019785\n",
      "621283539  0.019945\n",
      "621283378  0.019985\n",
      "621283418  0.019986\n",
      "621283543  0.020046\n",
      "621283482  0.020105\n",
      "621283414  0.020205\n",
      "621283540  0.020206\n",
      "621283415  0.020266\n",
      "621283469  0.020346\n",
      "621283412  0.020346\n",
      "621283393  0.020446\n",
      "621283564  0.020487\n",
      "621283328  0.020487\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13891  0.023413\n",
      "13911  0.023493\n",
      "67410  0.023573\n",
      "68873  0.023694\n",
      "67361  0.023754\n",
      "67544  0.023774\n",
      "13847  0.023814\n",
      "13841  0.023954\n",
      "68893  0.023975\n",
      "13903  0.023975\n",
      "67280  0.024175\n",
      "67566  0.024295\n",
      "13924  0.024335\n",
      "67536  0.024475\n",
      "13793  0.024736\n",
      "66849  0.024776\n",
      "13756  0.024916\n",
      "13919  0.025056\n",
      "67162  0.025057\n",
      "66855  0.025057\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283418  0.015980\n",
      "621283379  0.016387\n",
      "621283328  0.016716\n",
      "621283540  0.017001\n",
      "621283564  0.017005\n",
      "621283393  0.017053\n",
      "621283469  0.017075\n",
      "621283412  0.017243\n",
      "621283416  0.017331\n",
      "621283656  0.017393\n",
      "621283539  0.017405\n",
      "621283620  0.017492\n",
      "621283378  0.017500\n",
      "621283543  0.017949\n",
      "621283482  0.017967\n",
      "621283533  0.018009\n",
      "621283415  0.018729\n",
      "621283414  0.019045\n",
      "621283411  0.021553\n",
      "621283417  0.022273\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13847  0.020442\n",
      "67566  0.020821\n",
      "13841  0.020873\n",
      "13919  0.021204\n",
      "67410  0.021824\n",
      "13793  0.021882\n",
      "13903  0.021924\n",
      "13924  0.022124\n",
      "67536  0.022541\n",
      "66855  0.022778\n",
      "68873  0.022902\n",
      "67162  0.022916\n",
      "67361  0.023052\n",
      "13756  0.023070\n",
      "67280  0.023339\n",
      "68893  0.023341\n",
      "13911  0.023451\n",
      "67544  0.023559\n",
      "66849  0.023962\n",
      "13891  0.027873\n",
      ">>> Fastest Solution is rocblas 621283418 0.015980249643325804\n",
      "M N K dtype 3584 248 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 248 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283364  0.036603\n",
      "621283329  0.037680\n",
      "621283399  0.037866\n",
      "621283330  0.038232\n",
      "621283363  0.040411\n",
      "621283388  0.040422\n",
      "621283365  0.040432\n",
      "621283325  0.040507\n",
      "621283333  0.040713\n",
      "621283391  0.041003\n",
      "621283366  0.041374\n",
      "621283338  0.041694\n",
      "621283332  0.041790\n",
      "621283328  0.041875\n",
      "621283331  0.042036\n",
      "621283397  0.042055\n",
      "621283339  0.042135\n",
      "621283531  0.042777\n",
      "621283341  0.042838\n",
      "621283327  0.043308\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13844  0.039049\n",
      "13853  0.039069\n",
      "13864  0.040231\n",
      "13851  0.040332\n",
      "13884  0.040452\n",
      "13863  0.040642\n",
      "13897  0.040693\n",
      "13876  0.040713\n",
      "13852  0.040863\n",
      "13860  0.041134\n",
      "13873  0.041434\n",
      "13875  0.041699\n",
      "13856  0.043078\n",
      "13883  0.043208\n",
      "13849  0.043258\n",
      "67919  0.043418\n",
      "13896  0.043690\n",
      "13759  0.044180\n",
      "13779  0.045297\n",
      "13746  0.046425\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283329  0.030785\n",
      "621283330  0.032337\n",
      "621283325  0.032382\n",
      "621283328  0.032896\n",
      "621283331  0.032909\n",
      "621283388  0.034798\n",
      "621283333  0.034811\n",
      "621283327  0.036273\n",
      "621283332  0.036637\n",
      "621283366  0.037148\n",
      "621283338  0.038277\n",
      "621283363  0.038367\n",
      "621283391  0.039658\n",
      "621283365  0.039953\n",
      "621283341  0.040624\n",
      "621283339  0.041346\n",
      "621283531  0.041933\n",
      "621283397  0.045249\n",
      "621283399  0.046273\n",
      "621283364  0.049322\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13863  0.033420\n",
      "13883  0.034612\n",
      "13897  0.035645\n",
      "13852  0.035689\n",
      "13853  0.035945\n",
      "13884  0.036309\n",
      "13896  0.036762\n",
      "13856  0.039730\n",
      "13875  0.039740\n",
      "13873  0.039772\n",
      "13860  0.040149\n",
      "13851  0.040915\n",
      "13746  0.040915\n",
      "13779  0.041060\n",
      "13876  0.041211\n",
      "13849  0.041789\n",
      "13864  0.041923\n",
      "13759  0.043733\n",
      "67919  0.043835\n",
      "13844  0.055606\n",
      ">>> Fastest Solution is rocblas 621283329 0.030785301327705385\n",
      "M N K dtype 4096 248 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 248 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283330  0.020887\n",
      "621283337  0.021409\n",
      "621283363  0.021528\n",
      "621283338  0.021669\n",
      "621283364  0.022090\n",
      "621283341  0.022090\n",
      "621283325  0.022095\n",
      "621283340  0.022110\n",
      "621283365  0.022271\n",
      "621283328  0.022275\n",
      "621283252  0.022431\n",
      "621283391  0.022451\n",
      "621283335  0.022511\n",
      "621283326  0.022867\n",
      "621283339  0.023513\n",
      "621283366  0.023594\n",
      "621283332  0.023869\n",
      "621283331  0.023899\n",
      "621283329  0.023945\n",
      "621283324  0.023950\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13851  0.024916\n",
      "13889  0.025097\n",
      "13893  0.025112\n",
      "13863  0.025313\n",
      "13852  0.025478\n",
      "13844  0.025502\n",
      "13746  0.025598\n",
      "13873  0.025658\n",
      "13864  0.025959\n",
      "67895  0.026455\n",
      "13856  0.026620\n",
      "13916  0.026710\n",
      "13790  0.026716\n",
      "13779  0.026806\n",
      "13860  0.026946\n",
      "13849  0.027021\n",
      "13892  0.027116\n",
      "13884  0.027217\n",
      "67919  0.027291\n",
      "13894  0.027322\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.017373\n",
      "621283324  0.018123\n",
      "621283329  0.018252\n",
      "621283364  0.018478\n",
      "621283366  0.018596\n",
      "621283335  0.018608\n",
      "621283339  0.018985\n",
      "621283365  0.019085\n",
      "621283340  0.019145\n",
      "621283326  0.019213\n",
      "621283341  0.019276\n",
      "621283332  0.019356\n",
      "621283325  0.019544\n",
      "621283331  0.019631\n",
      "621283391  0.019869\n",
      "621283338  0.020222\n",
      "621283330  0.020456\n",
      "621283363  0.022329\n",
      "621283252  0.026875\n",
      "621283337  0.027045\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13856  0.021411\n",
      "13860  0.021756\n",
      "13884  0.021942\n",
      "13849  0.022086\n",
      "13779  0.022154\n",
      "67919  0.022485\n",
      "13873  0.022541\n",
      "13864  0.022604\n",
      "13844  0.022718\n",
      "13892  0.022727\n",
      "13894  0.022727\n",
      "13790  0.022747\n",
      "13893  0.022916\n",
      "67895  0.023009\n",
      "13863  0.023363\n",
      "13852  0.023397\n",
      "13746  0.023434\n",
      "13916  0.023704\n",
      "13851  0.029234\n",
      "13889  0.034200\n",
      ">>> Fastest Solution is rocblas 621283328 0.017373399436473848\n",
      "M N K dtype 768 240 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 240 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.018863\n",
      "621283244  0.019524\n",
      "621283392  0.019865\n",
      "621283320  0.020707\n",
      "621283393  0.020767\n",
      "621283322  0.020947\n",
      "621283319  0.021188\n",
      "621283246  0.021228\n",
      "621283323  0.021549\n",
      "621283325  0.021649\n",
      "621283238  0.021769\n",
      "621283330  0.022110\n",
      "621283329  0.022231\n",
      "621283245  0.022351\n",
      "621283257  0.022411\n",
      "621283395  0.022411\n",
      "621283388  0.022471\n",
      "621283332  0.023132\n",
      "621283324  0.023232\n",
      "621283327  0.023573\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13883  0.023854\n",
      "13896  0.024135\n",
      "13907  0.024195\n",
      "67678  0.024215\n",
      "13850  0.024335\n",
      "13882  0.024516\n",
      "67746  0.024676\n",
      "13897  0.024817\n",
      "13920  0.024997\n",
      "13852  0.025097\n",
      "13779  0.025157\n",
      "13916  0.025157\n",
      "67808  0.025177\n",
      "67865  0.025257\n",
      "13858  0.025378\n",
      "13859  0.025378\n",
      "67777  0.025478\n",
      "13842  0.025493\n",
      "13912  0.025523\n",
      "36974  0.025578\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.016026\n",
      "621283322  0.016676\n",
      "621283392  0.016694\n",
      "621283323  0.017039\n",
      "621283246  0.017183\n",
      "621283320  0.017203\n",
      "621283319  0.017241\n",
      "621283393  0.017840\n",
      "621283325  0.019370\n",
      "621283329  0.019386\n",
      "621283330  0.019560\n",
      "621283388  0.019687\n",
      "621283238  0.020228\n",
      "621283395  0.020509\n",
      "621283332  0.020559\n",
      "621283324  0.020649\n",
      "621283257  0.020815\n",
      "621283327  0.021092\n",
      "621283244  0.021761\n",
      "621283530  0.023802\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13912  0.020474\n",
      "13850  0.020727\n",
      "13897  0.020982\n",
      "13852  0.021038\n",
      "13882  0.021040\n",
      "13842  0.021214\n",
      "67808  0.021228\n",
      "67746  0.021401\n",
      "13920  0.021955\n",
      "13779  0.022012\n",
      "13916  0.022120\n",
      "13859  0.022315\n",
      "67865  0.022407\n",
      "36974  0.022479\n",
      "13858  0.022843\n",
      "13907  0.023039\n",
      "67777  0.023107\n",
      "67678  0.023333\n",
      "13896  0.024969\n",
      "13883  0.033538\n",
      ">>> Fastest Solution is rocblas 621283245 0.016026349365711214\n",
      "M N K dtype 4096 240 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 240 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283379  0.019264\n",
      "621283410  0.019264\n",
      "621283380  0.019404\n",
      "621283540  0.019464\n",
      "621283416  0.019705\n",
      "621283620  0.019805\n",
      "621283543  0.019825\n",
      "621283564  0.019885\n",
      "621283258  0.019965\n",
      "621283417  0.020025\n",
      "621283413  0.020065\n",
      "621283542  0.020206\n",
      "621283656  0.020306\n",
      "621283254  0.020366\n",
      "621283260  0.020366\n",
      "621283401  0.020427\n",
      "621283412  0.020506\n",
      "621283469  0.020567\n",
      "621283541  0.020587\n",
      "621283424  0.020627\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13891  0.022992\n",
      "67280  0.023473\n",
      "67989  0.023494\n",
      "67244  0.024195\n",
      "13756  0.024215\n",
      "13911  0.024335\n",
      "13858  0.024355\n",
      "67104  0.024435\n",
      "67398  0.024496\n",
      "13919  0.024516\n",
      "13879  0.024556\n",
      "13918  0.024556\n",
      "67536  0.024736\n",
      "13907  0.024757\n",
      "67566  0.024897\n",
      "13841  0.024916\n",
      "66868  0.025016\n",
      "67527  0.025097\n",
      "66891  0.025097\n",
      "67156  0.025157\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283412  0.016345\n",
      "621283620  0.016648\n",
      "621283417  0.016846\n",
      "621283413  0.016928\n",
      "621283564  0.016978\n",
      "621283258  0.017063\n",
      "621283469  0.017089\n",
      "621283254  0.017187\n",
      "621283656  0.017359\n",
      "621283401  0.017504\n",
      "621283540  0.017534\n",
      "621283260  0.017556\n",
      "621283416  0.017694\n",
      "621283543  0.017887\n",
      "621283542  0.018831\n",
      "621283541  0.019091\n",
      "621283380  0.019671\n",
      "621283424  0.020717\n",
      "621283410  0.022078\n",
      "621283379  0.027402\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13841  0.020286\n",
      "13918  0.020881\n",
      "13907  0.021066\n",
      "13919  0.021110\n",
      "67566  0.021571\n",
      "66891  0.021906\n",
      "13911  0.022070\n",
      "13879  0.022248\n",
      "13858  0.022297\n",
      "67527  0.022413\n",
      "67536  0.022581\n",
      "66868  0.022714\n",
      "67244  0.022892\n",
      "67156  0.023006\n",
      "67398  0.023175\n",
      "67104  0.023181\n",
      "13756  0.023261\n",
      "67989  0.023353\n",
      "67280  0.024057\n",
      "13891  0.029130\n",
      ">>> Fastest Solution is rocblas 621283412 0.01634510010480881\n",
      "M N K dtype 3584 240 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 240 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283332  0.034699\n",
      "621283330  0.036608\n",
      "621283365  0.037505\n",
      "621283388  0.038362\n",
      "621283328  0.038573\n",
      "621283333  0.038743\n",
      "621283327  0.039806\n",
      "621283329  0.039885\n",
      "621283331  0.040001\n",
      "621283364  0.040091\n",
      "621283325  0.040562\n",
      "621283366  0.040572\n",
      "621283363  0.040833\n",
      "621283338  0.041093\n",
      "621283391  0.041534\n",
      "621283324  0.042837\n",
      "621283390  0.042868\n",
      "621283340  0.042937\n",
      "621283397  0.043058\n",
      "621283531  0.043153\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13853  0.035841\n",
      "13873  0.037765\n",
      "13844  0.037967\n",
      "13876  0.038066\n",
      "13863  0.038948\n",
      "13917  0.039104\n",
      "13884  0.039229\n",
      "13852  0.039770\n",
      "13759  0.039790\n",
      "13883  0.039816\n",
      "13897  0.040983\n",
      "13896  0.042161\n",
      "13864  0.042236\n",
      "13849  0.042656\n",
      "13851  0.042897\n",
      "67919  0.042998\n",
      "13856  0.043017\n",
      "13875  0.043278\n",
      "13746  0.043469\n",
      "13790  0.044151\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283329  0.029638\n",
      "621283327  0.030559\n",
      "621283330  0.030998\n",
      "621283328  0.031302\n",
      "621283388  0.031502\n",
      "621283325  0.032856\n",
      "621283331  0.035404\n",
      "621283333  0.035436\n",
      "621283324  0.036530\n",
      "621283332  0.038796\n",
      "621283390  0.038976\n",
      "621283338  0.039486\n",
      "621283363  0.039828\n",
      "621283531  0.040057\n",
      "621283397  0.040311\n",
      "621283391  0.040574\n",
      "621283364  0.040873\n",
      "621283365  0.041588\n",
      "621283340  0.041885\n",
      "621283366  0.042005\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.029339\n",
      "13852  0.029411\n",
      "13917  0.031488\n",
      "13863  0.033951\n",
      "13896  0.035189\n",
      "13883  0.036092\n",
      "13897  0.036604\n",
      "13876  0.038636\n",
      "13790  0.039808\n",
      "13856  0.040069\n",
      "13851  0.040530\n",
      "13844  0.041061\n",
      "13875  0.041408\n",
      "13849  0.041480\n",
      "13873  0.041903\n",
      "67919  0.041925\n",
      "13864  0.042017\n",
      "13746  0.042453\n",
      "13853  0.045245\n",
      "13759  0.048666\n",
      ">>> Fastest Solution is hipblaslt 13884 0.029338550567626954\n",
      "M N K dtype 4096 240 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 240 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283390  0.018702\n",
      "621283330  0.019419\n",
      "621283338  0.019584\n",
      "621283366  0.021028\n",
      "621283339  0.021348\n",
      "621283363  0.021349\n",
      "621283342  0.021489\n",
      "621283391  0.021529\n",
      "621283365  0.021609\n",
      "621283340  0.021629\n",
      "621283331  0.021894\n",
      "621283328  0.022371\n",
      "621283324  0.022460\n",
      "621283325  0.022561\n",
      "621283323  0.022752\n",
      "621283329  0.022831\n",
      "621283364  0.022972\n",
      "621283332  0.023008\n",
      "621283333  0.023042\n",
      "621283326  0.023328\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13893  0.023834\n",
      "13849  0.024250\n",
      "13915  0.024306\n",
      "13894  0.024450\n",
      "13864  0.024475\n",
      "13759  0.024696\n",
      "13892  0.024786\n",
      "13916  0.025302\n",
      "13853  0.025432\n",
      "67045  0.025794\n",
      "13779  0.025919\n",
      "13860  0.026094\n",
      "67916  0.026279\n",
      "67895  0.026294\n",
      "13791  0.026425\n",
      "13777  0.026571\n",
      "13844  0.026906\n",
      "13769  0.026911\n",
      "13863  0.026921\n",
      "67869  0.027097\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283325  0.017056\n",
      "621283329  0.017506\n",
      "621283330  0.017542\n",
      "621283324  0.018020\n",
      "621283364  0.018103\n",
      "621283332  0.018227\n",
      "621283328  0.018441\n",
      "621283363  0.018474\n",
      "621283333  0.019117\n",
      "621283391  0.019234\n",
      "621283365  0.019274\n",
      "621283340  0.019604\n",
      "621283326  0.019675\n",
      "621283339  0.019709\n",
      "621283331  0.019793\n",
      "621283323  0.020471\n",
      "621283366  0.020565\n",
      "621283342  0.021725\n",
      "621283338  0.021990\n",
      "621283390  0.027039\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13864  0.021076\n",
      "13860  0.021574\n",
      "67916  0.021847\n",
      "13863  0.022040\n",
      "13915  0.022275\n",
      "13844  0.022365\n",
      "13894  0.022511\n",
      "67895  0.022737\n",
      "13853  0.022961\n",
      "13777  0.022986\n",
      "13769  0.023019\n",
      "13759  0.023195\n",
      "13779  0.023471\n",
      "13892  0.023764\n",
      "67869  0.024203\n",
      "13791  0.024291\n",
      "67045  0.024850\n",
      "13849  0.024884\n",
      "13916  0.025452\n",
      "13893  0.027623\n",
      ">>> Fastest Solution is rocblas 621283325 0.01705619990825653\n",
      "M N K dtype 768 232 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 232 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283392  0.019609\n",
      "621283322  0.019624\n",
      "621283393  0.019724\n",
      "621283530  0.020226\n",
      "621283319  0.020286\n",
      "621283334  0.020426\n",
      "621283245  0.020487\n",
      "621283244  0.020707\n",
      "621283321  0.021188\n",
      "621283320  0.021308\n",
      "621283328  0.021508\n",
      "621283325  0.021690\n",
      "621283323  0.021729\n",
      "621283330  0.022131\n",
      "621283329  0.022171\n",
      "621283257  0.022431\n",
      "621283531  0.022451\n",
      "621283388  0.022471\n",
      "621283395  0.022992\n",
      "621283332  0.023032\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13907  0.023248\n",
      "13883  0.023373\n",
      "13897  0.023834\n",
      "67865  0.023994\n",
      "13896  0.024015\n",
      "13920  0.024255\n",
      "36974  0.024857\n",
      "67808  0.024897\n",
      "13879  0.024916\n",
      "13841  0.024952\n",
      "13912  0.024982\n",
      "13779  0.025017\n",
      "13765  0.025022\n",
      "13847  0.025172\n",
      "67717  0.025198\n",
      "13927  0.025213\n",
      "67678  0.025277\n",
      "13915  0.025297\n",
      "13855  0.025298\n",
      "13892  0.025317\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283244  0.015128\n",
      "621283245  0.015782\n",
      "621283334  0.016501\n",
      "621283323  0.016562\n",
      "621283321  0.016822\n",
      "621283320  0.016973\n",
      "621283319  0.017395\n",
      "621283530  0.017552\n",
      "621283392  0.017985\n",
      "621283388  0.019081\n",
      "621283325  0.019262\n",
      "621283330  0.019270\n",
      "621283328  0.019388\n",
      "621283329  0.019661\n",
      "621283395  0.019833\n",
      "621283257  0.020154\n",
      "621283531  0.020170\n",
      "621283332  0.020547\n",
      "621283322  0.021014\n",
      "621283393  0.022002\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13896  0.020693\n",
      "67808  0.020761\n",
      "13912  0.020839\n",
      "13897  0.022036\n",
      "13892  0.022271\n",
      "13779  0.022287\n",
      "13927  0.022451\n",
      "13847  0.022581\n",
      "13765  0.022585\n",
      "13915  0.022651\n",
      "36974  0.022695\n",
      "13920  0.022719\n",
      "67865  0.022798\n",
      "13855  0.022868\n",
      "13841  0.022924\n",
      "67678  0.022998\n",
      "13879  0.023177\n",
      "67717  0.023847\n",
      "13883  0.026999\n",
      "13907  0.028753\n",
      ">>> Fastest Solution is rocblas 621283244 0.015128299593925476\n",
      "M N K dtype 4096 232 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 232 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283416  0.019164\n",
      "621283620  0.019384\n",
      "621283412  0.019584\n",
      "621283418  0.019645\n",
      "621283656  0.019664\n",
      "621283413  0.019665\n",
      "621283333  0.019705\n",
      "621283540  0.019705\n",
      "621283379  0.019765\n",
      "621283564  0.019885\n",
      "621283238  0.019946\n",
      "621283539  0.019965\n",
      "621283410  0.020025\n",
      "621283414  0.020246\n",
      "621283542  0.020306\n",
      "621283324  0.020326\n",
      "621283469  0.020366\n",
      "621283254  0.020387\n",
      "621283366  0.020427\n",
      "621283336  0.020446\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.021830\n",
      "67410  0.022691\n",
      "13891  0.022992\n",
      "68893  0.023313\n",
      "68063  0.023473\n",
      "68873  0.023573\n",
      "66855  0.023774\n",
      "13878  0.023774\n",
      "66923  0.023834\n",
      "67361  0.023994\n",
      "67594  0.023994\n",
      "13772  0.024075\n",
      "13903  0.024134\n",
      "67162  0.024316\n",
      "67536  0.024415\n",
      "67566  0.024435\n",
      "67244  0.024556\n",
      "67280  0.024756\n",
      "13919  0.024816\n",
      "66891  0.024817\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283379  0.015652\n",
      "621283410  0.016387\n",
      "621283413  0.016650\n",
      "621283238  0.016706\n",
      "621283469  0.016818\n",
      "621283366  0.016826\n",
      "621283336  0.016884\n",
      "621283539  0.016892\n",
      "621283333  0.016952\n",
      "621283418  0.017039\n",
      "621283564  0.017109\n",
      "621283324  0.017123\n",
      "621283656  0.017369\n",
      "621283412  0.017422\n",
      "621283540  0.017425\n",
      "621283254  0.017612\n",
      "621283542  0.018813\n",
      "621283414  0.018901\n",
      "621283620  0.023321\n",
      "621283416  0.031167\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13919  0.020998\n",
      "66891  0.021916\n",
      "67162  0.021922\n",
      "13891  0.022000\n",
      "67566  0.022070\n",
      "68873  0.022218\n",
      "13878  0.022373\n",
      "67536  0.022499\n",
      "13903  0.022515\n",
      "66855  0.022607\n",
      "66923  0.022621\n",
      "67361  0.022699\n",
      "13772  0.022752\n",
      "67244  0.022846\n",
      "67280  0.022956\n",
      "67594  0.023136\n",
      "68893  0.023201\n",
      "68063  0.023245\n",
      "67410  0.025227\n",
      "13911  0.027695\n",
      ">>> Fastest Solution is rocblas 621283379 0.015651500225067137\n",
      "M N K dtype 3584 232 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 232 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283330  0.035085\n",
      "621283329  0.036527\n",
      "621283331  0.037515\n",
      "621283328  0.037806\n",
      "621283333  0.038352\n",
      "621283366  0.038407\n",
      "621283332  0.039099\n",
      "621283325  0.039315\n",
      "621283391  0.039575\n",
      "621283327  0.039956\n",
      "621283364  0.039971\n",
      "621283396  0.040111\n",
      "621283390  0.040472\n",
      "621283388  0.040502\n",
      "621283326  0.040617\n",
      "621283363  0.041113\n",
      "621283324  0.041153\n",
      "621283341  0.041194\n",
      "621283365  0.041494\n",
      "621283532  0.041644\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13852  0.033241\n",
      "13875  0.037445\n",
      "13873  0.037765\n",
      "13853  0.038849\n",
      "13883  0.039039\n",
      "13884  0.039164\n",
      "13917  0.039324\n",
      "13876  0.040672\n",
      "13860  0.040732\n",
      "13844  0.040913\n",
      "13896  0.041575\n",
      "13897  0.042316\n",
      "13856  0.042556\n",
      "67919  0.043258\n",
      "13759  0.043278\n",
      "13849  0.043398\n",
      "13790  0.043518\n",
      "13863  0.043734\n",
      "13851  0.044461\n",
      "13746  0.045157\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.028659\n",
      "621283325  0.030183\n",
      "621283329  0.032201\n",
      "621283332  0.032345\n",
      "621283327  0.033909\n",
      "621283330  0.034825\n",
      "621283331  0.035037\n",
      "621283333  0.035374\n",
      "621283388  0.035747\n",
      "621283324  0.036099\n",
      "621283365  0.038050\n",
      "621283390  0.038078\n",
      "621283363  0.038822\n",
      "621283364  0.039161\n",
      "621283326  0.039279\n",
      "621283366  0.040734\n",
      "621283391  0.041219\n",
      "621283341  0.041484\n",
      "621283532  0.042085\n",
      "621283396  0.042747\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13917  0.031038\n",
      "13884  0.031451\n",
      "13853  0.033460\n",
      "13883  0.034070\n",
      "13897  0.034437\n",
      "13896  0.035327\n",
      "13863  0.035579\n",
      "13852  0.036769\n",
      "13873  0.038499\n",
      "13790  0.038810\n",
      "13860  0.040728\n",
      "13849  0.040764\n",
      "13746  0.040791\n",
      "13851  0.040913\n",
      "13844  0.042023\n",
      "13876  0.042352\n",
      "13856  0.042591\n",
      "13759  0.043368\n",
      "13875  0.043418\n",
      "67919  0.044441\n",
      ">>> Fastest Solution is rocblas 621283328 0.02865850031375885\n",
      "M N K dtype 4096 232 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 232 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283336  0.021328\n",
      "621283329  0.021334\n",
      "621283333  0.021383\n",
      "621283339  0.021389\n",
      "621283338  0.021428\n",
      "621283330  0.022140\n",
      "621283323  0.022341\n",
      "621283322  0.022341\n",
      "621283340  0.022411\n",
      "621283391  0.022450\n",
      "621283334  0.022571\n",
      "621283328  0.022687\n",
      "621283236  0.022701\n",
      "621283335  0.022712\n",
      "621283326  0.022801\n",
      "621283337  0.022912\n",
      "621283321  0.023253\n",
      "621283365  0.023253\n",
      "621283390  0.023293\n",
      "621283366  0.023613\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13894  0.024120\n",
      "13915  0.024631\n",
      "13860  0.024726\n",
      "13893  0.025087\n",
      "13732  0.025172\n",
      "13889  0.025558\n",
      "67895  0.025628\n",
      "13759  0.025658\n",
      "13790  0.025664\n",
      "13746  0.025773\n",
      "13779  0.025789\n",
      "13916  0.025819\n",
      "67869  0.025923\n",
      "13864  0.025939\n",
      "13849  0.026049\n",
      "13856  0.026104\n",
      "13873  0.026540\n",
      "13897  0.026750\n",
      "13789  0.026861\n",
      "13863  0.027157\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283329  0.016040\n",
      "621283328  0.017817\n",
      "621283330  0.017867\n",
      "621283333  0.018468\n",
      "621283366  0.018600\n",
      "621283390  0.018706\n",
      "621283335  0.019027\n",
      "621283338  0.019328\n",
      "621283365  0.019366\n",
      "621283391  0.019548\n",
      "621283236  0.019663\n",
      "621283323  0.019739\n",
      "621283337  0.019837\n",
      "621283340  0.019857\n",
      "621283321  0.019960\n",
      "621283322  0.020206\n",
      "621283326  0.020368\n",
      "621283334  0.020522\n",
      "621283339  0.021146\n",
      "621283336  0.034135\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13849  0.021768\n",
      "13897  0.022135\n",
      "13893  0.022183\n",
      "13873  0.022222\n",
      "13860  0.022321\n",
      "13863  0.022332\n",
      "13864  0.022401\n",
      "13856  0.022600\n",
      "13732  0.022782\n",
      "13916  0.022907\n",
      "67869  0.022992\n",
      "13915  0.023010\n",
      "13746  0.023076\n",
      "13779  0.023232\n",
      "13790  0.023316\n",
      "67895  0.023494\n",
      "13759  0.023907\n",
      "13894  0.027615\n",
      "13889  0.029204\n",
      "13789  0.029838\n",
      ">>> Fastest Solution is rocblas 621283329 0.01604039967060089\n",
      "M N K dtype 768 224 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 224 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283319  0.020707\n",
      "621283393  0.020807\n",
      "621283392  0.021087\n",
      "621283530  0.021168\n",
      "621283320  0.021228\n",
      "621283238  0.021288\n",
      "621283334  0.021629\n",
      "621283328  0.021629\n",
      "621283322  0.021669\n",
      "621283246  0.021750\n",
      "621283395  0.022210\n",
      "621283245  0.022250\n",
      "621283330  0.022271\n",
      "621283531  0.022451\n",
      "621283325  0.022572\n",
      "621283329  0.022792\n",
      "621283332  0.023093\n",
      "621283323  0.023132\n",
      "621283324  0.023213\n",
      "621283327  0.023433\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.023553\n",
      "13845  0.024015\n",
      "67808  0.024035\n",
      "13884  0.024776\n",
      "13917  0.024857\n",
      "13925  0.024897\n",
      "13906  0.024916\n",
      "13882  0.024957\n",
      "13746  0.025217\n",
      "67865  0.025238\n",
      "13907  0.025272\n",
      "66998  0.025378\n",
      "13858  0.025428\n",
      "13850  0.025518\n",
      "13831  0.025613\n",
      "13912  0.025628\n",
      "13915  0.025658\n",
      "13883  0.025658\n",
      "67837  0.025819\n",
      "67746  0.025819\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.015629\n",
      "621283319  0.015796\n",
      "621283392  0.015838\n",
      "621283322  0.016151\n",
      "621283246  0.016708\n",
      "621283323  0.017021\n",
      "621283320  0.017131\n",
      "621283334  0.017219\n",
      "621283530  0.017760\n",
      "621283328  0.018883\n",
      "621283325  0.018973\n",
      "621283238  0.019590\n",
      "621283329  0.019717\n",
      "621283395  0.019747\n",
      "621283330  0.019793\n",
      "621283531  0.020198\n",
      "621283332  0.020270\n",
      "621283327  0.020643\n",
      "621283324  0.020677\n",
      "621283393  0.024810\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13850  0.020316\n",
      "13883  0.020701\n",
      "13917  0.020829\n",
      "13884  0.021284\n",
      "67837  0.021332\n",
      "13882  0.021396\n",
      "13912  0.021444\n",
      "67808  0.021737\n",
      "67746  0.021801\n",
      "13907  0.021932\n",
      "13915  0.022208\n",
      "13746  0.022339\n",
      "13925  0.022397\n",
      "13906  0.022487\n",
      "13831  0.022865\n",
      "67865  0.023425\n",
      "13858  0.023527\n",
      "66998  0.023876\n",
      "13845  0.024744\n",
      "13897  0.031997\n",
      ">>> Fastest Solution is rocblas 621283245 0.015629449486732484\n",
      "M N K dtype 4096 224 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 224 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283379  0.018482\n",
      "621283410  0.018643\n",
      "621283417  0.019143\n",
      "621283380  0.019364\n",
      "621283416  0.019384\n",
      "621283620  0.019404\n",
      "621283247  0.019484\n",
      "621283249  0.019524\n",
      "621283543  0.019565\n",
      "621283248  0.019764\n",
      "621283539  0.019765\n",
      "621283378  0.019765\n",
      "621283469  0.019905\n",
      "621283418  0.020086\n",
      "621283412  0.020105\n",
      "621283415  0.020126\n",
      "621283376  0.020226\n",
      "621283414  0.020266\n",
      "621283542  0.020266\n",
      "621283337  0.020286\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13861  0.022030\n",
      "13857  0.022611\n",
      "67410  0.023032\n",
      "13911  0.023172\n",
      "68893  0.023373\n",
      "13918  0.023373\n",
      "67280  0.023493\n",
      "67213  0.023534\n",
      "13903  0.023634\n",
      "67104  0.023694\n",
      "67170  0.023994\n",
      "66899  0.024075\n",
      "13856  0.024075\n",
      "68102  0.024154\n",
      "13891  0.024175\n",
      "67585  0.024435\n",
      "66923  0.024435\n",
      "66906  0.024456\n",
      "13879  0.024516\n",
      "66868  0.024616\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283418  0.016654\n",
      "621283620  0.016822\n",
      "621283469  0.016882\n",
      "621283249  0.016987\n",
      "621283378  0.017029\n",
      "621283412  0.017077\n",
      "621283247  0.017133\n",
      "621283248  0.017171\n",
      "621283539  0.017295\n",
      "621283337  0.017542\n",
      "621283376  0.017734\n",
      "621283416  0.017858\n",
      "621283380  0.018119\n",
      "621283543  0.018137\n",
      "621283417  0.018518\n",
      "621283415  0.018544\n",
      "621283542  0.018731\n",
      "621283414  0.018891\n",
      "621283410  0.022497\n",
      "621283379  0.026861\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13918  0.021056\n",
      "67213  0.021523\n",
      "66899  0.022244\n",
      "66906  0.022291\n",
      "13879  0.022349\n",
      "13911  0.022463\n",
      "66923  0.022477\n",
      "67280  0.022531\n",
      "67104  0.022607\n",
      "67170  0.022882\n",
      "13856  0.022912\n",
      "66868  0.022974\n",
      "13903  0.023006\n",
      "68102  0.023020\n",
      "13891  0.023136\n",
      "67585  0.023321\n",
      "67410  0.023461\n",
      "68893  0.023980\n",
      "13857  0.025339\n",
      "13861  0.031287\n",
      ">>> Fastest Solution is rocblas 621283418 0.016653749346733093\n",
      "M N K dtype 3584 224 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 224 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283328  0.035200\n",
      "621283333  0.037104\n",
      "621283325  0.037971\n",
      "621283388  0.038017\n",
      "621283330  0.038918\n",
      "621283390  0.039409\n",
      "621283332  0.039474\n",
      "621283327  0.039524\n",
      "621283366  0.039530\n",
      "621283329  0.039535\n",
      "621283365  0.039670\n",
      "621283364  0.039690\n",
      "621283331  0.039781\n",
      "621283391  0.039910\n",
      "621283363  0.040131\n",
      "621283324  0.040607\n",
      "621283533  0.041253\n",
      "621283531  0.041379\n",
      "621283323  0.041444\n",
      "621283397  0.042075\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13884  0.036477\n",
      "13883  0.037094\n",
      "13873  0.038027\n",
      "13896  0.038121\n",
      "13853  0.038187\n",
      "13852  0.039315\n",
      "13849  0.039630\n",
      "13917  0.040316\n",
      "13875  0.040532\n",
      "13844  0.041013\n",
      "13876  0.041314\n",
      "13897  0.041754\n",
      "13856  0.042015\n",
      "13863  0.042055\n",
      "13759  0.042436\n",
      "13851  0.042537\n",
      "13860  0.042877\n",
      "67118  0.043238\n",
      "67919  0.043479\n",
      "67869  0.044370\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283330  0.028998\n",
      "621283332  0.029889\n",
      "621283327  0.029985\n",
      "621283388  0.030066\n",
      "621283325  0.030292\n",
      "621283329  0.031950\n",
      "621283328  0.032676\n",
      "621283364  0.035865\n",
      "621283324  0.035880\n",
      "621283363  0.036928\n",
      "621283333  0.037038\n",
      "621283531  0.037521\n",
      "621283365  0.037740\n",
      "621283391  0.038058\n",
      "621283331  0.038493\n",
      "621283323  0.040150\n",
      "621283366  0.041215\n",
      "621283533  0.041534\n",
      "621283390  0.043008\n",
      "621283397  0.043892\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13852  0.029211\n",
      "13883  0.030393\n",
      "13863  0.034703\n",
      "13917  0.034898\n",
      "13896  0.035061\n",
      "13897  0.035670\n",
      "13853  0.036387\n",
      "13884  0.037397\n",
      "13876  0.037850\n",
      "13851  0.039135\n",
      "67919  0.039684\n",
      "13860  0.039700\n",
      "13856  0.040287\n",
      "13875  0.041230\n",
      "13873  0.041697\n",
      "13844  0.041795\n",
      "67869  0.042661\n",
      "13759  0.043683\n",
      "13849  0.046213\n",
      "67118  0.057358\n",
      ">>> Fastest Solution is rocblas 621283330 0.028997749090194702\n",
      "M N K dtype 4096 224 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 224 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283391  0.019724\n",
      "621283339  0.019745\n",
      "621283330  0.020411\n",
      "621283327  0.020677\n",
      "621283332  0.020947\n",
      "621283331  0.021269\n",
      "621283365  0.021568\n",
      "621283338  0.021609\n",
      "621283323  0.021794\n",
      "621283324  0.021860\n",
      "621283322  0.021985\n",
      "621283321  0.021985\n",
      "621283333  0.022045\n",
      "621283326  0.022280\n",
      "621283390  0.022311\n",
      "621283325  0.022651\n",
      "621283329  0.022697\n",
      "621283334  0.022721\n",
      "621283364  0.022752\n",
      "621283335  0.022752\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13893  0.024425\n",
      "67817  0.024882\n",
      "13892  0.024961\n",
      "13856  0.025042\n",
      "13849  0.025243\n",
      "13852  0.025257\n",
      "13897  0.025643\n",
      "13894  0.025683\n",
      "13915  0.025849\n",
      "13759  0.025879\n",
      "67869  0.025934\n",
      "13860  0.025974\n",
      "13884  0.026009\n",
      "67592  0.026320\n",
      "13863  0.026360\n",
      "13791  0.026741\n",
      "67879  0.026886\n",
      "67919  0.026941\n",
      "13896  0.026966\n",
      "13876  0.026981\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283330  0.017101\n",
      "621283324  0.017254\n",
      "621283332  0.017406\n",
      "621283364  0.017670\n",
      "621283333  0.017745\n",
      "621283325  0.017807\n",
      "621283329  0.018318\n",
      "621283331  0.018534\n",
      "621283327  0.018987\n",
      "621283390  0.019133\n",
      "621283335  0.019534\n",
      "621283365  0.019795\n",
      "621283334  0.019946\n",
      "621283323  0.020070\n",
      "621283338  0.020077\n",
      "621283321  0.020269\n",
      "621283326  0.020595\n",
      "621283322  0.020689\n",
      "621283339  0.023656\n",
      "621283391  0.026598\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13856  0.021260\n",
      "13849  0.021318\n",
      "13892  0.021650\n",
      "13884  0.021665\n",
      "13863  0.021949\n",
      "13759  0.021964\n",
      "13897  0.022232\n",
      "13860  0.022364\n",
      "13896  0.022386\n",
      "67919  0.022423\n",
      "13852  0.022579\n",
      "13915  0.022707\n",
      "13876  0.022992\n",
      "13894  0.023241\n",
      "67817  0.023826\n",
      "13791  0.024356\n",
      "67869  0.024470\n",
      "67879  0.027251\n",
      "13893  0.027999\n",
      "67592  0.030678\n",
      ">>> Fastest Solution is rocblas 621283330 0.01710129976272583\n",
      "M N K dtype 768 216 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 216 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.019284\n",
      "621283245  0.019404\n",
      "621283246  0.019845\n",
      "621283244  0.020366\n",
      "621283393  0.020747\n",
      "621283334  0.020908\n",
      "621283320  0.021128\n",
      "621283319  0.021248\n",
      "621283321  0.021389\n",
      "621283323  0.021469\n",
      "621283238  0.021589\n",
      "621283328  0.021669\n",
      "621283325  0.021729\n",
      "621283322  0.021950\n",
      "621283330  0.022070\n",
      "621283392  0.022431\n",
      "621283329  0.022471\n",
      "621283531  0.022712\n",
      "621283332  0.023072\n",
      "621283324  0.023975\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13850  0.023212\n",
      "13845  0.023553\n",
      "67808  0.023854\n",
      "13896  0.023954\n",
      "13790  0.024516\n",
      "13917  0.024556\n",
      "13779  0.024616\n",
      "13883  0.024756\n",
      "13746  0.024836\n",
      "13847  0.025006\n",
      "13927  0.025182\n",
      "13886  0.025238\n",
      "13897  0.025257\n",
      "67837  0.025317\n",
      "13892  0.025338\n",
      "13859  0.025458\n",
      "13915  0.025578\n",
      "13788  0.025598\n",
      "13858  0.025623\n",
      "13857  0.025628\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283244  0.016022\n",
      "621283334  0.016147\n",
      "621283392  0.016463\n",
      "621283321  0.016750\n",
      "621283322  0.016868\n",
      "621283323  0.016876\n",
      "621283320  0.017744\n",
      "621283393  0.017758\n",
      "621283319  0.017772\n",
      "621283246  0.018570\n",
      "621283328  0.018989\n",
      "621283325  0.019422\n",
      "621283330  0.019500\n",
      "621283238  0.019755\n",
      "621283329  0.019817\n",
      "621283531  0.020595\n",
      "621283324  0.020793\n",
      "621283332  0.021080\n",
      "621283245  0.021910\n",
      "621283530  0.025486\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13883  0.020765\n",
      "13897  0.020998\n",
      "13917  0.021182\n",
      "13896  0.021210\n",
      "67837  0.021445\n",
      "13859  0.021537\n",
      "67808  0.021567\n",
      "13927  0.021579\n",
      "13892  0.021721\n",
      "13779  0.021854\n",
      "13915  0.022198\n",
      "13746  0.022202\n",
      "13788  0.022379\n",
      "13790  0.022709\n",
      "13886  0.022964\n",
      "13847  0.023268\n",
      "13858  0.023356\n",
      "13857  0.023630\n",
      "13845  0.025552\n",
      "13850  0.034097\n",
      ">>> Fastest Solution is rocblas 621283244 0.016022300720214842\n",
      "M N K dtype 4096 216 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 216 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283410  0.018823\n",
      "621283540  0.018963\n",
      "621283379  0.019023\n",
      "621283620  0.019164\n",
      "621283543  0.019364\n",
      "621283418  0.019404\n",
      "621283412  0.019464\n",
      "621283564  0.019484\n",
      "621283417  0.019505\n",
      "621283380  0.019664\n",
      "621283248  0.019985\n",
      "621283247  0.020005\n",
      "621283416  0.020025\n",
      "621283415  0.020025\n",
      "621283240  0.020086\n",
      "621283413  0.020166\n",
      "621283414  0.020205\n",
      "621283246  0.020266\n",
      "621283469  0.020327\n",
      "621283541  0.020387\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.023634\n",
      "68873  0.023753\n",
      "67566  0.023975\n",
      "13929  0.024075\n",
      "67280  0.024395\n",
      "13838  0.024716\n",
      "67410  0.024736\n",
      "67361  0.024736\n",
      "66868  0.024796\n",
      "13878  0.024916\n",
      "67162  0.024997\n",
      "67244  0.025057\n",
      "67334  0.025197\n",
      "67602  0.025198\n",
      "13858  0.025237\n",
      "68102  0.025257\n",
      "13862  0.025277\n",
      "67213  0.025277\n",
      "13842  0.025317\n",
      "67536  0.025338\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283380  0.016425\n",
      "621283417  0.016447\n",
      "621283412  0.016554\n",
      "621283564  0.016702\n",
      "621283469  0.016820\n",
      "621283418  0.016934\n",
      "621283247  0.017051\n",
      "621283248  0.017221\n",
      "621283620  0.017227\n",
      "621283413  0.017229\n",
      "621283246  0.017257\n",
      "621283416  0.017391\n",
      "621283543  0.017993\n",
      "621283240  0.018628\n",
      "621283540  0.018704\n",
      "621283379  0.018781\n",
      "621283414  0.018935\n",
      "621283541  0.019049\n",
      "621283415  0.019248\n",
      "621283410  0.026157\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13858  0.020701\n",
      "67162  0.021445\n",
      "67602  0.021523\n",
      "67213  0.021627\n",
      "13842  0.021972\n",
      "67566  0.022020\n",
      "13929  0.022062\n",
      "68102  0.022176\n",
      "67244  0.022251\n",
      "67361  0.022313\n",
      "67280  0.022319\n",
      "66868  0.022375\n",
      "67334  0.022415\n",
      "67536  0.022443\n",
      "13878  0.022511\n",
      "13862  0.022595\n",
      "67410  0.022724\n",
      "13838  0.023610\n",
      "68873  0.024391\n",
      "13911  0.029395\n",
      ">>> Fastest Solution is rocblas 621283380 0.016425250470638274\n",
      "M N K dtype 3584 216 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 216 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283364  0.035220\n",
      "621283329  0.035686\n",
      "621283338  0.037084\n",
      "621283531  0.037415\n",
      "621283328  0.037495\n",
      "621283339  0.037666\n",
      "621283325  0.037821\n",
      "621283341  0.038107\n",
      "621283398  0.038587\n",
      "621283330  0.038598\n",
      "621283333  0.038718\n",
      "621283617  0.038838\n",
      "621283363  0.039028\n",
      "621283332  0.039083\n",
      "621283327  0.039139\n",
      "621283619  0.039169\n",
      "621283388  0.039169\n",
      "621283391  0.039269\n",
      "621283324  0.039364\n",
      "621283366  0.039370\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13853  0.036122\n",
      "13863  0.036172\n",
      "13852  0.036298\n",
      "13917  0.037526\n",
      "13896  0.037555\n",
      "13884  0.037876\n",
      "13875  0.037966\n",
      "13844  0.038387\n",
      "13883  0.038903\n",
      "13897  0.038929\n",
      "13856  0.039049\n",
      "67919  0.039189\n",
      "13851  0.040592\n",
      "13873  0.040812\n",
      "13876  0.041013\n",
      "13864  0.041814\n",
      "13849  0.042236\n",
      "13759  0.042456\n",
      "13779  0.042661\n",
      "67863  0.042737\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.030604\n",
      "621283388  0.031048\n",
      "621283329  0.031267\n",
      "621283325  0.032221\n",
      "621283330  0.033065\n",
      "621283327  0.033957\n",
      "621283333  0.034486\n",
      "621283324  0.035459\n",
      "621283332  0.036146\n",
      "621283531  0.037031\n",
      "621283363  0.037730\n",
      "621283391  0.038744\n",
      "621283366  0.039734\n",
      "621283338  0.040107\n",
      "621283398  0.040680\n",
      "621283341  0.041123\n",
      "621283339  0.042909\n",
      "621283617  0.043461\n",
      "621283364  0.044186\n",
      "621283619  0.044455\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13896  0.027995\n",
      "13884  0.029140\n",
      "13917  0.031560\n",
      "13852  0.032634\n",
      "13897  0.034186\n",
      "13883  0.034801\n",
      "13863  0.035825\n",
      "13873  0.036729\n",
      "13851  0.036866\n",
      "13876  0.038515\n",
      "13864  0.039391\n",
      "13856  0.039898\n",
      "13849  0.039908\n",
      "13779  0.040417\n",
      "13844  0.041420\n",
      "13875  0.042464\n",
      "13759  0.042797\n",
      "67919  0.043408\n",
      "13853  0.046076\n",
      "67863  0.047500\n",
      ">>> Fastest Solution is hipblaslt 13896 0.027995499968528747\n",
      "M N K dtype 4096 216 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 216 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283338  0.019124\n",
      "621283388  0.020226\n",
      "621283390  0.021128\n",
      "621283327  0.021279\n",
      "621283329  0.021379\n",
      "621283335  0.021389\n",
      "621283321  0.021413\n",
      "621283365  0.021428\n",
      "621283339  0.021629\n",
      "621283328  0.021690\n",
      "621283334  0.021745\n",
      "621283323  0.021760\n",
      "621283364  0.021769\n",
      "621283322  0.022195\n",
      "621283326  0.022250\n",
      "621283363  0.022310\n",
      "621283330  0.022451\n",
      "621283340  0.022511\n",
      "621283325  0.022576\n",
      "621283391  0.022651\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13892  0.023869\n",
      "13893  0.024030\n",
      "13894  0.024300\n",
      "13856  0.024370\n",
      "13779  0.024931\n",
      "13759  0.024957\n",
      "67895  0.024976\n",
      "13873  0.024997\n",
      "13876  0.025017\n",
      "13769  0.025047\n",
      "13849  0.025192\n",
      "67863  0.025357\n",
      "67014  0.025363\n",
      "13860  0.025493\n",
      "13864  0.025809\n",
      "13790  0.025838\n",
      "13875  0.025858\n",
      "13897  0.025989\n",
      "13916  0.026190\n",
      "67869  0.026305\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283388  0.016319\n",
      "621283330  0.017113\n",
      "621283328  0.017194\n",
      "621283329  0.017282\n",
      "621283364  0.017554\n",
      "621283325  0.018250\n",
      "621283326  0.018430\n",
      "621283363  0.018452\n",
      "621283327  0.018632\n",
      "621283340  0.018690\n",
      "621283391  0.018823\n",
      "621283335  0.019310\n",
      "621283365  0.019526\n",
      "621283321  0.019728\n",
      "621283390  0.019797\n",
      "621283323  0.019880\n",
      "621283339  0.019961\n",
      "621283334  0.020017\n",
      "621283322  0.020769\n",
      "621283338  0.024906\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13864  0.021688\n",
      "13894  0.021720\n",
      "13779  0.021813\n",
      "13897  0.021833\n",
      "13856  0.021972\n",
      "13873  0.022301\n",
      "13875  0.022457\n",
      "13860  0.022493\n",
      "13849  0.022517\n",
      "13759  0.022549\n",
      "13876  0.022708\n",
      "67863  0.022724\n",
      "67895  0.022776\n",
      "13893  0.022936\n",
      "13916  0.022960\n",
      "13790  0.023133\n",
      "67869  0.025504\n",
      "13769  0.025529\n",
      "67014  0.025722\n",
      "13892  0.027817\n",
      ">>> Fastest Solution is rocblas 621283388 0.01631904989480972\n",
      "M N K dtype 768 208 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 208 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283322  0.018422\n",
      "621283244  0.019925\n",
      "621283393  0.020165\n",
      "621283530  0.020266\n",
      "621283334  0.021268\n",
      "621283321  0.021428\n",
      "621283328  0.021549\n",
      "621283238  0.021549\n",
      "621283325  0.021669\n",
      "621283246  0.021709\n",
      "621283392  0.021809\n",
      "621283319  0.021970\n",
      "621283330  0.022150\n",
      "621283388  0.022210\n",
      "621283395  0.022230\n",
      "621283531  0.022271\n",
      "621283320  0.022350\n",
      "621283245  0.022371\n",
      "621283323  0.022791\n",
      "621283332  0.023053\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13896  0.023393\n",
      "13883  0.023794\n",
      "13793  0.023924\n",
      "13897  0.024035\n",
      "67757  0.024155\n",
      "13884  0.024175\n",
      "67837  0.024475\n",
      "36974  0.024736\n",
      "13927  0.024746\n",
      "13912  0.024796\n",
      "13845  0.024936\n",
      "13906  0.024997\n",
      "13790  0.025077\n",
      "13746  0.025097\n",
      "13916  0.025338\n",
      "67777  0.025378\n",
      "13765  0.025748\n",
      "67808  0.025758\n",
      "67895  0.025779\n",
      "13831  0.025809\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283392  0.016540\n",
      "621283245  0.016545\n",
      "621283334  0.016802\n",
      "621283530  0.016818\n",
      "621283246  0.016981\n",
      "621283321  0.017063\n",
      "621283323  0.017101\n",
      "621283320  0.017131\n",
      "621283319  0.017321\n",
      "621283328  0.018823\n",
      "621283388  0.019103\n",
      "621283393  0.019206\n",
      "621283531  0.019586\n",
      "621283325  0.019596\n",
      "621283330  0.019813\n",
      "621283244  0.019883\n",
      "621283395  0.020053\n",
      "621283332  0.020450\n",
      "621283238  0.020551\n",
      "621283322  0.022076\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.020300\n",
      "67808  0.020504\n",
      "13897  0.020865\n",
      "13845  0.020871\n",
      "13906  0.021052\n",
      "67757  0.021405\n",
      "67837  0.021461\n",
      "13927  0.021538\n",
      "67895  0.021938\n",
      "13916  0.021996\n",
      "36974  0.022043\n",
      "13912  0.022313\n",
      "13746  0.022443\n",
      "13790  0.023002\n",
      "67777  0.023096\n",
      "13831  0.023228\n",
      "13883  0.023527\n",
      "13793  0.023602\n",
      "13765  0.024235\n",
      "13896  0.029812\n",
      ">>> Fastest Solution is rocblas 621283392 0.016539500653743745\n",
      "M N K dtype 4096 208 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 208 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283532  0.018783\n",
      "621283412  0.018984\n",
      "621283620  0.019003\n",
      "621283378  0.019003\n",
      "621283417  0.019164\n",
      "621283380  0.019404\n",
      "621283247  0.019444\n",
      "621283327  0.019484\n",
      "621283237  0.019524\n",
      "621283539  0.019605\n",
      "621283656  0.019664\n",
      "621283248  0.019725\n",
      "621283373  0.019825\n",
      "621283413  0.019865\n",
      "621283540  0.019865\n",
      "621283469  0.019925\n",
      "621283246  0.019946\n",
      "621283543  0.020106\n",
      "621283249  0.020126\n",
      "621283415  0.020165\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13912  0.021689\n",
      "13861  0.022852\n",
      "13911  0.022992\n",
      "67566  0.023153\n",
      "13891  0.023613\n",
      "66919  0.023975\n",
      "67086  0.024035\n",
      "67280  0.024135\n",
      "13858  0.024235\n",
      "67170  0.024275\n",
      "67162  0.024376\n",
      "66926  0.024475\n",
      "66864  0.024696\n",
      "67410  0.024696\n",
      "66880  0.024797\n",
      "68102  0.024916\n",
      "13772  0.024977\n",
      "67544  0.025037\n",
      "66899  0.025217\n",
      "66855  0.025238\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283532  0.012250\n",
      "621283378  0.016379\n",
      "621283380  0.016550\n",
      "621283248  0.016618\n",
      "621283249  0.016678\n",
      "621283469  0.016726\n",
      "621283247  0.016822\n",
      "621283417  0.016884\n",
      "621283539  0.016999\n",
      "621283327  0.017029\n",
      "621283656  0.017029\n",
      "621283373  0.017169\n",
      "621283413  0.017339\n",
      "621283246  0.017359\n",
      "621283540  0.017405\n",
      "621283237  0.017804\n",
      "621283543  0.018059\n",
      "621283415  0.018638\n",
      "621283620  0.019222\n",
      "621283412  0.023130\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13911  0.019629\n",
      "13858  0.020789\n",
      "67162  0.021388\n",
      "67566  0.021403\n",
      "66880  0.021665\n",
      "67086  0.022098\n",
      "66899  0.022122\n",
      "67280  0.022166\n",
      "66919  0.022184\n",
      "13891  0.022303\n",
      "66855  0.022375\n",
      "68102  0.022451\n",
      "13861  0.022511\n",
      "67544  0.022878\n",
      "13772  0.023012\n",
      "66864  0.023100\n",
      "67410  0.023361\n",
      "67170  0.023479\n",
      "66926  0.023638\n",
      "13912  0.024024\n",
      ">>> Fastest Solution is rocblas 621283532 0.012249749898910523\n",
      "M N K dtype 3584 208 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 208 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283391  0.035260\n",
      "621283388  0.036488\n",
      "621283364  0.036823\n",
      "621283325  0.036823\n",
      "621283328  0.037490\n",
      "621283329  0.037820\n",
      "621283330  0.037905\n",
      "621283396  0.038427\n",
      "621283390  0.038668\n",
      "621283332  0.038763\n",
      "621283327  0.038948\n",
      "621283617  0.039028\n",
      "621283324  0.039224\n",
      "621283366  0.039249\n",
      "621283338  0.039389\n",
      "621283398  0.039490\n",
      "621283365  0.039590\n",
      "621283399  0.039871\n",
      "621283331  0.040166\n",
      "621283395  0.040171\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13852  0.036017\n",
      "13917  0.036187\n",
      "13883  0.036718\n",
      "13884  0.037090\n",
      "13896  0.037359\n",
      "13853  0.037685\n",
      "13863  0.038006\n",
      "13897  0.038442\n",
      "13860  0.038888\n",
      "13759  0.039510\n",
      "13875  0.039510\n",
      "13873  0.040913\n",
      "13844  0.040933\n",
      "13876  0.041033\n",
      "67504  0.041308\n",
      "13849  0.041414\n",
      "13779  0.041549\n",
      "67869  0.041795\n",
      "13856  0.042196\n",
      "67856  0.042296\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283330  0.028719\n",
      "621283329  0.028770\n",
      "621283327  0.030312\n",
      "621283388  0.030675\n",
      "621283328  0.031109\n",
      "621283324  0.031471\n",
      "621283331  0.032418\n",
      "621283325  0.034453\n",
      "621283332  0.034650\n",
      "621283395  0.036910\n",
      "621283390  0.037822\n",
      "621283364  0.037970\n",
      "621283365  0.038283\n",
      "621283338  0.039476\n",
      "621283366  0.039682\n",
      "621283398  0.040654\n",
      "621283399  0.041737\n",
      "621283617  0.043092\n",
      "621283396  0.045854\n",
      "621283391  0.048596\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13896  0.027767\n",
      "13917  0.028180\n",
      "13884  0.028374\n",
      "13883  0.028406\n",
      "13897  0.031272\n",
      "13852  0.035607\n",
      "13863  0.035841\n",
      "13853  0.036876\n",
      "13873  0.037665\n",
      "13876  0.038618\n",
      "13875  0.039131\n",
      "13849  0.040311\n",
      "13779  0.040328\n",
      "13856  0.040708\n",
      "13844  0.041175\n",
      "67869  0.041454\n",
      "13759  0.042967\n",
      "67504  0.043102\n",
      "13860  0.043561\n",
      "67856  0.043599\n",
      ">>> Fastest Solution is hipblaslt 13896 0.02776694893836975\n",
      "M N K dtype 4096 208 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 208 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283363  0.019925\n",
      "621283326  0.020105\n",
      "621283330  0.020216\n",
      "621283333  0.020957\n",
      "621283329  0.021434\n",
      "621283341  0.021629\n",
      "621283321  0.021669\n",
      "621283338  0.021709\n",
      "621283328  0.021900\n",
      "621283327  0.022150\n",
      "621283388  0.022246\n",
      "621283334  0.022250\n",
      "621283324  0.022306\n",
      "621283325  0.022360\n",
      "621283340  0.022390\n",
      "621283366  0.022431\n",
      "621283322  0.022526\n",
      "621283337  0.022691\n",
      "621283339  0.022752\n",
      "621283335  0.022812\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13864  0.023158\n",
      "13894  0.023708\n",
      "13893  0.024215\n",
      "67895  0.024270\n",
      "13892  0.024736\n",
      "13856  0.024876\n",
      "13915  0.025007\n",
      "67833  0.025037\n",
      "13916  0.025067\n",
      "13860  0.025092\n",
      "13896  0.025227\n",
      "13790  0.025443\n",
      "67863  0.025658\n",
      "13863  0.025843\n",
      "13746  0.025874\n",
      "13914  0.025949\n",
      "13881  0.026034\n",
      "13849  0.026309\n",
      "13897  0.026379\n",
      "13876  0.026400\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283325  0.016584\n",
      "621283333  0.016686\n",
      "621283324  0.016945\n",
      "621283330  0.017081\n",
      "621283388  0.017378\n",
      "621283328  0.018157\n",
      "621283366  0.018181\n",
      "621283327  0.018715\n",
      "621283335  0.018718\n",
      "621283337  0.018747\n",
      "621283340  0.018871\n",
      "621283329  0.019015\n",
      "621283338  0.019031\n",
      "621283339  0.019306\n",
      "621283321  0.019877\n",
      "621283334  0.020169\n",
      "621283322  0.020226\n",
      "621283341  0.020885\n",
      "621283326  0.021449\n",
      "621283363  0.026785\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13876  0.021749\n",
      "13860  0.021858\n",
      "13856  0.021905\n",
      "13892  0.021969\n",
      "13849  0.022099\n",
      "13863  0.022173\n",
      "13915  0.022211\n",
      "13897  0.022373\n",
      "67833  0.022447\n",
      "13916  0.022502\n",
      "13893  0.022529\n",
      "13896  0.022555\n",
      "13746  0.022774\n",
      "67895  0.022904\n",
      "13894  0.022923\n",
      "67863  0.023527\n",
      "13790  0.023547\n",
      "13881  0.025182\n",
      "13914  0.025861\n",
      "13864  0.026975\n",
      ">>> Fastest Solution is rocblas 621283325 0.01658415049314499\n",
      "M N K dtype 768 200 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 200 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283244  0.018883\n",
      "621283530  0.018983\n",
      "621283393  0.020166\n",
      "621283321  0.020767\n",
      "621283319  0.020988\n",
      "621283334  0.021088\n",
      "621283238  0.021528\n",
      "621283392  0.021549\n",
      "621283323  0.021609\n",
      "621283328  0.021649\n",
      "621283325  0.021749\n",
      "621283388  0.021790\n",
      "621283320  0.021830\n",
      "621283330  0.022171\n",
      "621283531  0.022191\n",
      "621283329  0.022231\n",
      "621283246  0.022431\n",
      "621283322  0.023053\n",
      "621283332  0.023053\n",
      "621283327  0.023534\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13845  0.023634\n",
      "13896  0.024054\n",
      "13850  0.024255\n",
      "13925  0.024416\n",
      "36974  0.024475\n",
      "13917  0.024475\n",
      "13883  0.024556\n",
      "13779  0.024576\n",
      "13750  0.024626\n",
      "67865  0.024716\n",
      "67757  0.024776\n",
      "13906  0.024857\n",
      "67837  0.025117\n",
      "13927  0.025217\n",
      "67808  0.025257\n",
      "13743  0.025298\n",
      "13907  0.025353\n",
      "13884  0.025357\n",
      "13790  0.025398\n",
      "13788  0.025497\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283334  0.016325\n",
      "621283246  0.016570\n",
      "621283323  0.016668\n",
      "621283322  0.016780\n",
      "621283392  0.016896\n",
      "621283319  0.017289\n",
      "621283321  0.017315\n",
      "621283320  0.017524\n",
      "621283238  0.018785\n",
      "621283328  0.018955\n",
      "621283388  0.019155\n",
      "621283325  0.019182\n",
      "621283330  0.019406\n",
      "621283329  0.019562\n",
      "621283393  0.019885\n",
      "621283531  0.020142\n",
      "621283332  0.020208\n",
      "621283530  0.020723\n",
      "621283327  0.020957\n",
      "621283244  0.024477\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.020605\n",
      "67808  0.020695\n",
      "13883  0.020725\n",
      "13906  0.020885\n",
      "13927  0.020921\n",
      "13925  0.020970\n",
      "13850  0.021158\n",
      "67757  0.021266\n",
      "13917  0.021314\n",
      "67837  0.021441\n",
      "13779  0.021825\n",
      "36974  0.021886\n",
      "13907  0.022253\n",
      "13790  0.022293\n",
      "13743  0.022311\n",
      "13788  0.022533\n",
      "67865  0.022806\n",
      "13750  0.024487\n",
      "13896  0.024750\n",
      "13845  0.031550\n",
      ">>> Fastest Solution is rocblas 621283334 0.016324999928474426\n",
      "M N K dtype 4096 200 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 200 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283418  0.019043\n",
      "621283540  0.019384\n",
      "621283248  0.019404\n",
      "621283380  0.019444\n",
      "621283378  0.019545\n",
      "621283416  0.019565\n",
      "621283469  0.019605\n",
      "621283417  0.019664\n",
      "621283620  0.019824\n",
      "621283543  0.019845\n",
      "621283249  0.019885\n",
      "621283414  0.020065\n",
      "621283415  0.020186\n",
      "621283542  0.020186\n",
      "621283564  0.020226\n",
      "621283541  0.020246\n",
      "621283393  0.020446\n",
      "621283539  0.020567\n",
      "621283379  0.020627\n",
      "621283240  0.020627\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.022792\n",
      "67244  0.023153\n",
      "68873  0.023493\n",
      "67410  0.023694\n",
      "67280  0.023694\n",
      "67162  0.023814\n",
      "68893  0.023994\n",
      "66864  0.024175\n",
      "67544  0.024235\n",
      "67334  0.024295\n",
      "67536  0.024415\n",
      "13857  0.024535\n",
      "67631  0.024535\n",
      "13756  0.024556\n",
      "13858  0.024556\n",
      "67213  0.024636\n",
      "67527  0.024776\n",
      "13870  0.024857\n",
      "13912  0.024896\n",
      "66913  0.024957\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283379  0.016369\n",
      "621283539  0.016377\n",
      "621283564  0.016415\n",
      "621283417  0.016505\n",
      "621283620  0.016636\n",
      "621283249  0.016716\n",
      "621283469  0.016730\n",
      "621283416  0.017051\n",
      "621283378  0.017237\n",
      "621283240  0.017436\n",
      "621283380  0.017594\n",
      "621283393  0.018007\n",
      "621283543  0.018101\n",
      "621283248  0.018336\n",
      "621283415  0.018670\n",
      "621283414  0.018729\n",
      "621283542  0.018769\n",
      "621283541  0.018791\n",
      "621283540  0.019192\n",
      "621283418  0.025724\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13858  0.020787\n",
      "68873  0.021388\n",
      "67213  0.021487\n",
      "13912  0.021723\n",
      "67162  0.021848\n",
      "67527  0.021944\n",
      "67631  0.022056\n",
      "13857  0.022216\n",
      "13870  0.022246\n",
      "66864  0.022379\n",
      "67536  0.022383\n",
      "67280  0.022820\n",
      "67334  0.022896\n",
      "13756  0.023022\n",
      "68893  0.023074\n",
      "67544  0.023179\n",
      "66913  0.023471\n",
      "67410  0.023517\n",
      "67244  0.026424\n",
      "13911  0.029100\n",
      ">>> Fastest Solution is rocblas 621283379 0.016369099915027618\n",
      "M N K dtype 3584 200 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 200 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283366  0.034940\n",
      "621283328  0.036203\n",
      "621283330  0.036909\n",
      "621283388  0.037515\n",
      "621283325  0.037620\n",
      "621283331  0.038116\n",
      "621283329  0.038292\n",
      "621283390  0.038347\n",
      "621283391  0.038548\n",
      "621283324  0.038798\n",
      "621283332  0.038874\n",
      "621283364  0.038909\n",
      "621283365  0.038928\n",
      "621283335  0.039069\n",
      "621283327  0.039083\n",
      "621283363  0.039650\n",
      "621283399  0.039811\n",
      "621283338  0.039990\n",
      "621283339  0.040372\n",
      "621283532  0.041434\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13863  0.035200\n",
      "13844  0.035280\n",
      "13884  0.036392\n",
      "13853  0.036703\n",
      "13883  0.037570\n",
      "13852  0.038497\n",
      "13873  0.038507\n",
      "13875  0.038708\n",
      "13896  0.039053\n",
      "13897  0.039239\n",
      "13917  0.039921\n",
      "13876  0.040351\n",
      "13864  0.040512\n",
      "67833  0.041023\n",
      "13856  0.041554\n",
      "67869  0.041565\n",
      "13849  0.041694\n",
      "13791  0.042046\n",
      "13759  0.042296\n",
      "13860  0.042376\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.027351\n",
      "621283325  0.027837\n",
      "621283329  0.027995\n",
      "621283388  0.029225\n",
      "621283330  0.030656\n",
      "621283332  0.030812\n",
      "621283327  0.034732\n",
      "621283331  0.035396\n",
      "621283324  0.035450\n",
      "621283339  0.036120\n",
      "621283391  0.036701\n",
      "621283365  0.037198\n",
      "621283364  0.037579\n",
      "621283363  0.038245\n",
      "621283390  0.038994\n",
      "621283338  0.039069\n",
      "621283532  0.039305\n",
      "621283335  0.040049\n",
      "621283399  0.040153\n",
      "621283366  0.047203\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13852  0.028461\n",
      "13883  0.030655\n",
      "13853  0.032987\n",
      "13917  0.034030\n",
      "13896  0.034823\n",
      "13897  0.035313\n",
      "13884  0.035577\n",
      "67833  0.037916\n",
      "67869  0.038313\n",
      "13875  0.038886\n",
      "13860  0.039319\n",
      "13849  0.039486\n",
      "13856  0.039528\n",
      "13864  0.039790\n",
      "13876  0.041905\n",
      "13791  0.041907\n",
      "13759  0.042813\n",
      "13873  0.043094\n",
      "13844  0.043433\n",
      "13863  0.048087\n",
      ">>> Fastest Solution is rocblas 621283328 0.027351000905036928\n",
      "M N K dtype 4096 200 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 200 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283327  0.019349\n",
      "621283324  0.019584\n",
      "621283341  0.019805\n",
      "621283329  0.020101\n",
      "621283328  0.020196\n",
      "621283236  0.020552\n",
      "621283364  0.020707\n",
      "621283366  0.021188\n",
      "621283323  0.021434\n",
      "621283332  0.021443\n",
      "621283340  0.021529\n",
      "621283321  0.021699\n",
      "621283395  0.021784\n",
      "621283252  0.021870\n",
      "621283333  0.022000\n",
      "621283334  0.022055\n",
      "621283339  0.022070\n",
      "621283619  0.022150\n",
      "621283531  0.022256\n",
      "621283398  0.022261\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13746  0.023358\n",
      "13892  0.024090\n",
      "13893  0.024105\n",
      "13849  0.024435\n",
      "13864  0.024645\n",
      "13790  0.025052\n",
      "13860  0.025242\n",
      "13779  0.025253\n",
      "13916  0.025317\n",
      "67045  0.025353\n",
      "13777  0.025427\n",
      "13856  0.025573\n",
      "13896  0.025603\n",
      "67903  0.025739\n",
      "13915  0.025764\n",
      "67504  0.025764\n",
      "67863  0.025979\n",
      "13876  0.026059\n",
      "13894  0.026129\n",
      "13897  0.026135\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283324  0.015593\n",
      "621283328  0.016393\n",
      "621283329  0.016867\n",
      "621283327  0.017608\n",
      "621283364  0.017634\n",
      "621283340  0.018670\n",
      "621283366  0.018694\n",
      "621283332  0.018846\n",
      "621283333  0.019028\n",
      "621283339  0.019366\n",
      "621283321  0.019481\n",
      "621283323  0.019671\n",
      "621283334  0.019984\n",
      "621283236  0.020039\n",
      "621283398  0.020316\n",
      "621283395  0.020637\n",
      "621283531  0.020973\n",
      "621283619  0.022176\n",
      "621283341  0.022617\n",
      "621283252  0.026117\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13746  0.020424\n",
      "13849  0.020849\n",
      "13864  0.021326\n",
      "13915  0.021428\n",
      "13860  0.021481\n",
      "13897  0.021659\n",
      "13894  0.021799\n",
      "13876  0.021815\n",
      "13893  0.021856\n",
      "13896  0.021971\n",
      "13916  0.022293\n",
      "13790  0.022346\n",
      "67903  0.022559\n",
      "67045  0.022692\n",
      "13779  0.022814\n",
      "13856  0.022855\n",
      "67863  0.023142\n",
      "13777  0.023856\n",
      "13892  0.024173\n",
      "67504  0.024221\n",
      ">>> Fastest Solution is rocblas 621283324 0.015593349933624268\n",
      "M N K dtype 768 192 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 192 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.019545\n",
      "621283530  0.019545\n",
      "621283248  0.020722\n",
      "621283319  0.020747\n",
      "621283249  0.020817\n",
      "621283247  0.020883\n",
      "621283334  0.020987\n",
      "621283246  0.021027\n",
      "621283245  0.021108\n",
      "621283238  0.021208\n",
      "621283328  0.021328\n",
      "621283320  0.021389\n",
      "621283325  0.021489\n",
      "621283392  0.021489\n",
      "621283329  0.021569\n",
      "621283323  0.021629\n",
      "621283322  0.022070\n",
      "621283257  0.022310\n",
      "621283321  0.022491\n",
      "621283395  0.022551\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13927  0.024415\n",
      "13845  0.024435\n",
      "13896  0.024475\n",
      "13850  0.024516\n",
      "67865  0.024535\n",
      "13906  0.024556\n",
      "13884  0.024836\n",
      "13886  0.025298\n",
      "13779  0.025317\n",
      "13895  0.025398\n",
      "13883  0.025438\n",
      "13894  0.025478\n",
      "13857  0.025523\n",
      "13859  0.025558\n",
      "13925  0.025719\n",
      "13893  0.025758\n",
      "67837  0.025979\n",
      "13892  0.025999\n",
      "13790  0.026019\n",
      "13916  0.026160\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.015301\n",
      "621283322  0.016544\n",
      "621283246  0.016547\n",
      "621283334  0.016556\n",
      "621283319  0.016788\n",
      "621283320  0.016934\n",
      "621283323  0.016960\n",
      "621283248  0.017093\n",
      "621283321  0.017375\n",
      "621283392  0.017408\n",
      "621283238  0.018414\n",
      "621283247  0.018737\n",
      "621283249  0.018776\n",
      "621283329  0.019007\n",
      "621283328  0.019135\n",
      "621283325  0.019228\n",
      "621283395  0.020057\n",
      "621283530  0.020186\n",
      "621283257  0.020879\n",
      "621283393  0.026195\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13883  0.020330\n",
      "13925  0.020462\n",
      "67837  0.020940\n",
      "13884  0.020988\n",
      "13850  0.021234\n",
      "13857  0.021348\n",
      "13859  0.021663\n",
      "13916  0.021731\n",
      "13906  0.021779\n",
      "13779  0.021912\n",
      "13894  0.021942\n",
      "13893  0.022018\n",
      "13790  0.022110\n",
      "13895  0.022403\n",
      "13886  0.022429\n",
      "67865  0.022681\n",
      "13892  0.022754\n",
      "13896  0.022768\n",
      "13927  0.024682\n",
      "13845  0.027613\n",
      ">>> Fastest Solution is rocblas 621283245 0.015300700068473816\n",
      "M N K dtype 4096 192 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 192 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283620  0.018723\n",
      "621283378  0.018763\n",
      "621283379  0.019023\n",
      "621283539  0.019264\n",
      "621283416  0.019565\n",
      "621283380  0.019605\n",
      "621283540  0.019645\n",
      "621283413  0.019684\n",
      "621283330  0.019705\n",
      "621283418  0.019765\n",
      "621283414  0.019785\n",
      "621283543  0.019824\n",
      "621283469  0.019865\n",
      "621283417  0.019885\n",
      "621283412  0.019886\n",
      "621283249  0.019986\n",
      "621283541  0.020005\n",
      "621283363  0.020025\n",
      "621283336  0.020025\n",
      "621283393  0.020046\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13891  0.022832\n",
      "67527  0.022952\n",
      "68102  0.023212\n",
      "67410  0.023253\n",
      "67280  0.023253\n",
      "13858  0.023353\n",
      "68873  0.023413\n",
      "13903  0.023674\n",
      "67361  0.023794\n",
      "67162  0.024054\n",
      "67086  0.024155\n",
      "66848  0.024195\n",
      "66932  0.024376\n",
      "67272  0.024556\n",
      "13887  0.024575\n",
      "68726  0.024575\n",
      "67536  0.024676\n",
      "67289  0.024736\n",
      "67244  0.024856\n",
      "66879  0.024897\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283418  0.016119\n",
      "621283249  0.016287\n",
      "621283417  0.016445\n",
      "621283540  0.016616\n",
      "621283539  0.016636\n",
      "621283413  0.016718\n",
      "621283469  0.016842\n",
      "621283336  0.016850\n",
      "621283363  0.016864\n",
      "621283380  0.016868\n",
      "621283330  0.016956\n",
      "621283412  0.017097\n",
      "621283393  0.017311\n",
      "621283416  0.017594\n",
      "621283543  0.018269\n",
      "621283414  0.018474\n",
      "621283379  0.018678\n",
      "621283541  0.018682\n",
      "621283378  0.019390\n",
      "621283620  0.026278\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67244  0.021507\n",
      "13887  0.021527\n",
      "68873  0.021529\n",
      "67162  0.021645\n",
      "68726  0.021653\n",
      "68102  0.021811\n",
      "13903  0.021920\n",
      "67361  0.022046\n",
      "67086  0.022060\n",
      "67536  0.022246\n",
      "66879  0.022515\n",
      "66932  0.022585\n",
      "13858  0.022597\n",
      "67272  0.022677\n",
      "67280  0.022712\n",
      "67289  0.022806\n",
      "66848  0.022952\n",
      "67410  0.023724\n",
      "67527  0.026151\n",
      "13891  0.028134\n",
      ">>> Fastest Solution is rocblas 621283418 0.016118550300598146\n",
      "M N K dtype 3584 192 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 192 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283330  0.031682\n",
      "621283332  0.032479\n",
      "621283328  0.032924\n",
      "621283338  0.033376\n",
      "621283325  0.034358\n",
      "621283388  0.034890\n",
      "621283327  0.034934\n",
      "621283399  0.035220\n",
      "621283329  0.035340\n",
      "621283324  0.036042\n",
      "621283331  0.036091\n",
      "621283394  0.036974\n",
      "621283395  0.037630\n",
      "621283236  0.037705\n",
      "621283531  0.037790\n",
      "621283340  0.037906\n",
      "621283364  0.038027\n",
      "621283342  0.038046\n",
      "621283237  0.038648\n",
      "621283363  0.038989\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.032543\n",
      "13852  0.032639\n",
      "13883  0.032690\n",
      "13917  0.033365\n",
      "13896  0.034984\n",
      "13884  0.035009\n",
      "13732  0.035741\n",
      "13853  0.035882\n",
      "13863  0.036102\n",
      "67849  0.036102\n",
      "13779  0.036598\n",
      "13845  0.036823\n",
      "13844  0.037064\n",
      "67817  0.037064\n",
      "67895  0.037329\n",
      "13860  0.037425\n",
      "13849  0.037445\n",
      "13851  0.037926\n",
      "67837  0.038076\n",
      "13850  0.038081\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283327  0.026919\n",
      "621283388  0.027502\n",
      "621283328  0.027601\n",
      "621283325  0.028083\n",
      "621283324  0.029098\n",
      "621283395  0.029542\n",
      "621283329  0.029567\n",
      "621283332  0.030407\n",
      "621283394  0.030568\n",
      "621283531  0.031430\n",
      "621283331  0.032622\n",
      "621283330  0.033724\n",
      "621283364  0.035418\n",
      "621283363  0.036523\n",
      "621283342  0.039437\n",
      "621283237  0.039834\n",
      "621283338  0.041474\n",
      "621283399  0.042075\n",
      "621283340  0.042222\n",
      "621283236  0.043521\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13917  0.026680\n",
      "13883  0.026731\n",
      "13884  0.026987\n",
      "13896  0.027522\n",
      "13852  0.028980\n",
      "13863  0.032375\n",
      "13845  0.033147\n",
      "13853  0.033582\n",
      "13850  0.034521\n",
      "13897  0.034580\n",
      "67849  0.034721\n",
      "13779  0.035658\n",
      "13732  0.035986\n",
      "67895  0.036662\n",
      "67837  0.036746\n",
      "67817  0.037581\n",
      "13851  0.037910\n",
      "13849  0.038131\n",
      "13860  0.038742\n",
      "13844  0.041047\n",
      ">>> Fastest Solution is hipblaslt 13917 0.02668049931526184\n",
      "M N K dtype 4096 192 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 192 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283398  0.018883\n",
      "621283363  0.019064\n",
      "621283399  0.019343\n",
      "621283333  0.019469\n",
      "621283532  0.019845\n",
      "621283531  0.019960\n",
      "621283394  0.020251\n",
      "621283617  0.020406\n",
      "621283321  0.021038\n",
      "621283328  0.021128\n",
      "621283337  0.021209\n",
      "621283329  0.021238\n",
      "621283331  0.021324\n",
      "621283341  0.021509\n",
      "621283323  0.021554\n",
      "621283322  0.021660\n",
      "621283395  0.021764\n",
      "621283334  0.021910\n",
      "621283619  0.022210\n",
      "621283364  0.022230\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67825  0.024015\n",
      "13860  0.024035\n",
      "13886  0.024110\n",
      "13859  0.024446\n",
      "13892  0.024486\n",
      "67863  0.024636\n",
      "67504  0.024816\n",
      "13916  0.024857\n",
      "13914  0.024936\n",
      "67118  0.025157\n",
      "13894  0.025247\n",
      "13898  0.025332\n",
      "13915  0.025843\n",
      "13864  0.026019\n",
      "67766  0.026039\n",
      "67817  0.026049\n",
      "13893  0.026064\n",
      "13881  0.026165\n",
      "13855  0.026184\n",
      "67833  0.026194\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.016934\n",
      "621283329  0.017017\n",
      "621283531  0.017608\n",
      "621283364  0.017636\n",
      "621283331  0.017937\n",
      "621283394  0.018061\n",
      "621283395  0.018304\n",
      "621283333  0.018308\n",
      "621283617  0.019406\n",
      "621283321  0.019410\n",
      "621283323  0.019532\n",
      "621283532  0.019556\n",
      "621283334  0.019644\n",
      "621283322  0.019806\n",
      "621283341  0.019849\n",
      "621283619  0.019923\n",
      "621283337  0.020348\n",
      "621283399  0.020988\n",
      "621283363  0.021631\n",
      "621283398  0.025664\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13894  0.020977\n",
      "13893  0.021147\n",
      "13892  0.021364\n",
      "67817  0.021428\n",
      "13915  0.021623\n",
      "67766  0.021887\n",
      "13916  0.022050\n",
      "67833  0.022201\n",
      "13864  0.022202\n",
      "13860  0.022740\n",
      "67504  0.022824\n",
      "13886  0.023184\n",
      "13881  0.023486\n",
      "67863  0.023495\n",
      "13855  0.023556\n",
      "13859  0.023570\n",
      "13914  0.023733\n",
      "67118  0.024245\n",
      "13898  0.024902\n",
      "67825  0.032516\n",
      ">>> Fastest Solution is rocblas 621283328 0.016933900117874146\n",
      "M N K dtype 768 184 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 184 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.019444\n",
      "621283244  0.019645\n",
      "621283323  0.020627\n",
      "621283247  0.020852\n",
      "621283322  0.020927\n",
      "621283321  0.021068\n",
      "621283249  0.021218\n",
      "621283248  0.021223\n",
      "621283238  0.021449\n",
      "621283328  0.021589\n",
      "621283392  0.021669\n",
      "621283319  0.021729\n",
      "621283325  0.021729\n",
      "621283246  0.022090\n",
      "621283245  0.022231\n",
      "621283330  0.022250\n",
      "621283257  0.022310\n",
      "621283329  0.022350\n",
      "621283395  0.023072\n",
      "621283320  0.023153\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13925  0.023232\n",
      "13852  0.023573\n",
      "13884  0.023734\n",
      "67837  0.024015\n",
      "13850  0.024035\n",
      "13845  0.024235\n",
      "13896  0.024395\n",
      "67808  0.024556\n",
      "13730  0.024576\n",
      "13788  0.024596\n",
      "67757  0.024616\n",
      "66998  0.024736\n",
      "13916  0.024817\n",
      "67678  0.025016\n",
      "13848  0.025257\n",
      "13755  0.025438\n",
      "67865  0.025598\n",
      "13917  0.025618\n",
      "13906  0.025658\n",
      "66941  0.025698\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283323  0.014665\n",
      "621283245  0.015585\n",
      "621283322  0.016676\n",
      "621283246  0.016928\n",
      "621283392  0.017145\n",
      "621283321  0.017247\n",
      "621283320  0.017381\n",
      "621283319  0.017560\n",
      "621283247  0.018285\n",
      "621283248  0.018616\n",
      "621283249  0.018675\n",
      "621283238  0.018698\n",
      "621283328  0.018931\n",
      "621283325  0.018971\n",
      "621283330  0.019180\n",
      "621283329  0.019340\n",
      "621283395  0.019721\n",
      "621283257  0.019849\n",
      "621283244  0.021096\n",
      "621283530  0.024335\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13845  0.020238\n",
      "67757  0.020452\n",
      "13917  0.020883\n",
      "13896  0.021170\n",
      "13850  0.021182\n",
      "67808  0.021192\n",
      "13906  0.021310\n",
      "67865  0.021627\n",
      "13916  0.021725\n",
      "67678  0.021743\n",
      "67837  0.022182\n",
      "66998  0.022192\n",
      "13788  0.022297\n",
      "13848  0.022319\n",
      "13884  0.022890\n",
      "13730  0.023016\n",
      "13755  0.023060\n",
      "66941  0.023946\n",
      "13852  0.026588\n",
      "13925  0.029212\n",
      ">>> Fastest Solution is rocblas 621283323 0.014665299654006958\n",
      "M N K dtype 4096 184 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 184 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283414  0.018562\n",
      "621283378  0.018823\n",
      "621283333  0.018923\n",
      "621283379  0.018963\n",
      "621283416  0.019103\n",
      "621283380  0.019103\n",
      "621283406  0.019283\n",
      "621283619  0.019384\n",
      "621283620  0.019464\n",
      "621283411  0.019505\n",
      "621283417  0.019685\n",
      "621283564  0.019725\n",
      "621283543  0.019745\n",
      "621283539  0.019825\n",
      "621283335  0.019925\n",
      "621283329  0.019986\n",
      "621283248  0.020005\n",
      "621283412  0.020025\n",
      "621283365  0.020025\n",
      "621283249  0.020126\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.022812\n",
      "13891  0.023232\n",
      "67361  0.023935\n",
      "67410  0.024015\n",
      "67527  0.024175\n",
      "67244  0.024315\n",
      "68893  0.024736\n",
      "67544  0.024796\n",
      "67566  0.024836\n",
      "67086  0.024836\n",
      "13918  0.024916\n",
      "67501  0.024936\n",
      "67170  0.025097\n",
      "66923  0.025117\n",
      "66919  0.025117\n",
      "67213  0.025137\n",
      "13831  0.025217\n",
      "66870  0.025257\n",
      "67156  0.025277\n",
      "13878  0.025357\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283539  0.016030\n",
      "621283412  0.016079\n",
      "621283329  0.016173\n",
      "621283417  0.016249\n",
      "621283248  0.016291\n",
      "621283620  0.016295\n",
      "621283249  0.016353\n",
      "621283564  0.016363\n",
      "621283335  0.016630\n",
      "621283333  0.016658\n",
      "621283379  0.016704\n",
      "621283365  0.016744\n",
      "621283411  0.016856\n",
      "621283380  0.016882\n",
      "621283378  0.016956\n",
      "621283406  0.017263\n",
      "621283416  0.017339\n",
      "621283619  0.017929\n",
      "621283543  0.018167\n",
      "621283414  0.026346\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13891  0.020342\n",
      "67566  0.021334\n",
      "67213  0.021417\n",
      "13918  0.021789\n",
      "13878  0.022126\n",
      "13831  0.022240\n",
      "67501  0.022323\n",
      "67086  0.022405\n",
      "67244  0.022503\n",
      "66919  0.022539\n",
      "67361  0.022607\n",
      "67544  0.022611\n",
      "68893  0.022693\n",
      "67156  0.022748\n",
      "66923  0.022814\n",
      "66870  0.022900\n",
      "67527  0.022902\n",
      "67170  0.023146\n",
      "67410  0.023750\n",
      "13911  0.029359\n",
      ">>> Fastest Solution is rocblas 621283539 0.01603035032749176\n",
      "M N K dtype 3584 184 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 184 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283237  0.031431\n",
      "621283325  0.032368\n",
      "621283388  0.032724\n",
      "621283328  0.033677\n",
      "621283329  0.033937\n",
      "621283332  0.034378\n",
      "621283330  0.034493\n",
      "621283327  0.035060\n",
      "621283324  0.035275\n",
      "621283394  0.035751\n",
      "621283395  0.037580\n",
      "621283531  0.037906\n",
      "621283366  0.037946\n",
      "621283331  0.038001\n",
      "621283364  0.039350\n",
      "621283390  0.039369\n",
      "621283391  0.039450\n",
      "621283617  0.039590\n",
      "621283333  0.039976\n",
      "621283363  0.040231\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13883  0.034649\n",
      "13852  0.034694\n",
      "13884  0.035380\n",
      "13897  0.035902\n",
      "13896  0.035977\n",
      "13863  0.036643\n",
      "13732  0.036663\n",
      "13853  0.036883\n",
      "13790  0.037545\n",
      "13739  0.037565\n",
      "13917  0.037881\n",
      "67849  0.037886\n",
      "13779  0.038142\n",
      "13845  0.038201\n",
      "13850  0.038688\n",
      "13849  0.039168\n",
      "13864  0.039169\n",
      "67869  0.039570\n",
      "67837  0.039690\n",
      "13875  0.039990\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.027083\n",
      "621283324  0.027107\n",
      "621283327  0.027136\n",
      "621283388  0.027247\n",
      "621283330  0.027292\n",
      "621283325  0.027440\n",
      "621283332  0.027528\n",
      "621283329  0.028318\n",
      "621283531  0.030362\n",
      "621283395  0.031623\n",
      "621283394  0.032313\n",
      "621283331  0.034400\n",
      "621283363  0.036318\n",
      "621283364  0.036976\n",
      "621283391  0.037267\n",
      "621283366  0.037992\n",
      "621283390  0.038008\n",
      "621283333  0.039570\n",
      "621283617  0.040356\n",
      "621283237  0.048562\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13896  0.026564\n",
      "13897  0.026983\n",
      "13884  0.027366\n",
      "13852  0.028920\n",
      "13850  0.031346\n",
      "13883  0.033430\n",
      "13853  0.033664\n",
      "13917  0.033790\n",
      "13845  0.034238\n",
      "13863  0.034725\n",
      "13779  0.036582\n",
      "67849  0.036705\n",
      "13732  0.037008\n",
      "67869  0.037112\n",
      "67837  0.037277\n",
      "13790  0.037374\n",
      "13864  0.038062\n",
      "13739  0.038784\n",
      "13875  0.039886\n",
      "13849  0.041428\n",
      ">>> Fastest Solution is hipblaslt 13896 0.02656424939632416\n",
      "M N K dtype 4096 184 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 184 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283365  0.018903\n",
      "621283337  0.019925\n",
      "621283531  0.019970\n",
      "621283366  0.020025\n",
      "621283333  0.020150\n",
      "621283330  0.020266\n",
      "621283363  0.020767\n",
      "621283235  0.020767\n",
      "621283617  0.020787\n",
      "621283339  0.020787\n",
      "621283324  0.020822\n",
      "621283395  0.021123\n",
      "621283323  0.021173\n",
      "621283321  0.021254\n",
      "621283532  0.021288\n",
      "621283334  0.021363\n",
      "621283335  0.021629\n",
      "621283331  0.021629\n",
      "621283327  0.021689\n",
      "621283341  0.021869\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13893  0.024300\n",
      "13855  0.024575\n",
      "67885  0.024606\n",
      "67863  0.024696\n",
      "67118  0.024736\n",
      "13886  0.024876\n",
      "66979  0.024927\n",
      "66966  0.025042\n",
      "13881  0.025057\n",
      "67817  0.025293\n",
      "13894  0.025298\n",
      "13880  0.025423\n",
      "13791  0.025438\n",
      "13853  0.025457\n",
      "13860  0.025578\n",
      "13915  0.025839\n",
      "13914  0.025944\n",
      "67823  0.026019\n",
      "67833  0.026194\n",
      "13739  0.026239\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283324  0.017705\n",
      "621283330  0.017753\n",
      "621283335  0.017756\n",
      "621283363  0.017877\n",
      "621283327  0.018086\n",
      "621283331  0.018141\n",
      "621283333  0.018282\n",
      "621283366  0.018370\n",
      "621283321  0.018564\n",
      "621283531  0.018568\n",
      "621283339  0.018658\n",
      "621283395  0.018865\n",
      "621283323  0.018993\n",
      "621283341  0.019161\n",
      "621283334  0.019267\n",
      "621283532  0.019639\n",
      "621283235  0.019765\n",
      "621283617  0.020851\n",
      "621283337  0.021862\n",
      "621283365  0.025873\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13915  0.021175\n",
      "13894  0.021398\n",
      "67823  0.021709\n",
      "67833  0.022084\n",
      "13860  0.022206\n",
      "13791  0.022355\n",
      "13881  0.022374\n",
      "13853  0.022475\n",
      "67885  0.022755\n",
      "13739  0.022862\n",
      "67863  0.023094\n",
      "67817  0.023379\n",
      "13886  0.023490\n",
      "66979  0.023556\n",
      "13893  0.023792\n",
      "13914  0.024021\n",
      "67118  0.024301\n",
      "13855  0.024422\n",
      "13880  0.024689\n",
      "66966  0.026251\n",
      ">>> Fastest Solution is rocblas 621283324 0.01770464926958084\n",
      "M N K dtype 768 176 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 176 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.019584\n",
      "621283244  0.019785\n",
      "621283247  0.020572\n",
      "621283245  0.020707\n",
      "621283238  0.020807\n",
      "621283249  0.021063\n",
      "621283393  0.021209\n",
      "621283323  0.021328\n",
      "621283248  0.021364\n",
      "621283321  0.021368\n",
      "621283246  0.021508\n",
      "621283328  0.021650\n",
      "621283392  0.021809\n",
      "621283325  0.021830\n",
      "621283322  0.021990\n",
      "621283388  0.022070\n",
      "621283330  0.022131\n",
      "621283319  0.022210\n",
      "621283257  0.022231\n",
      "621283329  0.022331\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13884  0.023654\n",
      "13896  0.023694\n",
      "13897  0.023914\n",
      "67757  0.024135\n",
      "13850  0.024175\n",
      "66998  0.024275\n",
      "67865  0.024556\n",
      "67746  0.024636\n",
      "67678  0.024656\n",
      "13917  0.024716\n",
      "13852  0.024876\n",
      "67808  0.024957\n",
      "13845  0.024997\n",
      "13790  0.025458\n",
      "13779  0.025538\n",
      "13927  0.025668\n",
      "13746  0.025718\n",
      "66941  0.025798\n",
      "67631  0.025874\n",
      "13855  0.025879\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.016113\n",
      "621283247  0.016309\n",
      "621283322  0.016529\n",
      "621283393  0.017017\n",
      "621283246  0.017111\n",
      "621283392  0.017203\n",
      "621283319  0.017253\n",
      "621283249  0.017405\n",
      "621283248  0.017410\n",
      "621283323  0.017552\n",
      "621283321  0.017744\n",
      "621283328  0.018899\n",
      "621283325  0.018939\n",
      "621283388  0.019063\n",
      "621283330  0.019192\n",
      "621283329  0.019322\n",
      "621283257  0.019735\n",
      "621283244  0.020691\n",
      "621283238  0.021296\n",
      "621283530  0.022543\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13845  0.020489\n",
      "13852  0.020581\n",
      "13850  0.020809\n",
      "67746  0.021288\n",
      "67808  0.021439\n",
      "67678  0.021445\n",
      "66998  0.021607\n",
      "67757  0.021755\n",
      "13917  0.021783\n",
      "67865  0.021838\n",
      "13897  0.021866\n",
      "13746  0.022030\n",
      "13790  0.022092\n",
      "13779  0.022124\n",
      "13855  0.022142\n",
      "67631  0.022727\n",
      "13927  0.022749\n",
      "13896  0.022842\n",
      "66941  0.023485\n",
      "13884  0.029785\n",
      ">>> Fastest Solution is rocblas 621283245 0.016112500429153444\n",
      "M N K dtype 4096 176 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 176 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283620  0.018943\n",
      "621283414  0.019264\n",
      "621283413  0.019424\n",
      "621283380  0.019424\n",
      "621283417  0.019444\n",
      "621283412  0.019505\n",
      "621283410  0.019545\n",
      "621283469  0.019565\n",
      "621283415  0.019664\n",
      "621283379  0.019684\n",
      "621283542  0.019765\n",
      "621283617  0.019845\n",
      "621283416  0.019886\n",
      "621283540  0.019905\n",
      "621283541  0.019905\n",
      "621283394  0.020146\n",
      "621283418  0.020266\n",
      "621283656  0.020427\n",
      "621283243  0.020446\n",
      "621283342  0.020506\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.022812\n",
      "13880  0.023313\n",
      "68893  0.023674\n",
      "67361  0.023734\n",
      "67280  0.023734\n",
      "67544  0.023774\n",
      "67170  0.023774\n",
      "67410  0.024115\n",
      "66923  0.024355\n",
      "13831  0.024475\n",
      "67536  0.024556\n",
      "67602  0.024916\n",
      "67501  0.025056\n",
      "66919  0.025057\n",
      "67272  0.025077\n",
      "67086  0.025157\n",
      "66899  0.025197\n",
      "66868  0.025217\n",
      "13918  0.025257\n",
      "68102  0.025257\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283413  0.015625\n",
      "621283379  0.016058\n",
      "621283418  0.016103\n",
      "621283410  0.016243\n",
      "621283469  0.016660\n",
      "621283416  0.016670\n",
      "621283417  0.016742\n",
      "621283415  0.017005\n",
      "621283412  0.017103\n",
      "621283542  0.017151\n",
      "621283380  0.017215\n",
      "621283541  0.017305\n",
      "621283540  0.017383\n",
      "621283394  0.017454\n",
      "621283617  0.017644\n",
      "621283342  0.017955\n",
      "621283656  0.018281\n",
      "621283243  0.019194\n",
      "621283414  0.022509\n",
      "621283620  0.027055\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "66899  0.021617\n",
      "67086  0.021695\n",
      "67602  0.021739\n",
      "13918  0.021822\n",
      "67280  0.021894\n",
      "67501  0.021996\n",
      "66919  0.022036\n",
      "67536  0.022084\n",
      "68102  0.022250\n",
      "13831  0.022399\n",
      "67272  0.022515\n",
      "66923  0.022581\n",
      "67410  0.022653\n",
      "67170  0.022752\n",
      "13880  0.022898\n",
      "67361  0.023018\n",
      "66868  0.023022\n",
      "67544  0.023088\n",
      "68893  0.023555\n",
      "13911  0.029830\n",
      ">>> Fastest Solution is rocblas 621283413 0.015625450015068054\n",
      "M N K dtype 3584 176 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 176 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283332  0.031872\n",
      "621283324  0.032137\n",
      "621283329  0.032439\n",
      "621283325  0.033014\n",
      "621283328  0.033255\n",
      "621283333  0.033957\n",
      "621283388  0.033983\n",
      "621283395  0.034102\n",
      "621283330  0.034293\n",
      "621283327  0.035234\n",
      "621283364  0.035400\n",
      "621283617  0.035480\n",
      "621283363  0.035560\n",
      "621283531  0.038192\n",
      "621283335  0.038688\n",
      "621283390  0.038868\n",
      "621283235  0.038989\n",
      "621283342  0.039089\n",
      "621283366  0.039229\n",
      "621283339  0.039389\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.031275\n",
      "67849  0.031311\n",
      "13852  0.031662\n",
      "13884  0.032920\n",
      "13853  0.033095\n",
      "13896  0.034088\n",
      "13883  0.034458\n",
      "13845  0.035626\n",
      "13863  0.036262\n",
      "13732  0.036362\n",
      "13917  0.036563\n",
      "13851  0.037124\n",
      "13850  0.037691\n",
      "13759  0.038367\n",
      "67869  0.039149\n",
      "13779  0.039610\n",
      "67895  0.039905\n",
      "13873  0.040071\n",
      "67817  0.040151\n",
      "13864  0.040171\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283325  0.026067\n",
      "621283328  0.026498\n",
      "621283330  0.027045\n",
      "621283388  0.027172\n",
      "621283327  0.027230\n",
      "621283329  0.027422\n",
      "621283324  0.029579\n",
      "621283395  0.030557\n",
      "621283333  0.034250\n",
      "621283332  0.034647\n",
      "621283366  0.035394\n",
      "621283531  0.035700\n",
      "621283363  0.035962\n",
      "621283335  0.036226\n",
      "621283390  0.036747\n",
      "621283339  0.036801\n",
      "621283364  0.039004\n",
      "621283235  0.039540\n",
      "621283617  0.040516\n",
      "621283342  0.040530\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13883  0.026926\n",
      "13896  0.027334\n",
      "13852  0.027812\n",
      "13884  0.028182\n",
      "13845  0.031956\n",
      "13863  0.033500\n",
      "13917  0.033646\n",
      "13897  0.034029\n",
      "13853  0.034316\n",
      "13850  0.034917\n",
      "13732  0.035067\n",
      "67895  0.036008\n",
      "67869  0.036495\n",
      "13851  0.037104\n",
      "13779  0.037474\n",
      "67817  0.037527\n",
      "13864  0.038714\n",
      "13873  0.038820\n",
      "67849  0.038900\n",
      "13759  0.040504\n",
      ">>> Fastest Solution is rocblas 621283325 0.026067098975181578\n",
      "M N K dtype 4096 176 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 176 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283531  0.019359\n",
      "621283333  0.019654\n",
      "621283394  0.019860\n",
      "621283395  0.020010\n",
      "621283340  0.020105\n",
      "621283329  0.020742\n",
      "621283397  0.020767\n",
      "621283321  0.020872\n",
      "621283339  0.020927\n",
      "621283532  0.021168\n",
      "621283319  0.021188\n",
      "621283617  0.021389\n",
      "621283234  0.021439\n",
      "621283233  0.021469\n",
      "621283322  0.021504\n",
      "621283341  0.021549\n",
      "621283323  0.021784\n",
      "621283331  0.021915\n",
      "621283334  0.022045\n",
      "621283363  0.022091\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13849  0.023052\n",
      "13860  0.023333\n",
      "13881  0.024150\n",
      "67823  0.024175\n",
      "13914  0.024465\n",
      "13893  0.024546\n",
      "13739  0.024626\n",
      "67856  0.024756\n",
      "13859  0.024801\n",
      "67746  0.024806\n",
      "13894  0.024841\n",
      "67504  0.025057\n",
      "13886  0.025298\n",
      "13892  0.025649\n",
      "67833  0.025819\n",
      "67837  0.025914\n",
      "13855  0.025924\n",
      "67825  0.025939\n",
      "67766  0.025969\n",
      "13875  0.025979\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283394  0.016221\n",
      "621283329  0.016271\n",
      "621283395  0.017164\n",
      "621283363  0.017498\n",
      "621283331  0.017777\n",
      "621283339  0.018626\n",
      "621283341  0.019163\n",
      "621283323  0.019168\n",
      "621283334  0.019314\n",
      "621283322  0.019321\n",
      "621283321  0.019416\n",
      "621283333  0.019705\n",
      "621283532  0.019843\n",
      "621283531  0.019919\n",
      "621283617  0.020312\n",
      "621283397  0.020705\n",
      "621283340  0.020857\n",
      "621283233  0.021102\n",
      "621283319  0.021297\n",
      "621283234  0.021312\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13894  0.021216\n",
      "13893  0.021256\n",
      "67823  0.021581\n",
      "13892  0.021637\n",
      "67833  0.021799\n",
      "13875  0.022487\n",
      "13881  0.022525\n",
      "67856  0.022577\n",
      "67825  0.022719\n",
      "13886  0.022759\n",
      "67837  0.022838\n",
      "13859  0.022925\n",
      "67766  0.023158\n",
      "67504  0.023452\n",
      "13739  0.023540\n",
      "13860  0.023595\n",
      "13855  0.023784\n",
      "13914  0.024213\n",
      "67746  0.024572\n",
      "13849  0.026550\n",
      ">>> Fastest Solution is rocblas 621283394 0.016220800578594208\n",
      "M N K dtype 768 168 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 168 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.018923\n",
      "621283393  0.019825\n",
      "621283245  0.020025\n",
      "621283248  0.020436\n",
      "621283244  0.020467\n",
      "621283334  0.020567\n",
      "621283247  0.021063\n",
      "621283392  0.021108\n",
      "621283319  0.021389\n",
      "621283238  0.021409\n",
      "621283321  0.021449\n",
      "621283328  0.021609\n",
      "621283323  0.021629\n",
      "621283325  0.021649\n",
      "621283388  0.021729\n",
      "621283249  0.021789\n",
      "621283246  0.021790\n",
      "621283322  0.022010\n",
      "621283330  0.022050\n",
      "621283329  0.022231\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13884  0.023293\n",
      "13927  0.023704\n",
      "13850  0.023734\n",
      "13845  0.024054\n",
      "13917  0.024155\n",
      "13896  0.024215\n",
      "67837  0.024516\n",
      "13730  0.024756\n",
      "67757  0.024916\n",
      "13843  0.024957\n",
      "13897  0.025097\n",
      "67808  0.025198\n",
      "13894  0.025317\n",
      "67678  0.025578\n",
      "67895  0.025578\n",
      "13916  0.025598\n",
      "13858  0.025773\n",
      "13855  0.025778\n",
      "13892  0.025779\n",
      "13846  0.025798\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283244  0.015493\n",
      "621283334  0.016247\n",
      "621283322  0.016493\n",
      "621283321  0.016852\n",
      "621283246  0.016886\n",
      "621283319  0.016916\n",
      "621283245  0.017121\n",
      "621283247  0.017397\n",
      "621283249  0.017493\n",
      "621283323  0.017546\n",
      "621283248  0.017745\n",
      "621283392  0.018265\n",
      "621283238  0.018680\n",
      "621283328  0.018895\n",
      "621283325  0.019087\n",
      "621283330  0.019326\n",
      "621283388  0.019378\n",
      "621283329  0.019504\n",
      "621283393  0.021038\n",
      "621283530  0.021641\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67808  0.020310\n",
      "13896  0.020583\n",
      "13897  0.020631\n",
      "67757  0.020697\n",
      "13917  0.020974\n",
      "13850  0.021122\n",
      "67837  0.021180\n",
      "13845  0.021328\n",
      "13855  0.021639\n",
      "13858  0.021779\n",
      "13916  0.021808\n",
      "13927  0.021920\n",
      "13730  0.022106\n",
      "67678  0.022180\n",
      "13846  0.022190\n",
      "67895  0.022303\n",
      "13894  0.022327\n",
      "13892  0.022379\n",
      "13843  0.022607\n",
      "13884  0.030363\n",
      ">>> Fastest Solution is rocblas 621283244 0.015493150055408477\n",
      "M N K dtype 4096 168 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 168 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283541  0.018623\n",
      "621283417  0.019224\n",
      "621283327  0.019264\n",
      "621283410  0.019344\n",
      "621283620  0.019364\n",
      "621283380  0.019424\n",
      "621283540  0.019845\n",
      "621283543  0.019865\n",
      "621283415  0.019905\n",
      "621283379  0.020046\n",
      "621283243  0.020286\n",
      "621283542  0.020346\n",
      "621283240  0.020467\n",
      "621283241  0.020527\n",
      "621283621  0.020606\n",
      "621283539  0.020667\n",
      "621283418  0.020667\n",
      "621283338  0.020687\n",
      "621283334  0.020727\n",
      "621283342  0.020727\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13891  0.022672\n",
      "67156  0.022831\n",
      "13756  0.023312\n",
      "13903  0.023433\n",
      "13911  0.023473\n",
      "67280  0.023854\n",
      "67536  0.024015\n",
      "67501  0.024115\n",
      "67361  0.024255\n",
      "67544  0.024416\n",
      "67213  0.024556\n",
      "13929  0.024736\n",
      "66855  0.024816\n",
      "66868  0.024816\n",
      "68873  0.024857\n",
      "67162  0.024957\n",
      "66857  0.024997\n",
      "67104  0.025017\n",
      "67410  0.025037\n",
      "67777  0.025037\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283410  0.015473\n",
      "621283539  0.015756\n",
      "621283379  0.015802\n",
      "621283240  0.016108\n",
      "621283418  0.016195\n",
      "621283380  0.016473\n",
      "621283540  0.016724\n",
      "621283338  0.017003\n",
      "621283620  0.017051\n",
      "621283334  0.017099\n",
      "621283415  0.017121\n",
      "621283542  0.017231\n",
      "621283327  0.017436\n",
      "621283342  0.017572\n",
      "621283243  0.018508\n",
      "621283543  0.018692\n",
      "621283241  0.018997\n",
      "621283621  0.019568\n",
      "621283417  0.020272\n",
      "621283541  0.028312\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67162  0.021298\n",
      "68873  0.021358\n",
      "13911  0.021451\n",
      "67213  0.021461\n",
      "67361  0.021918\n",
      "66868  0.022098\n",
      "13929  0.022176\n",
      "67501  0.022184\n",
      "13903  0.022455\n",
      "66857  0.022477\n",
      "67536  0.022687\n",
      "67104  0.022802\n",
      "66855  0.022816\n",
      "67777  0.022882\n",
      "67410  0.022928\n",
      "67156  0.022970\n",
      "13756  0.023038\n",
      "67544  0.023132\n",
      "67280  0.023229\n",
      "13891  0.026382\n",
      ">>> Fastest Solution is rocblas 621283410 0.015473100543022155\n",
      "M N K dtype 3584 168 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 168 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283330  0.029838\n",
      "621283324  0.031256\n",
      "621283332  0.031641\n",
      "621283329  0.032759\n",
      "621283331  0.032874\n",
      "621283325  0.033712\n",
      "621283327  0.034529\n",
      "621283328  0.034669\n",
      "621283366  0.035460\n",
      "621283617  0.035601\n",
      "621283397  0.036262\n",
      "621283388  0.036367\n",
      "621283531  0.037329\n",
      "621283235  0.037726\n",
      "621283395  0.037981\n",
      "621283237  0.038427\n",
      "621283364  0.038427\n",
      "621283391  0.038968\n",
      "621283390  0.039148\n",
      "621283339  0.039289\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.033501\n",
      "13917  0.033626\n",
      "13896  0.033681\n",
      "13852  0.033817\n",
      "13884  0.033987\n",
      "13883  0.034068\n",
      "13779  0.034995\n",
      "13850  0.035436\n",
      "13767  0.035460\n",
      "13863  0.036062\n",
      "13853  0.036121\n",
      "13732  0.036723\n",
      "13873  0.037465\n",
      "13851  0.037526\n",
      "13875  0.037926\n",
      "13845  0.038027\n",
      "67849  0.038187\n",
      "67869  0.038207\n",
      "13849  0.038307\n",
      "67837  0.039515\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.026428\n",
      "621283332  0.027015\n",
      "621283325  0.027248\n",
      "621283327  0.027530\n",
      "621283329  0.027556\n",
      "621283388  0.029858\n",
      "621283324  0.029996\n",
      "621283331  0.032450\n",
      "621283531  0.032657\n",
      "621283330  0.033334\n",
      "621283364  0.034266\n",
      "621283395  0.034354\n",
      "621283339  0.035635\n",
      "621283390  0.036188\n",
      "621283391  0.036954\n",
      "621283366  0.037433\n",
      "621283237  0.039004\n",
      "621283617  0.039431\n",
      "621283235  0.040143\n",
      "621283397  0.040321\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.026436\n",
      "13883  0.026759\n",
      "13917  0.026823\n",
      "13896  0.027019\n",
      "13852  0.027338\n",
      "13850  0.028173\n",
      "13853  0.032045\n",
      "13779  0.033406\n",
      "13897  0.033602\n",
      "13863  0.034286\n",
      "13845  0.035370\n",
      "13732  0.035657\n",
      "67837  0.036084\n",
      "13851  0.036559\n",
      "13875  0.036803\n",
      "67869  0.036940\n",
      "67849  0.037146\n",
      "13873  0.038395\n",
      "13849  0.040163\n",
      "13767  0.042324\n",
      ">>> Fastest Solution is hipblaslt 13884 0.02643600106239319\n",
      "M N K dtype 4096 168 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 168 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283335  0.019724\n",
      "621283395  0.020116\n",
      "621283337  0.020326\n",
      "621283330  0.020472\n",
      "621283532  0.020667\n",
      "621283399  0.020707\n",
      "621283531  0.020927\n",
      "621283339  0.021008\n",
      "621283233  0.021168\n",
      "621283331  0.021183\n",
      "621283533  0.021309\n",
      "621283338  0.021389\n",
      "621283363  0.021428\n",
      "621283321  0.021464\n",
      "621283323  0.021619\n",
      "621283334  0.021805\n",
      "621283319  0.021860\n",
      "621283322  0.021879\n",
      "621283234  0.022290\n",
      "621283398  0.022390\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13739  0.023579\n",
      "13892  0.024245\n",
      "13916  0.024316\n",
      "13881  0.024335\n",
      "13894  0.024350\n",
      "13859  0.024425\n",
      "13898  0.024526\n",
      "13855  0.024826\n",
      "13893  0.024991\n",
      "13844  0.025257\n",
      "67833  0.025342\n",
      "67817  0.025643\n",
      "67505  0.025678\n",
      "13845  0.025713\n",
      "67895  0.025828\n",
      "67903  0.025858\n",
      "67885  0.025879\n",
      "13880  0.025919\n",
      "13886  0.025939\n",
      "13876  0.026019\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283395  0.016353\n",
      "621283330  0.017298\n",
      "621283363  0.017470\n",
      "621283331  0.017819\n",
      "621283531  0.017844\n",
      "621283338  0.018346\n",
      "621283339  0.018398\n",
      "621283323  0.018716\n",
      "621283321  0.018765\n",
      "621283233  0.019101\n",
      "621283322  0.019162\n",
      "621283334  0.019333\n",
      "621283399  0.019741\n",
      "621283398  0.019935\n",
      "621283533  0.020593\n",
      "621283532  0.020645\n",
      "621283319  0.020727\n",
      "621283234  0.020742\n",
      "621283337  0.021324\n",
      "621283335  0.024957\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13894  0.020996\n",
      "13893  0.021239\n",
      "67833  0.021630\n",
      "13844  0.021797\n",
      "13876  0.022142\n",
      "13845  0.022276\n",
      "67903  0.022517\n",
      "13886  0.022529\n",
      "67817  0.022608\n",
      "13880  0.022660\n",
      "13855  0.022708\n",
      "13881  0.022708\n",
      "13892  0.022792\n",
      "67895  0.022907\n",
      "13859  0.022926\n",
      "13898  0.023205\n",
      "67885  0.024323\n",
      "67505  0.024510\n",
      "13916  0.024691\n",
      "13739  0.026951\n",
      ">>> Fastest Solution is rocblas 621283395 0.016353100538253784\n",
      "M N K dtype 768 160 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 160 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283247  0.019319\n",
      "621283530  0.020005\n",
      "621283393  0.020105\n",
      "621283248  0.020487\n",
      "621283320  0.020646\n",
      "621283249  0.021008\n",
      "621283238  0.021228\n",
      "621283334  0.021489\n",
      "621283328  0.021489\n",
      "621283319  0.021549\n",
      "621283322  0.021549\n",
      "621283321  0.021669\n",
      "621283325  0.021689\n",
      "621283388  0.021709\n",
      "621283246  0.021909\n",
      "621283323  0.022010\n",
      "621283244  0.022131\n",
      "621283392  0.022191\n",
      "621283330  0.022331\n",
      "621283329  0.022371\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.023674\n",
      "67837  0.023994\n",
      "13883  0.024275\n",
      "13845  0.024395\n",
      "13884  0.024596\n",
      "13896  0.024636\n",
      "67865  0.024736\n",
      "13850  0.024836\n",
      "67678  0.024876\n",
      "13894  0.024916\n",
      "67777  0.025056\n",
      "67895  0.025077\n",
      "66957  0.025097\n",
      "67808  0.025117\n",
      "13831  0.025338\n",
      "13895  0.025398\n",
      "13857  0.025433\n",
      "13852  0.025458\n",
      "13779  0.025478\n",
      "13790  0.025518\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283322  0.015822\n",
      "621283244  0.015824\n",
      "621283334  0.016108\n",
      "621283321  0.016660\n",
      "621283323  0.016834\n",
      "621283246  0.016908\n",
      "621283319  0.017063\n",
      "621283392  0.017325\n",
      "621283320  0.017434\n",
      "621283249  0.017992\n",
      "621283248  0.018426\n",
      "621283328  0.018801\n",
      "621283325  0.019047\n",
      "621283238  0.019125\n",
      "621283330  0.019222\n",
      "621283388  0.019352\n",
      "621283329  0.019386\n",
      "621283393  0.019763\n",
      "621283530  0.019925\n",
      "621283247  0.020998\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13850  0.020106\n",
      "13896  0.020342\n",
      "13852  0.020352\n",
      "67808  0.020543\n",
      "13845  0.020787\n",
      "13883  0.021054\n",
      "13884  0.021234\n",
      "67865  0.021607\n",
      "67895  0.022030\n",
      "13790  0.022150\n",
      "13831  0.022186\n",
      "67678  0.022258\n",
      "13857  0.022301\n",
      "13779  0.022403\n",
      "13894  0.022677\n",
      "67777  0.022734\n",
      "66957  0.023024\n",
      "13895  0.023082\n",
      "67837  0.024341\n",
      "13897  0.029320\n",
      ">>> Fastest Solution is rocblas 621283322 0.015821899473667144\n",
      "M N K dtype 4096 160 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 160 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283620  0.017480\n",
      "621283413  0.018562\n",
      "621283414  0.018702\n",
      "621283379  0.018723\n",
      "621283410  0.018783\n",
      "621283415  0.019003\n",
      "621283541  0.019023\n",
      "621283564  0.019224\n",
      "621283380  0.019344\n",
      "621283412  0.019524\n",
      "621283418  0.019725\n",
      "621283621  0.019785\n",
      "621283530  0.019986\n",
      "621283411  0.020046\n",
      "621283240  0.020065\n",
      "621283539  0.020146\n",
      "621283469  0.020186\n",
      "621283542  0.020206\n",
      "621283397  0.020266\n",
      "621283540  0.020286\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13903  0.023353\n",
      "67527  0.023834\n",
      "67536  0.024035\n",
      "67162  0.024435\n",
      "13862  0.024456\n",
      "67280  0.024535\n",
      "67361  0.024596\n",
      "68873  0.024716\n",
      "67086  0.024876\n",
      "13756  0.024957\n",
      "13911  0.024997\n",
      "13870  0.025076\n",
      "67104  0.025117\n",
      "68102  0.025257\n",
      "66872  0.025277\n",
      "67244  0.025298\n",
      "13918  0.025338\n",
      "67501  0.025378\n",
      "67877  0.025378\n",
      "67213  0.025417\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283539  0.015459\n",
      "621283418  0.016151\n",
      "621283564  0.016157\n",
      "621283240  0.016241\n",
      "621283469  0.016287\n",
      "621283410  0.016547\n",
      "621283380  0.016598\n",
      "621283412  0.016614\n",
      "621283379  0.016658\n",
      "621283397  0.016850\n",
      "621283415  0.016946\n",
      "621283540  0.016950\n",
      "621283411  0.017057\n",
      "621283542  0.017065\n",
      "621283530  0.017263\n",
      "621283541  0.017474\n",
      "621283414  0.017957\n",
      "621283621  0.018285\n",
      "621283413  0.019059\n",
      "621283620  0.022770\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67213  0.020996\n",
      "13918  0.021577\n",
      "67244  0.021837\n",
      "67501  0.021936\n",
      "13862  0.022234\n",
      "67280  0.022447\n",
      "67527  0.022565\n",
      "13911  0.022625\n",
      "67086  0.022625\n",
      "13870  0.022653\n",
      "67536  0.022732\n",
      "13756  0.022858\n",
      "66872  0.022904\n",
      "67877  0.023012\n",
      "68102  0.023066\n",
      "68873  0.023155\n",
      "67361  0.023385\n",
      "67104  0.023489\n",
      "67162  0.023656\n",
      "13903  0.025315\n",
      ">>> Fastest Solution is rocblas 621283539 0.015459099411964416\n",
      "M N K dtype 3584 160 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 160 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283327  0.026701\n",
      "621283325  0.029682\n",
      "621283334  0.029943\n",
      "621283333  0.031546\n",
      "621283328  0.033340\n",
      "621283388  0.033395\n",
      "621283330  0.033411\n",
      "621283233  0.033456\n",
      "621283319  0.033531\n",
      "621283329  0.033611\n",
      "621283332  0.034052\n",
      "621283324  0.034413\n",
      "621283364  0.034619\n",
      "621283234  0.035781\n",
      "621283394  0.037134\n",
      "621283320  0.037199\n",
      "621283395  0.037666\n",
      "621283531  0.037926\n",
      "621283331  0.038733\n",
      "621283391  0.038828\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13853  0.030349\n",
      "13916  0.031502\n",
      "13892  0.032985\n",
      "13863  0.033075\n",
      "13896  0.033556\n",
      "13883  0.033831\n",
      "13884  0.034017\n",
      "13852  0.034052\n",
      "67817  0.035681\n",
      "13897  0.035817\n",
      "13917  0.036794\n",
      "13779  0.036934\n",
      "13732  0.037205\n",
      "67833  0.037285\n",
      "13850  0.037460\n",
      "13845  0.037600\n",
      "67849  0.037685\n",
      "13746  0.037901\n",
      "13881  0.037967\n",
      "13743  0.038011\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283334  0.025361\n",
      "621283332  0.026085\n",
      "621283328  0.026107\n",
      "621283330  0.026695\n",
      "621283324  0.026737\n",
      "621283388  0.026947\n",
      "621283325  0.027795\n",
      "621283329  0.027841\n",
      "621283395  0.028959\n",
      "621283531  0.029256\n",
      "621283320  0.030479\n",
      "621283333  0.030545\n",
      "621283331  0.031397\n",
      "621283394  0.033510\n",
      "621283319  0.033553\n",
      "621283327  0.034107\n",
      "621283364  0.036260\n",
      "621283234  0.037285\n",
      "621283391  0.039307\n",
      "621283233  0.039457\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.026090\n",
      "13897  0.026464\n",
      "13852  0.026497\n",
      "13916  0.026761\n",
      "13892  0.026798\n",
      "13896  0.026816\n",
      "13883  0.027764\n",
      "13917  0.029472\n",
      "13845  0.032887\n",
      "13779  0.033092\n",
      "13881  0.033285\n",
      "13850  0.035283\n",
      "67833  0.035462\n",
      "13863  0.035487\n",
      "13743  0.035586\n",
      "13732  0.036188\n",
      "67849  0.036549\n",
      "13746  0.036985\n",
      "67817  0.038327\n",
      "13853  0.040638\n",
      ">>> Fastest Solution is rocblas 621283334 0.025361499190330504\n",
      "M N K dtype 4096 160 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 160 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283340  0.019343\n",
      "621283531  0.019379\n",
      "621283363  0.020065\n",
      "621283395  0.020221\n",
      "621283331  0.020527\n",
      "621283532  0.020747\n",
      "621283321  0.020777\n",
      "621283394  0.020807\n",
      "621283338  0.021128\n",
      "621283365  0.021188\n",
      "621283323  0.021188\n",
      "621283336  0.021328\n",
      "621283319  0.021364\n",
      "621283339  0.021529\n",
      "621283334  0.021845\n",
      "621283399  0.021850\n",
      "621283322  0.021965\n",
      "621283233  0.022050\n",
      "621283320  0.022296\n",
      "621283617  0.022371\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13898  0.024050\n",
      "13894  0.024410\n",
      "67825  0.024616\n",
      "13914  0.024791\n",
      "13880  0.024967\n",
      "67856  0.024976\n",
      "13864  0.025016\n",
      "13893  0.025197\n",
      "67885  0.025202\n",
      "13855  0.025357\n",
      "13886  0.025378\n",
      "13881  0.025483\n",
      "67766  0.025498\n",
      "13743  0.025618\n",
      "13892  0.025628\n",
      "67755  0.025764\n",
      "13860  0.025779\n",
      "13856  0.025798\n",
      "67817  0.025858\n",
      "13739  0.026024\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283531  0.016253\n",
      "621283331  0.017091\n",
      "621283338  0.017377\n",
      "621283395  0.017642\n",
      "621283394  0.017976\n",
      "621283339  0.018384\n",
      "621283365  0.018799\n",
      "621283321  0.018871\n",
      "621283323  0.018923\n",
      "621283363  0.019057\n",
      "621283320  0.019122\n",
      "621283334  0.019156\n",
      "621283322  0.019237\n",
      "621283233  0.019428\n",
      "621283399  0.019675\n",
      "621283319  0.019921\n",
      "621283336  0.020170\n",
      "621283532  0.020545\n",
      "621283617  0.020679\n",
      "621283340  0.025400\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13892  0.020640\n",
      "13893  0.020916\n",
      "67817  0.021072\n",
      "13894  0.021298\n",
      "13881  0.021592\n",
      "67885  0.021759\n",
      "13860  0.021791\n",
      "67766  0.021838\n",
      "67755  0.021883\n",
      "13739  0.021904\n",
      "13864  0.022030\n",
      "13856  0.022297\n",
      "67856  0.022409\n",
      "13886  0.022506\n",
      "13914  0.022730\n",
      "13855  0.022794\n",
      "13880  0.023015\n",
      "67825  0.023816\n",
      "13743  0.024613\n",
      "13898  0.027466\n",
      ">>> Fastest Solution is rocblas 621283531 0.016252849996089936\n",
      "M N K dtype 768 152 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 152 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283248  0.018221\n",
      "621283244  0.019886\n",
      "621283530  0.020025\n",
      "621283319  0.020586\n",
      "621283249  0.020913\n",
      "621283247  0.020913\n",
      "621283245  0.020927\n",
      "621283321  0.021168\n",
      "621283334  0.021269\n",
      "621283320  0.021349\n",
      "621283393  0.021349\n",
      "621283238  0.021589\n",
      "621283388  0.021669\n",
      "621283325  0.021689\n",
      "621283392  0.021750\n",
      "621283322  0.021809\n",
      "621283246  0.022010\n",
      "621283257  0.022210\n",
      "621283330  0.022231\n",
      "621283329  0.022231\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13883  0.024054\n",
      "13917  0.024115\n",
      "67837  0.024115\n",
      "13850  0.024154\n",
      "13852  0.024275\n",
      "13884  0.024276\n",
      "13897  0.024315\n",
      "67757  0.024516\n",
      "67746  0.024876\n",
      "67808  0.025097\n",
      "66957  0.025157\n",
      "13857  0.025297\n",
      "67895  0.025338\n",
      "13728  0.025357\n",
      "67661  0.025678\n",
      "66941  0.025679\n",
      "67865  0.025779\n",
      "67631  0.025939\n",
      "13896  0.025979\n",
      "66947  0.026120\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.015487\n",
      "621283322  0.016096\n",
      "621283246  0.016473\n",
      "621283334  0.016568\n",
      "621283321  0.016718\n",
      "621283319  0.017217\n",
      "621283393  0.017293\n",
      "621283392  0.017349\n",
      "621283247  0.017489\n",
      "621283249  0.017496\n",
      "621283530  0.018392\n",
      "621283320  0.018486\n",
      "621283388  0.018981\n",
      "621283325  0.019167\n",
      "621283329  0.019472\n",
      "621283330  0.019721\n",
      "621283238  0.020096\n",
      "621283257  0.020218\n",
      "621283248  0.021360\n",
      "621283244  0.022018\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13897  0.020390\n",
      "67757  0.020428\n",
      "13896  0.020519\n",
      "13884  0.020559\n",
      "13852  0.020821\n",
      "67808  0.021715\n",
      "67865  0.021723\n",
      "13850  0.021765\n",
      "13917  0.021936\n",
      "67746  0.021992\n",
      "67895  0.022038\n",
      "13857  0.022258\n",
      "67837  0.022279\n",
      "13728  0.022908\n",
      "66957  0.023054\n",
      "67631  0.023197\n",
      "66941  0.023359\n",
      "66947  0.023888\n",
      "67661  0.024004\n",
      "13883  0.028593\n",
      ">>> Fastest Solution is rocblas 621283245 0.015487100183963775\n",
      "M N K dtype 4096 152 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 152 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283410  0.018642\n",
      "621283418  0.018903\n",
      "621283240  0.019103\n",
      "621283379  0.019244\n",
      "621283388  0.019444\n",
      "621283416  0.019484\n",
      "621283620  0.019505\n",
      "621283542  0.019565\n",
      "621283327  0.019605\n",
      "621283656  0.019624\n",
      "621283540  0.019745\n",
      "621283335  0.019885\n",
      "621283411  0.019905\n",
      "621283415  0.019925\n",
      "621283564  0.020005\n",
      "621283539  0.020126\n",
      "621283366  0.020166\n",
      "621283400  0.020387\n",
      "621283336  0.020406\n",
      "621283331  0.020446\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.022591\n",
      "13896  0.022932\n",
      "67544  0.023573\n",
      "67536  0.024275\n",
      "67410  0.024295\n",
      "67280  0.024395\n",
      "13878  0.024636\n",
      "66932  0.024936\n",
      "68873  0.024976\n",
      "13918  0.025137\n",
      "67953  0.025237\n",
      "67213  0.025417\n",
      "13867  0.025417\n",
      "13756  0.025457\n",
      "13919  0.025478\n",
      "67594  0.025518\n",
      "66923  0.025578\n",
      "13866  0.025578\n",
      "13772  0.025598\n",
      "13870  0.025618\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283539  0.015559\n",
      "621283564  0.016050\n",
      "621283240  0.016371\n",
      "621283327  0.016447\n",
      "621283416  0.016654\n",
      "621283388  0.016664\n",
      "621283620  0.016684\n",
      "621283379  0.016956\n",
      "621283415  0.016959\n",
      "621283542  0.016964\n",
      "621283540  0.016966\n",
      "621283366  0.016997\n",
      "621283331  0.017017\n",
      "621283335  0.017173\n",
      "621283411  0.017255\n",
      "621283656  0.017410\n",
      "621283400  0.017570\n",
      "621283336  0.017678\n",
      "621283418  0.021517\n",
      "621283410  0.022780\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "66923  0.021276\n",
      "13918  0.021515\n",
      "13919  0.021557\n",
      "68873  0.021930\n",
      "67280  0.021934\n",
      "67213  0.021958\n",
      "13866  0.022046\n",
      "13867  0.022126\n",
      "67953  0.022134\n",
      "13878  0.022397\n",
      "13870  0.022479\n",
      "67594  0.022722\n",
      "66932  0.022840\n",
      "13756  0.022880\n",
      "13772  0.023054\n",
      "67410  0.023102\n",
      "67536  0.023120\n",
      "67544  0.023148\n",
      "13896  0.023443\n",
      "13911  0.030710\n",
      ">>> Fastest Solution is rocblas 621283539 0.015559299290180207\n",
      "M N K dtype 3584 152 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 152 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283332  0.030901\n",
      "621283325  0.031095\n",
      "621283327  0.031301\n",
      "621283334  0.031546\n",
      "621283363  0.032313\n",
      "621283329  0.032664\n",
      "621283388  0.032699\n",
      "621283330  0.033065\n",
      "621283328  0.033787\n",
      "621283324  0.033792\n",
      "621283319  0.033847\n",
      "621283326  0.034518\n",
      "621283331  0.035004\n",
      "621283531  0.035491\n",
      "621283320  0.035641\n",
      "621283235  0.037104\n",
      "621283394  0.037205\n",
      "621283395  0.037641\n",
      "621283617  0.038167\n",
      "621283396  0.038327\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13883  0.026179\n",
      "13884  0.027357\n",
      "13897  0.030956\n",
      "13863  0.033356\n",
      "13852  0.033450\n",
      "13892  0.033721\n",
      "13896  0.034037\n",
      "13850  0.035039\n",
      "13853  0.036002\n",
      "13917  0.036277\n",
      "13732  0.036302\n",
      "13916  0.036408\n",
      "13873  0.036704\n",
      "13851  0.036723\n",
      "67849  0.037225\n",
      "13746  0.037264\n",
      "13844  0.037325\n",
      "13845  0.037540\n",
      "67833  0.037846\n",
      "67869  0.037926\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283334  0.025827\n",
      "621283328  0.026570\n",
      "621283330  0.026718\n",
      "621283327  0.026735\n",
      "621283329  0.027238\n",
      "621283388  0.027532\n",
      "621283325  0.027689\n",
      "621283324  0.027870\n",
      "621283320  0.031774\n",
      "621283332  0.031928\n",
      "621283395  0.032367\n",
      "621283531  0.033388\n",
      "621283319  0.033585\n",
      "621283331  0.033722\n",
      "621283394  0.034956\n",
      "621283363  0.036884\n",
      "621283326  0.037359\n",
      "621283235  0.038698\n",
      "621283396  0.038850\n",
      "621283617  0.040448\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13897  0.026185\n",
      "13896  0.026221\n",
      "13884  0.028170\n",
      "13892  0.028592\n",
      "13852  0.029728\n",
      "13850  0.030521\n",
      "13916  0.031120\n",
      "13917  0.031240\n",
      "13883  0.031666\n",
      "13845  0.033254\n",
      "13863  0.033729\n",
      "13746  0.033858\n",
      "13853  0.034570\n",
      "67849  0.035064\n",
      "13732  0.035551\n",
      "67869  0.035585\n",
      "13851  0.036146\n",
      "13873  0.036645\n",
      "67833  0.037341\n",
      "13844  0.040005\n",
      ">>> Fastest Solution is rocblas 621283334 0.025826549530029295\n",
      "M N K dtype 4096 152 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 152 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283394  0.019690\n",
      "621283234  0.019965\n",
      "621283531  0.020136\n",
      "621283333  0.020266\n",
      "621283328  0.020607\n",
      "621283320  0.020777\n",
      "621283398  0.020847\n",
      "621283319  0.021133\n",
      "621283331  0.021188\n",
      "621283323  0.021353\n",
      "621283321  0.021458\n",
      "621283397  0.021750\n",
      "621283341  0.021769\n",
      "621283363  0.021769\n",
      "621283339  0.021850\n",
      "621283334  0.021854\n",
      "621283395  0.021855\n",
      "621283338  0.021870\n",
      "621283322  0.021930\n",
      "621283340  0.022010\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13880  0.023684\n",
      "13886  0.024416\n",
      "13894  0.024420\n",
      "13892  0.024441\n",
      "13893  0.024671\n",
      "13916  0.024846\n",
      "13915  0.024851\n",
      "13859  0.024851\n",
      "13898  0.024957\n",
      "13917  0.024982\n",
      "13855  0.025142\n",
      "13864  0.025217\n",
      "67885  0.025468\n",
      "67972  0.025497\n",
      "67825  0.025598\n",
      "67951  0.025598\n",
      "67746  0.025678\n",
      "13850  0.025728\n",
      "13791  0.025899\n",
      "67903  0.025919\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283333  0.016740\n",
      "621283331  0.016797\n",
      "621283531  0.016898\n",
      "621283395  0.017333\n",
      "621283328  0.017860\n",
      "621283363  0.017921\n",
      "621283339  0.017987\n",
      "621283338  0.018111\n",
      "621283340  0.018285\n",
      "621283323  0.018652\n",
      "621283394  0.018718\n",
      "621283334  0.019074\n",
      "621283321  0.019172\n",
      "621283322  0.019431\n",
      "621283319  0.019582\n",
      "621283398  0.019626\n",
      "621283341  0.019991\n",
      "621283320  0.020161\n",
      "621283397  0.020579\n",
      "621283234  0.022393\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13916  0.021117\n",
      "13915  0.021244\n",
      "13893  0.021282\n",
      "13892  0.021411\n",
      "67885  0.021899\n",
      "13791  0.022018\n",
      "67903  0.022062\n",
      "13917  0.022127\n",
      "13859  0.022257\n",
      "13894  0.022365\n",
      "13864  0.022533\n",
      "67746  0.022598\n",
      "67825  0.022631\n",
      "13898  0.022727\n",
      "13850  0.022781\n",
      "13855  0.022782\n",
      "67951  0.022986\n",
      "13886  0.024223\n",
      "67972  0.025213\n",
      "13880  0.026632\n",
      ">>> Fastest Solution is rocblas 621283333 0.016740499436855315\n",
      "M N K dtype 768 144 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 144 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283248  0.018261\n",
      "621283249  0.019624\n",
      "621283530  0.019885\n",
      "621283244  0.020046\n",
      "621283245  0.020266\n",
      "621283392  0.021088\n",
      "621283323  0.021228\n",
      "621283334  0.021449\n",
      "621283238  0.021449\n",
      "621283322  0.021468\n",
      "621283393  0.021549\n",
      "621283325  0.021750\n",
      "621283321  0.021790\n",
      "621283247  0.022250\n",
      "621283257  0.022250\n",
      "621283531  0.022331\n",
      "621283330  0.022411\n",
      "621283319  0.022431\n",
      "621283329  0.022511\n",
      "621283328  0.023132\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13728  0.023954\n",
      "13897  0.024175\n",
      "13852  0.024316\n",
      "13850  0.024496\n",
      "13896  0.024916\n",
      "67837  0.024916\n",
      "13883  0.024976\n",
      "13839  0.024997\n",
      "13917  0.025177\n",
      "13857  0.025198\n",
      "67678  0.025378\n",
      "13884  0.025498\n",
      "13831  0.025518\n",
      "67757  0.025538\n",
      "13846  0.025578\n",
      "13859  0.025598\n",
      "67631  0.025638\n",
      "13894  0.025739\n",
      "67808  0.025739\n",
      "13895  0.025779\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.015601\n",
      "621283322  0.015840\n",
      "621283244  0.016032\n",
      "621283247  0.016309\n",
      "621283323  0.016898\n",
      "621283334  0.016964\n",
      "621283393  0.016983\n",
      "621283392  0.016993\n",
      "621283321  0.017518\n",
      "621283319  0.017724\n",
      "621283530  0.018259\n",
      "621283328  0.018787\n",
      "621283325  0.018973\n",
      "621283330  0.019232\n",
      "621283329  0.019426\n",
      "621283238  0.019759\n",
      "621283531  0.019781\n",
      "621283257  0.019977\n",
      "621283249  0.020248\n",
      "621283248  0.021791\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67808  0.020306\n",
      "13896  0.020340\n",
      "13850  0.020617\n",
      "13883  0.020725\n",
      "67757  0.020831\n",
      "13884  0.020855\n",
      "13917  0.020950\n",
      "13852  0.021431\n",
      "67678  0.021439\n",
      "13857  0.021643\n",
      "67837  0.021739\n",
      "13859  0.021779\n",
      "13846  0.021864\n",
      "13839  0.022128\n",
      "67631  0.022467\n",
      "13895  0.022605\n",
      "13894  0.022758\n",
      "13831  0.022788\n",
      "13897  0.024195\n",
      "13728  0.028769\n",
      ">>> Fastest Solution is rocblas 621283245 0.015601350367069245\n",
      "M N K dtype 4096 144 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 144 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283410  0.018662\n",
      "621283540  0.018923\n",
      "621283379  0.019023\n",
      "621283542  0.019083\n",
      "621283564  0.019204\n",
      "621283417  0.019204\n",
      "621283380  0.019243\n",
      "621283469  0.019324\n",
      "621283620  0.019424\n",
      "621283391  0.019764\n",
      "621283412  0.019925\n",
      "621283415  0.020005\n",
      "621283414  0.020186\n",
      "621283413  0.020386\n",
      "621283364  0.020727\n",
      "621283365  0.020767\n",
      "621283236  0.020787\n",
      "621283374  0.020807\n",
      "621283238  0.020827\n",
      "621283260  0.020827\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.022812\n",
      "67361  0.023935\n",
      "67527  0.024054\n",
      "67544  0.024194\n",
      "67536  0.024195\n",
      "13790  0.024275\n",
      "13756  0.024355\n",
      "66932  0.024395\n",
      "67162  0.024575\n",
      "67566  0.024656\n",
      "66864  0.024696\n",
      "66906  0.024736\n",
      "67086  0.024756\n",
      "68893  0.024897\n",
      "67104  0.024936\n",
      "13918  0.024976\n",
      "67037  0.025057\n",
      "67180  0.025137\n",
      "67352  0.025217\n",
      "66923  0.025257\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283417  0.016108\n",
      "621283469  0.016548\n",
      "621283380  0.016694\n",
      "621283620  0.016760\n",
      "621283415  0.016898\n",
      "621283365  0.017003\n",
      "621283391  0.017021\n",
      "621283412  0.017029\n",
      "621283414  0.017031\n",
      "621283413  0.017089\n",
      "621283564  0.017135\n",
      "621283238  0.017175\n",
      "621283374  0.017251\n",
      "621283236  0.017500\n",
      "621283542  0.017626\n",
      "621283260  0.018017\n",
      "621283364  0.018159\n",
      "621283540  0.018524\n",
      "621283379  0.018632\n",
      "621283410  0.024010\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67162  0.021332\n",
      "67566  0.021681\n",
      "13918  0.021687\n",
      "67180  0.021824\n",
      "66864  0.022118\n",
      "66923  0.022210\n",
      "66906  0.022379\n",
      "67536  0.022541\n",
      "68893  0.022571\n",
      "67527  0.022619\n",
      "13790  0.022822\n",
      "13756  0.022934\n",
      "67104  0.023056\n",
      "67086  0.023118\n",
      "67352  0.023341\n",
      "66932  0.023389\n",
      "67544  0.023850\n",
      "67037  0.024071\n",
      "67361  0.024381\n",
      "13911  0.029178\n",
      ">>> Fastest Solution is rocblas 621283417 0.016108499467372896\n",
      "M N K dtype 3584 144 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 144 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283328  0.026335\n",
      "621283388  0.029397\n",
      "621283334  0.031041\n",
      "621283332  0.032178\n",
      "621283324  0.032559\n",
      "621283325  0.032745\n",
      "621283319  0.032880\n",
      "621283233  0.032955\n",
      "621283329  0.033671\n",
      "621283327  0.033977\n",
      "621283333  0.035035\n",
      "621283330  0.035581\n",
      "621283363  0.036202\n",
      "621283339  0.036563\n",
      "621283234  0.036764\n",
      "621283320  0.036814\n",
      "621283395  0.036973\n",
      "621283531  0.037119\n",
      "621283364  0.037265\n",
      "621283331  0.037746\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13852  0.028168\n",
      "13896  0.030750\n",
      "13732  0.031852\n",
      "13883  0.032213\n",
      "13892  0.032218\n",
      "13917  0.032809\n",
      "13884  0.033095\n",
      "13916  0.033707\n",
      "13897  0.033831\n",
      "67869  0.035000\n",
      "13863  0.035540\n",
      "67817  0.035721\n",
      "13860  0.036001\n",
      "13881  0.036102\n",
      "13779  0.036292\n",
      "13853  0.036523\n",
      "13921  0.036718\n",
      "67757  0.036858\n",
      "13850  0.036929\n",
      "67849  0.036984\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283334  0.025197\n",
      "621283325  0.026015\n",
      "621283332  0.026288\n",
      "621283330  0.026388\n",
      "621283329  0.026763\n",
      "621283324  0.026837\n",
      "621283327  0.026875\n",
      "621283388  0.028691\n",
      "621283333  0.030993\n",
      "621283319  0.031034\n",
      "621283531  0.031224\n",
      "621283331  0.031554\n",
      "621283395  0.031590\n",
      "621283328  0.031724\n",
      "621283320  0.033624\n",
      "621283363  0.034867\n",
      "621283364  0.035966\n",
      "621283234  0.036537\n",
      "621283339  0.036649\n",
      "621283233  0.038922\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13917  0.024701\n",
      "13884  0.025686\n",
      "13897  0.026169\n",
      "13896  0.026967\n",
      "13916  0.027614\n",
      "13883  0.028389\n",
      "13892  0.029085\n",
      "13850  0.030561\n",
      "13852  0.031846\n",
      "67757  0.032312\n",
      "13779  0.033396\n",
      "13921  0.033892\n",
      "13853  0.034149\n",
      "13863  0.034428\n",
      "67817  0.035553\n",
      "13881  0.035606\n",
      "13732  0.035623\n",
      "67869  0.036739\n",
      "67849  0.037609\n",
      "13860  0.038353\n",
      ">>> Fastest Solution is hipblaslt 13917 0.024700550734996794\n",
      "M N K dtype 4096 144 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 144 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283395  0.018908\n",
      "621283328  0.019168\n",
      "621283340  0.019824\n",
      "621283394  0.019980\n",
      "621283338  0.020086\n",
      "621283339  0.020105\n",
      "621283396  0.020226\n",
      "621283391  0.020266\n",
      "621283619  0.020426\n",
      "621283333  0.020506\n",
      "621283531  0.020677\n",
      "621283321  0.020802\n",
      "621283324  0.021078\n",
      "621283335  0.021148\n",
      "621283332  0.021319\n",
      "621283334  0.021339\n",
      "621283366  0.021368\n",
      "621283323  0.021389\n",
      "621283319  0.021429\n",
      "621283365  0.021449\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13915  0.024290\n",
      "13845  0.024325\n",
      "13859  0.024416\n",
      "13842  0.024435\n",
      "13893  0.024495\n",
      "13881  0.024505\n",
      "13898  0.024510\n",
      "13855  0.024565\n",
      "13892  0.024566\n",
      "13886  0.024651\n",
      "13851  0.024796\n",
      "67863  0.024817\n",
      "13749  0.024927\n",
      "13894  0.024982\n",
      "13880  0.025047\n",
      "13856  0.025197\n",
      "13899  0.025253\n",
      "66966  0.025287\n",
      "13849  0.025317\n",
      "13860  0.025398\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283328  0.016800\n",
      "621283395  0.016978\n",
      "621283531  0.017009\n",
      "621283333  0.017192\n",
      "621283324  0.017568\n",
      "621283394  0.017797\n",
      "621283335  0.017844\n",
      "621283332  0.017885\n",
      "621283338  0.018077\n",
      "621283366  0.018097\n",
      "621283365  0.018119\n",
      "621283339  0.018167\n",
      "621283391  0.018255\n",
      "621283321  0.018532\n",
      "621283323  0.018659\n",
      "621283334  0.019092\n",
      "621283396  0.019394\n",
      "621283319  0.019470\n",
      "621283619  0.020067\n",
      "621283340  0.020573\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13892  0.020986\n",
      "13894  0.021059\n",
      "13881  0.021572\n",
      "13849  0.021878\n",
      "13898  0.021911\n",
      "13880  0.021958\n",
      "13842  0.022066\n",
      "13886  0.022079\n",
      "13893  0.022168\n",
      "13855  0.022222\n",
      "13851  0.022597\n",
      "13856  0.022758\n",
      "13860  0.022796\n",
      "13899  0.022874\n",
      "13749  0.022880\n",
      "13859  0.023103\n",
      "13915  0.023104\n",
      "66966  0.023121\n",
      "13845  0.023157\n",
      "67863  0.023459\n",
      ">>> Fastest Solution is rocblas 621283328 0.01680009961128235\n",
      "M N K dtype 768 136 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 768 136 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283248  0.019484\n",
      "621283530  0.019545\n",
      "621283247  0.019925\n",
      "621283244  0.020105\n",
      "621283245  0.020767\n",
      "621283323  0.021068\n",
      "621283320  0.021208\n",
      "621283393  0.021309\n",
      "621283322  0.021328\n",
      "621283319  0.021389\n",
      "621283334  0.021428\n",
      "621283321  0.021469\n",
      "621283238  0.021489\n",
      "621283392  0.021508\n",
      "621283325  0.021529\n",
      "621283328  0.021649\n",
      "621283257  0.022271\n",
      "621283531  0.022350\n",
      "621283329  0.022451\n",
      "621283246  0.022532\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.024115\n",
      "13896  0.024295\n",
      "67865  0.024416\n",
      "67808  0.024696\n",
      "13884  0.024957\n",
      "66957  0.025017\n",
      "13852  0.025056\n",
      "13790  0.025177\n",
      "13746  0.025197\n",
      "13779  0.025277\n",
      "13912  0.025338\n",
      "13842  0.025378\n",
      "66998  0.025378\n",
      "13916  0.025498\n",
      "13848  0.025518\n",
      "13743  0.025558\n",
      "13917  0.025598\n",
      "67678  0.025638\n",
      "13894  0.025758\n",
      "67631  0.025819\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283322  0.016133\n",
      "621283244  0.016169\n",
      "621283245  0.016772\n",
      "621283334  0.016784\n",
      "621283323  0.016792\n",
      "621283319  0.016842\n",
      "621283393  0.016868\n",
      "621283246  0.016995\n",
      "621283320  0.017015\n",
      "621283321  0.017311\n",
      "621283247  0.017317\n",
      "621283392  0.017518\n",
      "621283328  0.018771\n",
      "621283325  0.018955\n",
      "621283329  0.019338\n",
      "621283531  0.019470\n",
      "621283238  0.019645\n",
      "621283257  0.020176\n",
      "621283530  0.021134\n",
      "621283248  0.022655\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.020284\n",
      "13852  0.020516\n",
      "13917  0.020903\n",
      "67808  0.021114\n",
      "66998  0.021443\n",
      "13894  0.021681\n",
      "13842  0.021709\n",
      "13779  0.021862\n",
      "13790  0.021922\n",
      "13916  0.021932\n",
      "67678  0.021992\n",
      "13746  0.022058\n",
      "13848  0.022180\n",
      "13912  0.022381\n",
      "67865  0.022387\n",
      "66957  0.022707\n",
      "13743  0.022980\n",
      "67631  0.023068\n",
      "13896  0.023489\n",
      "13897  0.029044\n",
      ">>> Fastest Solution is rocblas 621283322 0.016132600605487823\n",
      "M N K dtype 4096 136 512 torch.bfloat16 >>> Total rocb solutions 429\n",
      "M N K bias dtype 4096 136 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283414  0.018401\n",
      "621283379  0.019143\n",
      "621283411  0.019243\n",
      "621283380  0.019244\n",
      "621283413  0.019444\n",
      "621283417  0.019444\n",
      "621283416  0.019624\n",
      "621283620  0.019745\n",
      "621283410  0.019785\n",
      "621283564  0.019825\n",
      "621283337  0.019965\n",
      "621283333  0.019985\n",
      "621283412  0.020065\n",
      "621283543  0.020086\n",
      "621283539  0.020266\n",
      "621283238  0.020366\n",
      "621283235  0.020406\n",
      "621283415  0.020467\n",
      "621283327  0.020527\n",
      "621283332  0.020627\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67544  0.023794\n",
      "67877  0.023954\n",
      "67280  0.023994\n",
      "67536  0.024075\n",
      "68726  0.024175\n",
      "68873  0.024435\n",
      "67501  0.024475\n",
      "67566  0.024736\n",
      "67410  0.024757\n",
      "67104  0.024776\n",
      "13878  0.024976\n",
      "66899  0.025037\n",
      "68299  0.025177\n",
      "67602  0.025257\n",
      "66868  0.025257\n",
      "67244  0.025277\n",
      "67213  0.025298\n",
      "13772  0.025357\n",
      "13870  0.025458\n",
      "13819  0.025478\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283411  0.015575\n",
      "621283564  0.016191\n",
      "621283410  0.016205\n",
      "621283539  0.016443\n",
      "621283417  0.016525\n",
      "621283416  0.016568\n",
      "621283337  0.016676\n",
      "621283620  0.016728\n",
      "621283415  0.016890\n",
      "621283332  0.017069\n",
      "621283412  0.017181\n",
      "621283413  0.017273\n",
      "621283333  0.017474\n",
      "621283235  0.017508\n",
      "621283380  0.017552\n",
      "621283327  0.017628\n",
      "621283238  0.017676\n",
      "621283543  0.018616\n",
      "621283379  0.019733\n",
      "621283414  0.027230\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67602  0.021348\n",
      "67213  0.021888\n",
      "67501  0.021992\n",
      "13870  0.022012\n",
      "67566  0.022138\n",
      "67280  0.022283\n",
      "67536  0.022299\n",
      "13878  0.022391\n",
      "68873  0.022776\n",
      "66899  0.022792\n",
      "13819  0.022812\n",
      "13772  0.022890\n",
      "67410  0.022992\n",
      "67244  0.023032\n",
      "68299  0.023072\n",
      "66868  0.023329\n",
      "67104  0.023724\n",
      "67877  0.023926\n",
      "68726  0.024953\n",
      "67544  0.028531\n",
      ">>> Fastest Solution is rocblas 621283411 0.0155753493309021\n",
      "M N K dtype 3584 136 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 3584 136 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283324  0.028183\n",
      "621283332  0.028886\n",
      "621283334  0.029186\n",
      "621283327  0.030289\n",
      "621283388  0.032188\n",
      "621283325  0.032428\n",
      "621283330  0.032488\n",
      "621283319  0.032685\n",
      "621283320  0.033200\n",
      "621283329  0.033266\n",
      "621283328  0.033275\n",
      "621283395  0.033847\n",
      "621283364  0.034298\n",
      "621283326  0.034619\n",
      "621283333  0.034949\n",
      "621283363  0.035099\n",
      "621283531  0.035150\n",
      "621283323  0.036277\n",
      "621283237  0.036282\n",
      "621283394  0.036367\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.031016\n",
      "13917  0.031817\n",
      "13884  0.032860\n",
      "13852  0.032885\n",
      "13892  0.033095\n",
      "13896  0.033115\n",
      "13883  0.033301\n",
      "13916  0.034123\n",
      "13850  0.034679\n",
      "13863  0.035381\n",
      "67757  0.035535\n",
      "67817  0.035601\n",
      "13853  0.035802\n",
      "13873  0.036162\n",
      "13732  0.036162\n",
      "13876  0.036182\n",
      "13845  0.036823\n",
      "67849  0.036904\n",
      "13856  0.036964\n",
      "67837  0.037440\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283325  0.025061\n",
      "621283334  0.025468\n",
      "621283388  0.025598\n",
      "621283328  0.025639\n",
      "621283329  0.026073\n",
      "621283330  0.026090\n",
      "621283327  0.026819\n",
      "621283332  0.027108\n",
      "621283320  0.030187\n",
      "621283395  0.030513\n",
      "621283319  0.030933\n",
      "621283531  0.031565\n",
      "621283394  0.032486\n",
      "621283333  0.033013\n",
      "621283363  0.033923\n",
      "621283324  0.034470\n",
      "621283364  0.035358\n",
      "621283237  0.036834\n",
      "621283326  0.036986\n",
      "621283323  0.038131\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13852  0.025251\n",
      "13884  0.025724\n",
      "13883  0.026007\n",
      "13892  0.026123\n",
      "13896  0.026195\n",
      "13917  0.027506\n",
      "13916  0.027537\n",
      "13850  0.030276\n",
      "67757  0.031114\n",
      "67849  0.033398\n",
      "13845  0.033487\n",
      "13853  0.033608\n",
      "13732  0.033857\n",
      "13897  0.034284\n",
      "13863  0.034408\n",
      "13876  0.034546\n",
      "67837  0.034915\n",
      "13873  0.036206\n",
      "67817  0.037531\n",
      "13856  0.038068\n",
      ">>> Fastest Solution is rocblas 621283325 0.025060850381851196\n",
      "M N K dtype 4096 136 1792 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 4096 136 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283324  0.018406\n",
      "621283335  0.019143\n",
      "621283365  0.019324\n",
      "621283233  0.019584\n",
      "621283333  0.019991\n",
      "621283394  0.020160\n",
      "621283399  0.020206\n",
      "621283531  0.020221\n",
      "621283331  0.020226\n",
      "621283340  0.020567\n",
      "621283319  0.020582\n",
      "621283337  0.020646\n",
      "621283321  0.020712\n",
      "621283395  0.020742\n",
      "621283341  0.021068\n",
      "621283363  0.021088\n",
      "621283390  0.021088\n",
      "621283323  0.021118\n",
      "621283532  0.021148\n",
      "621283336  0.021508\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13859  0.024160\n",
      "13886  0.024285\n",
      "13893  0.024425\n",
      "13892  0.024721\n",
      "67825  0.024736\n",
      "13855  0.024791\n",
      "13793  0.024796\n",
      "13898  0.024851\n",
      "13907  0.024957\n",
      "13860  0.024976\n",
      "13880  0.024991\n",
      "13913  0.025037\n",
      "13920  0.025202\n",
      "67118  0.025237\n",
      "13845  0.025372\n",
      "66966  0.025412\n",
      "13743  0.025443\n",
      "13873  0.025478\n",
      "13921  0.025583\n",
      "13850  0.025648\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283331  0.016652\n",
      "621283531  0.016725\n",
      "621283363  0.016740\n",
      "621283333  0.016841\n",
      "621283395  0.017144\n",
      "621283324  0.017375\n",
      "621283390  0.017742\n",
      "621283340  0.017764\n",
      "621283394  0.017881\n",
      "621283337  0.018346\n",
      "621283233  0.018386\n",
      "621283321  0.018588\n",
      "621283323  0.018713\n",
      "621283341  0.019005\n",
      "621283399  0.019099\n",
      "621283532  0.019470\n",
      "621283319  0.019782\n",
      "621283365  0.020444\n",
      "621283336  0.020657\n",
      "621283335  0.023060\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13892  0.020917\n",
      "13898  0.021246\n",
      "13880  0.021400\n",
      "13893  0.021771\n",
      "13873  0.021882\n",
      "13860  0.021966\n",
      "13850  0.022020\n",
      "67118  0.022088\n",
      "13845  0.022150\n",
      "13855  0.022153\n",
      "13921  0.022287\n",
      "13920  0.022469\n",
      "67825  0.022631\n",
      "66966  0.022671\n",
      "13886  0.022969\n",
      "13913  0.023001\n",
      "13907  0.023878\n",
      "13793  0.024295\n",
      "13859  0.024443\n",
      "13743  0.024733\n",
      ">>> Fastest Solution is rocblas 621283331 0.016651800274848937\n",
      "M N K dtype 768 128 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 128 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283248  0.019444\n",
      "621283393  0.019605\n",
      "621283244  0.019684\n",
      "621283247  0.020005\n",
      "621283249  0.020046\n",
      "621283191  0.020567\n",
      "621283322  0.020647\n",
      "621283530  0.020647\n",
      "621283323  0.020887\n",
      "621283320  0.020927\n",
      "621283187  0.021168\n",
      "621283238  0.021168\n",
      "621286474  0.021168\n",
      "621283319  0.021228\n",
      "621283328  0.021409\n",
      "621283325  0.021429\n",
      "621283334  0.021508\n",
      "621286475  0.021508\n",
      "621283388  0.021529\n",
      "621283189  0.021569\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13896  0.023513\n",
      "13922  0.023734\n",
      "67837  0.023894\n",
      "13852  0.023954\n",
      "13897  0.024235\n",
      "13845  0.024355\n",
      "13884  0.024416\n",
      "13917  0.024816\n",
      "67757  0.024916\n",
      "13855  0.024936\n",
      "13883  0.025016\n",
      "13746  0.025016\n",
      "66941  0.025056\n",
      "13850  0.025157\n",
      "13779  0.025398\n",
      "13892  0.025558\n",
      "13848  0.025618\n",
      "13859  0.025618\n",
      "66939  0.025719\n",
      "13916  0.025758\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283249  0.015587\n",
      "621283247  0.015754\n",
      "621283530  0.015826\n",
      "621283244  0.016185\n",
      "621283322  0.016235\n",
      "621283320  0.016788\n",
      "621286474  0.016798\n",
      "621283334  0.016914\n",
      "621283323  0.016956\n",
      "621283191  0.016989\n",
      "621283189  0.017055\n",
      "621283319  0.017237\n",
      "621283187  0.017301\n",
      "621286475  0.017666\n",
      "621283328  0.018618\n",
      "621283238  0.018640\n",
      "621283325  0.018803\n",
      "621283388  0.019049\n",
      "621283248  0.020831\n",
      "621283393  0.022427\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13845  0.020049\n",
      "13850  0.020104\n",
      "13897  0.020290\n",
      "13884  0.020392\n",
      "13917  0.020452\n",
      "13883  0.020486\n",
      "67757  0.021413\n",
      "13779  0.021529\n",
      "13859  0.021545\n",
      "67837  0.021557\n",
      "13848  0.022026\n",
      "13855  0.022078\n",
      "13916  0.022226\n",
      "13892  0.022266\n",
      "13746  0.022609\n",
      "13852  0.022693\n",
      "13922  0.022808\n",
      "66941  0.022828\n",
      "66939  0.023461\n",
      "13896  0.029994\n",
      ">>> Fastest Solution is rocblas 621283249 0.015587350726127625\n",
      "M N K dtype 4096 128 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 128 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283410  0.018161\n",
      "621283414  0.018181\n",
      "621283539  0.018201\n",
      "621283541  0.018261\n",
      "621283417  0.018683\n",
      "621283416  0.018723\n",
      "621283482  0.018742\n",
      "621283564  0.018823\n",
      "621283411  0.019103\n",
      "621283543  0.019143\n",
      "621283415  0.019444\n",
      "621283620  0.019484\n",
      "621283425  0.019545\n",
      "621283457  0.019605\n",
      "621283542  0.019645\n",
      "621283413  0.019645\n",
      "621283412  0.019865\n",
      "621283545  0.020005\n",
      "621283327  0.020065\n",
      "621283544  0.020065\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13772  0.022250\n",
      "67361  0.023212\n",
      "68102  0.023594\n",
      "66919  0.023794\n",
      "67162  0.023854\n",
      "67527  0.023854\n",
      "67544  0.023875\n",
      "13756  0.023954\n",
      "67086  0.023994\n",
      "13734  0.024035\n",
      "67501  0.024154\n",
      "66864  0.024175\n",
      "13838  0.024195\n",
      "66849  0.024295\n",
      "68873  0.024375\n",
      "67953  0.024516\n",
      "67536  0.024616\n",
      "13866  0.024636\n",
      "66868  0.024756\n",
      "67602  0.024776\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283417  0.015411\n",
      "621283416  0.015664\n",
      "621283564  0.015958\n",
      "621283542  0.016155\n",
      "621283415  0.016253\n",
      "621283412  0.016387\n",
      "621283539  0.016395\n",
      "621283543  0.016415\n",
      "621283620  0.016546\n",
      "621283482  0.016620\n",
      "621283545  0.016698\n",
      "621283327  0.016804\n",
      "621283457  0.016854\n",
      "621283541  0.016868\n",
      "621283425  0.016906\n",
      "621283411  0.016978\n",
      "621283413  0.017147\n",
      "621283544  0.018223\n",
      "621283414  0.022060\n",
      "621283410  0.023209\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67536  0.020781\n",
      "67602  0.021212\n",
      "68102  0.021627\n",
      "67162  0.021769\n",
      "66919  0.021779\n",
      "13866  0.021964\n",
      "67501  0.022062\n",
      "13838  0.022234\n",
      "66868  0.022401\n",
      "67544  0.022499\n",
      "68873  0.022916\n",
      "67527  0.022930\n",
      "67953  0.023048\n",
      "66864  0.023158\n",
      "67361  0.023173\n",
      "67086  0.023187\n",
      "13756  0.023271\n",
      "13734  0.023465\n",
      "66849  0.023527\n",
      "13772  0.028952\n",
      ">>> Fastest Solution is rocblas 621283417 0.015410949289798737\n",
      "M N K dtype 3584 128 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 128 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283191  0.025082\n",
      "621283190  0.025217\n",
      "621286476  0.025463\n",
      "621283189  0.025923\n",
      "621286474  0.025979\n",
      "621283321  0.026150\n",
      "621286475  0.026440\n",
      "621283323  0.026565\n",
      "621283334  0.027327\n",
      "621283389  0.027493\n",
      "621283322  0.027593\n",
      "621283332  0.028344\n",
      "621283330  0.028905\n",
      "621283395  0.029146\n",
      "621283394  0.029187\n",
      "621283324  0.029206\n",
      "621283388  0.029246\n",
      "621283327  0.029246\n",
      "621283325  0.029487\n",
      "621283531  0.029908\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13892  0.027267\n",
      "13852  0.027642\n",
      "13855  0.028500\n",
      "13883  0.028545\n",
      "13893  0.028620\n",
      "13897  0.028905\n",
      "13896  0.029587\n",
      "13915  0.029793\n",
      "67895  0.029808\n",
      "13884  0.029968\n",
      "13779  0.030590\n",
      "13859  0.030905\n",
      "13790  0.031150\n",
      "67808  0.031171\n",
      "13894  0.031652\n",
      "13886  0.031822\n",
      "13916  0.031998\n",
      "13917  0.032038\n",
      "67797  0.032609\n",
      "13732  0.032694\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621286474  0.019778\n",
      "621283190  0.019936\n",
      "621286476  0.020371\n",
      "621286475  0.020829\n",
      "621283322  0.020960\n",
      "621283191  0.021062\n",
      "621283189  0.021172\n",
      "621283334  0.021288\n",
      "621283321  0.021315\n",
      "621283323  0.021362\n",
      "621283389  0.025102\n",
      "621283324  0.026278\n",
      "621283330  0.026556\n",
      "621283327  0.026556\n",
      "621283325  0.026574\n",
      "621283388  0.026865\n",
      "621283395  0.026917\n",
      "621283531  0.027146\n",
      "621283332  0.027152\n",
      "621283394  0.027905\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13917  0.024560\n",
      "13894  0.025120\n",
      "13893  0.025293\n",
      "13892  0.025566\n",
      "13884  0.026270\n",
      "13896  0.026472\n",
      "67797  0.026479\n",
      "13897  0.026923\n",
      "13915  0.026970\n",
      "13883  0.027043\n",
      "67895  0.027226\n",
      "13886  0.027234\n",
      "13916  0.027281\n",
      "67808  0.027414\n",
      "13855  0.027427\n",
      "13779  0.027681\n",
      "13859  0.028195\n",
      "13790  0.028553\n",
      "13852  0.030329\n",
      "13732  0.035128\n",
      ">>> Fastest Solution is rocblas 621286474 0.019777849316596985\n",
      "M N K dtype 4096 128 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 128 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.019955\n",
      "621283246  0.020601\n",
      "621283530  0.020657\n",
      "621286472  0.021013\n",
      "621283321  0.021123\n",
      "621283391  0.021228\n",
      "621283187  0.021309\n",
      "621283191  0.021328\n",
      "621283330  0.021389\n",
      "621283190  0.021649\n",
      "621283322  0.021684\n",
      "621283533  0.021709\n",
      "621283335  0.021830\n",
      "621283320  0.021835\n",
      "621283397  0.021850\n",
      "621283189  0.021864\n",
      "621283258  0.021930\n",
      "621283340  0.022030\n",
      "621283619  0.022030\n",
      "621283342  0.022110\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13898  0.023954\n",
      "13740  0.024094\n",
      "13765  0.024215\n",
      "67777  0.024501\n",
      "13863  0.024575\n",
      "67863  0.025097\n",
      "67755  0.025338\n",
      "13789  0.025338\n",
      "66966  0.025357\n",
      "13896  0.025417\n",
      "67825  0.025438\n",
      "13899  0.025458\n",
      "13873  0.025678\n",
      "67055  0.025679\n",
      "13848  0.025843\n",
      "13851  0.025919\n",
      "66835  0.025939\n",
      "67833  0.025939\n",
      "13788  0.026079\n",
      "13926  0.026220\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283393  0.015906\n",
      "621283530  0.016440\n",
      "621283320  0.016769\n",
      "621283322  0.016904\n",
      "621283191  0.017016\n",
      "621286472  0.017140\n",
      "621283189  0.017216\n",
      "621283330  0.017259\n",
      "621283190  0.017315\n",
      "621283321  0.017567\n",
      "621283335  0.018277\n",
      "621283187  0.018329\n",
      "621283340  0.018726\n",
      "621283397  0.019113\n",
      "621283533  0.019208\n",
      "621283619  0.019631\n",
      "621283391  0.019775\n",
      "621283246  0.019801\n",
      "621283258  0.019897\n",
      "621283342  0.020079\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13898  0.018943\n",
      "13848  0.021625\n",
      "13873  0.021807\n",
      "13926  0.022072\n",
      "67825  0.022413\n",
      "13896  0.022525\n",
      "13788  0.022690\n",
      "13899  0.022703\n",
      "67755  0.022774\n",
      "13863  0.022778\n",
      "67863  0.022822\n",
      "13851  0.022916\n",
      "67833  0.023038\n",
      "13765  0.023098\n",
      "67777  0.023929\n",
      "66966  0.024834\n",
      "13740  0.025434\n",
      "67055  0.027081\n",
      "66835  0.028429\n",
      "13789  0.030209\n",
      ">>> Fastest Solution is rocblas 621283393 0.01590604931116104\n",
      "M N K dtype 768 120 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 120 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.019023\n",
      "621283248  0.019845\n",
      "621283393  0.019865\n",
      "621283244  0.020126\n",
      "621286475  0.020146\n",
      "621283249  0.020146\n",
      "621283245  0.020246\n",
      "621283191  0.020527\n",
      "621283189  0.020627\n",
      "621286476  0.020707\n",
      "621283247  0.020787\n",
      "621283319  0.021128\n",
      "621283323  0.021228\n",
      "621286473  0.021349\n",
      "621283238  0.021428\n",
      "621283334  0.021429\n",
      "621283188  0.021609\n",
      "621283328  0.021629\n",
      "621283187  0.021689\n",
      "621283246  0.021709\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67738  0.023232\n",
      "13852  0.023253\n",
      "67757  0.023494\n",
      "13884  0.023975\n",
      "13883  0.024095\n",
      "13917  0.024215\n",
      "13896  0.024435\n",
      "67837  0.024435\n",
      "67895  0.024475\n",
      "13790  0.024635\n",
      "13845  0.024696\n",
      "67746  0.024816\n",
      "13850  0.025016\n",
      "67865  0.025137\n",
      "13897  0.025177\n",
      "13839  0.025257\n",
      "13893  0.025317\n",
      "66941  0.025378\n",
      "13925  0.025578\n",
      "13779  0.025638\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283245  0.015878\n",
      "621283249  0.015968\n",
      "621283244  0.016088\n",
      "621283191  0.016708\n",
      "621283247  0.016886\n",
      "621283189  0.016928\n",
      "621283246  0.016978\n",
      "621286475  0.017071\n",
      "621283334  0.017129\n",
      "621286476  0.017215\n",
      "621283188  0.017253\n",
      "621283323  0.017345\n",
      "621286473  0.017373\n",
      "621283319  0.017869\n",
      "621283187  0.017905\n",
      "621283238  0.018600\n",
      "621283328  0.018771\n",
      "621283393  0.018951\n",
      "621283248  0.019444\n",
      "621283530  0.022469\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13850  0.019929\n",
      "13897  0.020434\n",
      "13896  0.020879\n",
      "67837  0.020984\n",
      "13839  0.021034\n",
      "13883  0.021054\n",
      "67757  0.021272\n",
      "13884  0.021296\n",
      "13845  0.021356\n",
      "13917  0.021655\n",
      "67746  0.021739\n",
      "67865  0.021775\n",
      "13779  0.021900\n",
      "13893  0.021930\n",
      "67895  0.022142\n",
      "13925  0.022220\n",
      "13790  0.022242\n",
      "66941  0.022491\n",
      "13852  0.024141\n",
      "67738  0.026642\n",
      ">>> Fastest Solution is rocblas 621283245 0.015877999365329742\n",
      "M N K dtype 4096 120 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 120 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283370  0.017280\n",
      "621283378  0.017701\n",
      "621283371  0.018061\n",
      "621283379  0.019003\n",
      "621283620  0.019304\n",
      "621283418  0.019344\n",
      "621283540  0.019645\n",
      "621283413  0.019845\n",
      "621283327  0.019946\n",
      "621283417  0.019946\n",
      "621283469  0.020005\n",
      "621283405  0.020166\n",
      "621283335  0.020226\n",
      "621283412  0.020226\n",
      "621283410  0.020266\n",
      "621283244  0.020306\n",
      "621283380  0.020406\n",
      "621283329  0.020406\n",
      "621283338  0.020487\n",
      "621283416  0.020527\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.023212\n",
      "68873  0.023594\n",
      "67527  0.023794\n",
      "67410  0.023875\n",
      "13772  0.024115\n",
      "67361  0.024195\n",
      "13756  0.024335\n",
      "66932  0.024535\n",
      "13730  0.024616\n",
      "67602  0.024676\n",
      "66899  0.024736\n",
      "67162  0.024857\n",
      "13878  0.024896\n",
      "67086  0.024936\n",
      "67244  0.024997\n",
      "67501  0.025057\n",
      "13903  0.025117\n",
      "66840  0.025117\n",
      "66855  0.025157\n",
      "66879  0.025357\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283380  0.016736\n",
      "621283417  0.016770\n",
      "621283412  0.016816\n",
      "621283469  0.016822\n",
      "621283410  0.016892\n",
      "621283540  0.016952\n",
      "621283335  0.016973\n",
      "621283418  0.017137\n",
      "621283244  0.017197\n",
      "621283379  0.017415\n",
      "621283327  0.017508\n",
      "621283416  0.017554\n",
      "621283405  0.017644\n",
      "621283413  0.017734\n",
      "621283329  0.017814\n",
      "621283338  0.017832\n",
      "621283620  0.018157\n",
      "621283371  0.018985\n",
      "621283370  0.019286\n",
      "621283378  0.019913\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67602  0.021503\n",
      "67162  0.021884\n",
      "67086  0.021978\n",
      "13903  0.022088\n",
      "67501  0.022186\n",
      "66899  0.022190\n",
      "66879  0.022341\n",
      "13730  0.022363\n",
      "67361  0.022401\n",
      "13878  0.022726\n",
      "13756  0.022850\n",
      "67244  0.022856\n",
      "66840  0.023016\n",
      "68873  0.023267\n",
      "13772  0.023321\n",
      "66932  0.023606\n",
      "66855  0.023622\n",
      "67410  0.023682\n",
      "67527  0.023824\n",
      "13911  0.027803\n",
      ">>> Fastest Solution is rocblas 621283380 0.016735999286174773\n",
      "M N K dtype 3584 120 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 120 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286476  0.024135\n",
      "621283191  0.024836\n",
      "621286475  0.025398\n",
      "621283329  0.025538\n",
      "621283388  0.025858\n",
      "621283321  0.025964\n",
      "621283190  0.026144\n",
      "621283189  0.026145\n",
      "621283332  0.026260\n",
      "621283323  0.026309\n",
      "621286474  0.026671\n",
      "621283334  0.026981\n",
      "621283322  0.027097\n",
      "621283324  0.027943\n",
      "621283328  0.027943\n",
      "621283330  0.028083\n",
      "621283325  0.028204\n",
      "621283327  0.029166\n",
      "621283320  0.029312\n",
      "621283395  0.029316\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13917  0.024510\n",
      "13852  0.028084\n",
      "13883  0.028123\n",
      "13896  0.028244\n",
      "13897  0.028545\n",
      "13893  0.028956\n",
      "13892  0.028960\n",
      "13894  0.029116\n",
      "13886  0.029487\n",
      "13915  0.029547\n",
      "13855  0.029797\n",
      "13916  0.029873\n",
      "13746  0.030209\n",
      "13859  0.030675\n",
      "13790  0.030931\n",
      "13884  0.030990\n",
      "67895  0.031171\n",
      "13779  0.031381\n",
      "67808  0.032469\n",
      "13846  0.032619\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283191  0.020180\n",
      "621286475  0.020865\n",
      "621286476  0.021643\n",
      "621283321  0.023251\n",
      "621283323  0.023256\n",
      "621283334  0.023674\n",
      "621286474  0.023773\n",
      "621283322  0.023865\n",
      "621283189  0.024025\n",
      "621283190  0.024460\n",
      "621283328  0.025181\n",
      "621283325  0.025350\n",
      "621283327  0.025364\n",
      "621283388  0.025516\n",
      "621283332  0.025590\n",
      "621283330  0.025937\n",
      "621283324  0.026534\n",
      "621283395  0.026741\n",
      "621283329  0.027987\n",
      "621283320  0.029333\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13916  0.023495\n",
      "13893  0.024307\n",
      "13896  0.024431\n",
      "13915  0.024810\n",
      "13894  0.024816\n",
      "13884  0.024884\n",
      "13897  0.025025\n",
      "13883  0.025295\n",
      "13892  0.025419\n",
      "13855  0.026794\n",
      "13859  0.027051\n",
      "13886  0.027077\n",
      "13790  0.027298\n",
      "13779  0.027879\n",
      "13852  0.028288\n",
      "67895  0.028603\n",
      "13746  0.028661\n",
      "67808  0.030179\n",
      "13846  0.030999\n",
      "13917  0.031287\n",
      ">>> Fastest Solution is rocblas 621283191 0.02017975002527237\n",
      "M N K dtype 4096 120 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 120 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.018913\n",
      "621283338  0.019785\n",
      "621283393  0.020031\n",
      "621283324  0.020627\n",
      "621283335  0.020707\n",
      "621283395  0.020767\n",
      "621283340  0.020767\n",
      "621283341  0.020807\n",
      "621283189  0.020843\n",
      "621283330  0.020868\n",
      "621283399  0.020987\n",
      "621283619  0.021008\n",
      "621283617  0.021027\n",
      "621283187  0.021168\n",
      "621283391  0.021249\n",
      "621283397  0.021269\n",
      "621283388  0.021349\n",
      "621283533  0.021428\n",
      "621283327  0.021429\n",
      "621283366  0.021509\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13844  0.024696\n",
      "67825  0.024696\n",
      "66957  0.024942\n",
      "67856  0.024976\n",
      "13860  0.025457\n",
      "67863  0.025498\n",
      "13846  0.025653\n",
      "13851  0.025819\n",
      "13770  0.025919\n",
      "67903  0.026159\n",
      "67755  0.026220\n",
      "13755  0.026360\n",
      "13876  0.026400\n",
      "36974  0.026420\n",
      "67738  0.026515\n",
      "13907  0.026661\n",
      "66859  0.026741\n",
      "13743  0.026756\n",
      "66966  0.026780\n",
      "13879  0.026790\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283393  0.015295\n",
      "621283530  0.016173\n",
      "621283330  0.016768\n",
      "621283324  0.017013\n",
      "621283388  0.017287\n",
      "621283189  0.017340\n",
      "621283395  0.017430\n",
      "621283187  0.017546\n",
      "621283327  0.017636\n",
      "621283340  0.018642\n",
      "621283366  0.018787\n",
      "621283391  0.018819\n",
      "621283341  0.018835\n",
      "621283399  0.019007\n",
      "621283397  0.019328\n",
      "621283619  0.019454\n",
      "621283335  0.019749\n",
      "621283533  0.019899\n",
      "621283617  0.020031\n",
      "621283338  0.024828\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13860  0.022349\n",
      "66957  0.022366\n",
      "13846  0.022387\n",
      "67738  0.022390\n",
      "67755  0.022459\n",
      "13876  0.022477\n",
      "13879  0.022494\n",
      "67903  0.022633\n",
      "13851  0.022671\n",
      "13907  0.022769\n",
      "67856  0.022950\n",
      "13743  0.022962\n",
      "66966  0.023401\n",
      "13755  0.023420\n",
      "67863  0.023812\n",
      "13770  0.023866\n",
      "67825  0.025125\n",
      "66859  0.025339\n",
      "13844  0.026077\n",
      "36974  0.026492\n",
      ">>> Fastest Solution is rocblas 621283393 0.015294699370861054\n",
      "M N K dtype 768 112 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 112 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.019183\n",
      "621283248  0.020767\n",
      "621283530  0.020768\n",
      "621283244  0.020787\n",
      "621286474  0.020968\n",
      "621283334  0.021008\n",
      "621283319  0.021027\n",
      "621283191  0.021048\n",
      "621286473  0.021148\n",
      "621283392  0.021228\n",
      "621283190  0.021228\n",
      "621286476  0.021428\n",
      "621283322  0.021429\n",
      "621283325  0.021589\n",
      "621283328  0.021589\n",
      "621283321  0.021629\n",
      "621286472  0.021809\n",
      "621283257  0.021910\n",
      "621283329  0.021990\n",
      "621283245  0.022010\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13917  0.022671\n",
      "13850  0.022952\n",
      "13896  0.023333\n",
      "13884  0.023333\n",
      "13883  0.023373\n",
      "13845  0.023393\n",
      "67757  0.023653\n",
      "67837  0.023674\n",
      "67746  0.024375\n",
      "67895  0.024456\n",
      "13746  0.024495\n",
      "66957  0.024496\n",
      "67865  0.024516\n",
      "13779  0.024556\n",
      "67808  0.024656\n",
      "13906  0.024736\n",
      "13855  0.024836\n",
      "67678  0.024916\n",
      "13859  0.024957\n",
      "13897  0.025037\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283244  0.016032\n",
      "621283245  0.016614\n",
      "621283530  0.016922\n",
      "621286476  0.017007\n",
      "621283319  0.017055\n",
      "621283321  0.017063\n",
      "621283322  0.017079\n",
      "621283190  0.017133\n",
      "621286474  0.017201\n",
      "621286473  0.017233\n",
      "621283334  0.017357\n",
      "621286472  0.017774\n",
      "621283191  0.018045\n",
      "621283392  0.018213\n",
      "621283248  0.018271\n",
      "621283328  0.018773\n",
      "621283325  0.019272\n",
      "621283329  0.019590\n",
      "621283257  0.020116\n",
      "621283393  0.026456\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.020382\n",
      "13897  0.020793\n",
      "13845  0.020947\n",
      "67746  0.021056\n",
      "67837  0.021369\n",
      "67757  0.021421\n",
      "67808  0.021535\n",
      "67678  0.021603\n",
      "13896  0.021767\n",
      "13883  0.021862\n",
      "67895  0.021952\n",
      "13859  0.022226\n",
      "13779  0.022234\n",
      "13746  0.022349\n",
      "13855  0.022405\n",
      "13906  0.023128\n",
      "67865  0.023535\n",
      "66957  0.023662\n",
      "13850  0.024051\n",
      "13917  0.029056\n",
      ">>> Fastest Solution is rocblas 621283244 0.016032350063323975\n",
      "M N K dtype 4096 112 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 112 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283378  0.017740\n",
      "621283371  0.018021\n",
      "621283370  0.018582\n",
      "621283342  0.018943\n",
      "621283564  0.019584\n",
      "621283418  0.019624\n",
      "621283379  0.019624\n",
      "621283416  0.019645\n",
      "621283380  0.019764\n",
      "621283374  0.019765\n",
      "621283417  0.019906\n",
      "621286476  0.020246\n",
      "621283369  0.020266\n",
      "621283620  0.020366\n",
      "621283469  0.020366\n",
      "621283372  0.020427\n",
      "621283482  0.020606\n",
      "621283543  0.020767\n",
      "621283398  0.021268\n",
      "621283408  0.021409\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13903  0.023694\n",
      "13870  0.023975\n",
      "67170  0.024094\n",
      "67280  0.024155\n",
      "67501  0.024495\n",
      "67566  0.024516\n",
      "13916  0.024656\n",
      "13820  0.024716\n",
      "68381  0.024716\n",
      "67953  0.024716\n",
      "67602  0.024717\n",
      "66868  0.024797\n",
      "13911  0.024876\n",
      "13772  0.025017\n",
      "13819  0.025037\n",
      "67244  0.025117\n",
      "67156  0.025157\n",
      "67361  0.025197\n",
      "66899  0.025237\n",
      "67162  0.025237\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283370  0.015824\n",
      "621283372  0.016271\n",
      "621283379  0.016507\n",
      "621283417  0.016684\n",
      "621286476  0.016726\n",
      "621283374  0.016754\n",
      "621283369  0.016888\n",
      "621283418  0.016918\n",
      "621283380  0.017147\n",
      "621283398  0.017163\n",
      "621283342  0.017412\n",
      "621283620  0.017454\n",
      "621283416  0.017508\n",
      "621283408  0.017528\n",
      "621283482  0.017983\n",
      "621283543  0.018101\n",
      "621283469  0.018265\n",
      "621283564  0.018845\n",
      "621283371  0.019711\n",
      "621283378  0.020743\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67602  0.021453\n",
      "67566  0.021966\n",
      "67244  0.022002\n",
      "13911  0.022044\n",
      "66899  0.022268\n",
      "66868  0.022515\n",
      "13916  0.022569\n",
      "67361  0.022581\n",
      "13820  0.022854\n",
      "67501  0.022932\n",
      "13819  0.022996\n",
      "67953  0.023008\n",
      "67162  0.023080\n",
      "67280  0.023134\n",
      "67156  0.023155\n",
      "13772  0.023307\n",
      "67170  0.025029\n",
      "13903  0.025241\n",
      "13870  0.027198\n",
      "68381  0.027364\n",
      ">>> Fastest Solution is rocblas 621283370 0.01582390069961548\n",
      "M N K dtype 3584 112 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 112 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283332  0.023994\n",
      "621283329  0.025357\n",
      "621283322  0.025583\n",
      "621283191  0.025753\n",
      "621283323  0.025874\n",
      "621283325  0.026019\n",
      "621286475  0.026029\n",
      "621283321  0.026084\n",
      "621283190  0.026135\n",
      "621286476  0.026395\n",
      "621283334  0.026886\n",
      "621283189  0.027006\n",
      "621283395  0.027061\n",
      "621283328  0.027342\n",
      "621283324  0.027583\n",
      "621286474  0.027608\n",
      "621283389  0.027803\n",
      "621283388  0.028104\n",
      "621283330  0.028123\n",
      "621283394  0.028324\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13917  0.023954\n",
      "13855  0.026476\n",
      "13894  0.026966\n",
      "13896  0.027602\n",
      "13893  0.027617\n",
      "13892  0.027663\n",
      "13746  0.027913\n",
      "13916  0.027953\n",
      "13852  0.028104\n",
      "13884  0.028144\n",
      "67808  0.028194\n",
      "13883  0.028405\n",
      "13897  0.028465\n",
      "13859  0.028746\n",
      "13886  0.029001\n",
      "13779  0.029166\n",
      "13915  0.029171\n",
      "67895  0.029361\n",
      "67797  0.031537\n",
      "13790  0.032408\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283321  0.021009\n",
      "621283189  0.022676\n",
      "621286475  0.022979\n",
      "621283334  0.023006\n",
      "621283191  0.023049\n",
      "621283190  0.023315\n",
      "621283323  0.023347\n",
      "621286476  0.023477\n",
      "621286474  0.023561\n",
      "621283322  0.023925\n",
      "621283330  0.024047\n",
      "621283388  0.025193\n",
      "621283328  0.025247\n",
      "621283324  0.025574\n",
      "621283394  0.026616\n",
      "621283395  0.026705\n",
      "621283325  0.026849\n",
      "621283389  0.027584\n",
      "621283329  0.028026\n",
      "621283332  0.032377\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13894  0.022937\n",
      "13893  0.022983\n",
      "13855  0.023672\n",
      "13916  0.023704\n",
      "13883  0.024199\n",
      "13897  0.024522\n",
      "13884  0.025311\n",
      "13892  0.025695\n",
      "13852  0.025734\n",
      "13896  0.025989\n",
      "13915  0.026291\n",
      "13886  0.026498\n",
      "13779  0.026977\n",
      "67895  0.027051\n",
      "13859  0.027241\n",
      "13790  0.027316\n",
      "13746  0.027560\n",
      "67797  0.028336\n",
      "67808  0.030234\n",
      "13917  0.031389\n",
      ">>> Fastest Solution is rocblas 621283321 0.02100915014743805\n",
      "M N K dtype 4096 112 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 112 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.017815\n",
      "621283337  0.019565\n",
      "621283363  0.019584\n",
      "621283393  0.019734\n",
      "621283234  0.019965\n",
      "621283319  0.020086\n",
      "621283392  0.021142\n",
      "621283390  0.021228\n",
      "621283187  0.021249\n",
      "621283532  0.021609\n",
      "621283398  0.021709\n",
      "621283332  0.021769\n",
      "621283327  0.021809\n",
      "621283619  0.021869\n",
      "621283191  0.021939\n",
      "621283189  0.022105\n",
      "621286476  0.022160\n",
      "621283251  0.022171\n",
      "621286475  0.022241\n",
      "621283397  0.022250\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67856  0.024836\n",
      "67738  0.025638\n",
      "67055  0.025698\n",
      "13849  0.025758\n",
      "67825  0.025859\n",
      "67678  0.025979\n",
      "67755  0.026260\n",
      "13843  0.026340\n",
      "67757  0.026576\n",
      "67903  0.026680\n",
      "66957  0.026710\n",
      "13912  0.026735\n",
      "13886  0.026851\n",
      "13864  0.026860\n",
      "67045  0.026861\n",
      "13921  0.026891\n",
      "67692  0.026921\n",
      "67972  0.026961\n",
      "67951  0.027042\n",
      "13844  0.027061\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.015692\n",
      "621283191  0.016619\n",
      "621283332  0.016634\n",
      "621283319  0.016841\n",
      "621283327  0.016948\n",
      "621286476  0.017078\n",
      "621283187  0.017084\n",
      "621286475  0.017138\n",
      "621283189  0.017186\n",
      "621283393  0.017453\n",
      "621283619  0.019093\n",
      "621283392  0.019127\n",
      "621283390  0.019296\n",
      "621283234  0.019472\n",
      "621283397  0.019556\n",
      "621283398  0.019685\n",
      "621283532  0.019687\n",
      "621283363  0.020374\n",
      "621283337  0.023507\n",
      "621283251  0.026907\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13921  0.021810\n",
      "67951  0.021837\n",
      "13886  0.021884\n",
      "66957  0.021988\n",
      "13864  0.022030\n",
      "13912  0.022090\n",
      "13844  0.022162\n",
      "67825  0.022174\n",
      "67755  0.022256\n",
      "67757  0.022328\n",
      "13843  0.022467\n",
      "67903  0.023064\n",
      "67678  0.023274\n",
      "67738  0.023456\n",
      "13849  0.023788\n",
      "67045  0.023990\n",
      "67972  0.024576\n",
      "67692  0.024694\n",
      "67055  0.026061\n",
      "67856  0.029132\n",
      ">>> Fastest Solution is rocblas 621283530 0.015691550076007844\n",
      "M N K dtype 768 104 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 104 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.018983\n",
      "621283530  0.019544\n",
      "621283319  0.020546\n",
      "621283191  0.020606\n",
      "621283189  0.021027\n",
      "621283320  0.021288\n",
      "621283238  0.021368\n",
      "621286474  0.021428\n",
      "621283334  0.021489\n",
      "621283328  0.021508\n",
      "621283325  0.021528\n",
      "621286476  0.021589\n",
      "621283388  0.021609\n",
      "621283322  0.021750\n",
      "621283188  0.021769\n",
      "621283323  0.021790\n",
      "621283190  0.021790\n",
      "621283321  0.021830\n",
      "621283392  0.021869\n",
      "621286475  0.021870\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13850  0.023674\n",
      "13884  0.023674\n",
      "67837  0.023814\n",
      "13897  0.023874\n",
      "67757  0.023935\n",
      "13896  0.023954\n",
      "13917  0.024276\n",
      "67895  0.024295\n",
      "67808  0.024395\n",
      "67865  0.024416\n",
      "13845  0.024516\n",
      "13883  0.024556\n",
      "13779  0.024596\n",
      "67746  0.024636\n",
      "13790  0.024757\n",
      "66957  0.024997\n",
      "13894  0.025498\n",
      "13746  0.025578\n",
      "13893  0.025638\n",
      "66941  0.025698\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283322  0.016175\n",
      "621286474  0.016179\n",
      "621283190  0.016519\n",
      "621283191  0.016648\n",
      "621286475  0.016692\n",
      "621283323  0.016696\n",
      "621283321  0.016772\n",
      "621283189  0.016814\n",
      "621283334  0.016820\n",
      "621283188  0.016904\n",
      "621286476  0.016969\n",
      "621283319  0.017073\n",
      "621283320  0.017277\n",
      "621283392  0.017568\n",
      "621283325  0.018851\n",
      "621283328  0.018861\n",
      "621283388  0.018947\n",
      "621283530  0.019027\n",
      "621283238  0.019370\n",
      "621283393  0.025530\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13896  0.020619\n",
      "13845  0.020661\n",
      "13883  0.020759\n",
      "13917  0.020932\n",
      "67746  0.021597\n",
      "67808  0.021641\n",
      "67837  0.021681\n",
      "13779  0.021864\n",
      "67757  0.021940\n",
      "13893  0.021952\n",
      "13790  0.022018\n",
      "67895  0.022152\n",
      "13746  0.022182\n",
      "66957  0.022405\n",
      "13897  0.022439\n",
      "13894  0.022756\n",
      "66941  0.022992\n",
      "67865  0.023046\n",
      "13884  0.024506\n",
      "13850  0.029363\n",
      ">>> Fastest Solution is rocblas 621283322 0.01617469936609268\n",
      "M N K dtype 4096 104 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 104 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283371  0.018282\n",
      "621283656  0.019164\n",
      "621283540  0.019183\n",
      "621283620  0.019484\n",
      "621283376  0.019505\n",
      "621283410  0.019664\n",
      "621283418  0.019705\n",
      "621283564  0.019724\n",
      "621283539  0.019785\n",
      "621283469  0.019845\n",
      "621283543  0.019945\n",
      "621283370  0.019965\n",
      "621283413  0.020065\n",
      "621283417  0.020266\n",
      "621283378  0.020586\n",
      "621283416  0.020667\n",
      "621283244  0.020687\n",
      "621283246  0.020707\n",
      "621283248  0.020808\n",
      "621283234  0.020827\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "68873  0.023293\n",
      "67280  0.023493\n",
      "67410  0.024155\n",
      "66868  0.024435\n",
      "13870  0.024456\n",
      "66932  0.024575\n",
      "67361  0.024596\n",
      "67527  0.024656\n",
      "13919  0.024716\n",
      "67086  0.024836\n",
      "13772  0.024876\n",
      "67142  0.024916\n",
      "13867  0.024957\n",
      "13866  0.025017\n",
      "66899  0.025057\n",
      "67170  0.025077\n",
      "13849  0.025117\n",
      "67244  0.025137\n",
      "67104  0.025257\n",
      "67953  0.025277\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283418  0.016161\n",
      "621283410  0.016706\n",
      "621283656  0.016808\n",
      "621283540  0.016852\n",
      "621283469  0.016864\n",
      "621283564  0.016920\n",
      "621283244  0.016928\n",
      "621283539  0.017047\n",
      "621283413  0.017257\n",
      "621283248  0.017422\n",
      "621283376  0.017460\n",
      "621283416  0.017504\n",
      "621283417  0.017594\n",
      "621283246  0.018005\n",
      "621283378  0.018039\n",
      "621283370  0.018059\n",
      "621283234  0.018157\n",
      "621283543  0.018197\n",
      "621283620  0.018566\n",
      "621283371  0.024183\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13919  0.021491\n",
      "13867  0.021721\n",
      "13866  0.021888\n",
      "67104  0.022032\n",
      "13870  0.022188\n",
      "66899  0.022367\n",
      "13849  0.022379\n",
      "67142  0.022381\n",
      "67086  0.022471\n",
      "66868  0.022491\n",
      "67244  0.022900\n",
      "67527  0.022908\n",
      "67170  0.023461\n",
      "67361  0.023501\n",
      "66932  0.023559\n",
      "67953  0.023571\n",
      "67410  0.023880\n",
      "13772  0.024263\n",
      "67280  0.024880\n",
      "68873  0.030658\n",
      ">>> Fastest Solution is rocblas 621283418 0.01616065055131912\n",
      "M N K dtype 3584 104 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 104 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286474  0.022631\n",
      "621283190  0.023298\n",
      "621283325  0.024676\n",
      "621283328  0.025638\n",
      "621283191  0.025864\n",
      "621286475  0.026029\n",
      "621286476  0.026069\n",
      "621283189  0.026084\n",
      "621283330  0.026099\n",
      "621283323  0.026119\n",
      "621283321  0.026310\n",
      "621283329  0.026720\n",
      "621283334  0.026720\n",
      "621283322  0.026826\n",
      "621283327  0.026982\n",
      "621283332  0.027021\n",
      "621283324  0.027302\n",
      "621283388  0.027743\n",
      "621283394  0.028665\n",
      "621283531  0.028665\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13917  0.024194\n",
      "13894  0.025673\n",
      "13896  0.025719\n",
      "13883  0.026460\n",
      "13852  0.027101\n",
      "13897  0.027763\n",
      "13884  0.028926\n",
      "13779  0.028950\n",
      "13859  0.029020\n",
      "13893  0.029166\n",
      "13855  0.029191\n",
      "13886  0.029316\n",
      "67004  0.029346\n",
      "67837  0.029628\n",
      "13916  0.029677\n",
      "67895  0.029757\n",
      "13915  0.029868\n",
      "13746  0.030048\n",
      "13790  0.030189\n",
      "67797  0.030995\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283190  0.020282\n",
      "621283191  0.021386\n",
      "621283321  0.022733\n",
      "621286474  0.022760\n",
      "621286476  0.022880\n",
      "621286475  0.022919\n",
      "621283189  0.023459\n",
      "621283323  0.023668\n",
      "621283322  0.023941\n",
      "621283334  0.023993\n",
      "621283388  0.024764\n",
      "621283324  0.024802\n",
      "621283332  0.024832\n",
      "621283328  0.024866\n",
      "621283329  0.025664\n",
      "621283330  0.025674\n",
      "621283531  0.026310\n",
      "621283327  0.026434\n",
      "621283394  0.026967\n",
      "621283325  0.027679\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13894  0.021631\n",
      "13916  0.023583\n",
      "13884  0.024191\n",
      "13852  0.024195\n",
      "13893  0.024757\n",
      "13897  0.025025\n",
      "13883  0.025043\n",
      "13896  0.025819\n",
      "13915  0.026275\n",
      "13855  0.026365\n",
      "13886  0.026893\n",
      "13746  0.026909\n",
      "13790  0.027007\n",
      "13779  0.027011\n",
      "13859  0.027036\n",
      "67797  0.027305\n",
      "67895  0.028182\n",
      "67837  0.030376\n",
      "13917  0.031686\n",
      "67004  0.033761\n",
      ">>> Fastest Solution is rocblas 621283190 0.020282000303268433\n",
      "M N K dtype 4096 104 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 104 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.019068\n",
      "621283530  0.020060\n",
      "621283619  0.020366\n",
      "621283319  0.020767\n",
      "621283328  0.020767\n",
      "621283363  0.020887\n",
      "621283246  0.021053\n",
      "621283532  0.021068\n",
      "621283394  0.021088\n",
      "621283338  0.021148\n",
      "621283340  0.021429\n",
      "621283331  0.021769\n",
      "621283365  0.021769\n",
      "621283388  0.021909\n",
      "621283335  0.021950\n",
      "621283323  0.022005\n",
      "621283396  0.022050\n",
      "621286474  0.022070\n",
      "621286476  0.022085\n",
      "621283187  0.022110\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13860  0.023473\n",
      "13896  0.023534\n",
      "13791  0.024736\n",
      "13770  0.024797\n",
      "67678  0.024836\n",
      "67856  0.024856\n",
      "13846  0.025212\n",
      "13920  0.025363\n",
      "67738  0.025493\n",
      "13851  0.025518\n",
      "13848  0.025883\n",
      "13912  0.025959\n",
      "13788  0.025999\n",
      "67045  0.026059\n",
      "13743  0.026075\n",
      "13844  0.026079\n",
      "13905  0.026094\n",
      "13873  0.026099\n",
      "67757  0.026320\n",
      "13843  0.026380\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.016381\n",
      "621283393  0.016808\n",
      "621283323  0.016874\n",
      "621286476  0.016903\n",
      "621283331  0.017035\n",
      "621283394  0.017295\n",
      "621283328  0.017377\n",
      "621283319  0.017530\n",
      "621283388  0.017814\n",
      "621283335  0.017854\n",
      "621286474  0.017896\n",
      "621283363  0.018209\n",
      "621283365  0.018279\n",
      "621283187  0.018583\n",
      "621283340  0.018586\n",
      "621283338  0.018690\n",
      "621283246  0.019215\n",
      "621283396  0.019494\n",
      "621283532  0.019522\n",
      "621283619  0.024792\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13791  0.021745\n",
      "13843  0.022220\n",
      "13848  0.022233\n",
      "13788  0.022401\n",
      "13920  0.022430\n",
      "13905  0.022443\n",
      "13912  0.022522\n",
      "13844  0.022760\n",
      "13846  0.022800\n",
      "13873  0.022844\n",
      "67757  0.022928\n",
      "13770  0.023046\n",
      "13851  0.023128\n",
      "67856  0.023156\n",
      "13896  0.023211\n",
      "67738  0.023742\n",
      "13743  0.023762\n",
      "67678  0.023802\n",
      "67045  0.023864\n",
      "13860  0.027759\n",
      ">>> Fastest Solution is rocblas 621283530 0.01638115048408508\n",
      "M N K dtype 768 96 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 96 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.020205\n",
      "621286475  0.020306\n",
      "621283191  0.020727\n",
      "621283334  0.021048\n",
      "621286476  0.021087\n",
      "621283320  0.021088\n",
      "621283248  0.021108\n",
      "621283188  0.021128\n",
      "621283238  0.021169\n",
      "621283322  0.021208\n",
      "621283245  0.021328\n",
      "621283187  0.021449\n",
      "621283249  0.021529\n",
      "621283388  0.021589\n",
      "621283392  0.021609\n",
      "621283321  0.021689\n",
      "621283319  0.021750\n",
      "621283530  0.021750\n",
      "621286474  0.021769\n",
      "621286473  0.021790\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13926  0.023534\n",
      "13883  0.023854\n",
      "13897  0.023894\n",
      "67895  0.024215\n",
      "13896  0.024235\n",
      "67865  0.024355\n",
      "68751  0.024355\n",
      "13790  0.024435\n",
      "13765  0.024475\n",
      "13779  0.024495\n",
      "13852  0.024495\n",
      "68875  0.024676\n",
      "66985  0.024736\n",
      "13746  0.024816\n",
      "13884  0.025177\n",
      "68804  0.025197\n",
      "13893  0.025277\n",
      "68699  0.025357\n",
      "13850  0.025378\n",
      "66941  0.025398\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.015952\n",
      "621283245  0.016123\n",
      "621283334  0.016165\n",
      "621283188  0.016193\n",
      "621286476  0.016295\n",
      "621283187  0.016301\n",
      "621283191  0.016435\n",
      "621286475  0.016483\n",
      "621286473  0.016542\n",
      "621283319  0.016746\n",
      "621283320  0.016750\n",
      "621286474  0.016894\n",
      "621283322  0.017037\n",
      "621283249  0.017644\n",
      "621283392  0.017744\n",
      "621283248  0.018049\n",
      "621283321  0.018109\n",
      "621283238  0.018478\n",
      "621283388  0.019270\n",
      "621283393  0.024746\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13926  0.014274\n",
      "13884  0.020314\n",
      "13852  0.020334\n",
      "13896  0.020831\n",
      "13850  0.020841\n",
      "67865  0.021150\n",
      "13897  0.021727\n",
      "13746  0.021795\n",
      "13790  0.022098\n",
      "66941  0.022108\n",
      "13779  0.022146\n",
      "13893  0.022575\n",
      "67895  0.022601\n",
      "13765  0.023640\n",
      "13883  0.026073\n",
      "66985  0.026304\n",
      "68751  0.026402\n",
      "68875  0.030219\n",
      "68804  0.031159\n",
      "68699  0.031842\n",
      ">>> Fastest Solution is hipblaslt 13926 0.014274349808692932\n",
      "M N K dtype 4096 96 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 96 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283371  0.018181\n",
      "621283411  0.018522\n",
      "621283379  0.018622\n",
      "621283418  0.018902\n",
      "621283410  0.019283\n",
      "621283417  0.019484\n",
      "621283564  0.019484\n",
      "621283540  0.019524\n",
      "621283656  0.019584\n",
      "621283620  0.019645\n",
      "621283469  0.019765\n",
      "621283413  0.019824\n",
      "621283320  0.019865\n",
      "621283333  0.019985\n",
      "621283370  0.019986\n",
      "621283416  0.020025\n",
      "621283539  0.020086\n",
      "621283380  0.020086\n",
      "621283377  0.020286\n",
      "621283374  0.020326\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "68381  0.023172\n",
      "66862  0.023413\n",
      "13756  0.023553\n",
      "66855  0.023674\n",
      "67879  0.023814\n",
      "67989  0.023994\n",
      "67280  0.024095\n",
      "13772  0.024194\n",
      "68873  0.024235\n",
      "66879  0.024315\n",
      "68726  0.024495\n",
      "68102  0.024616\n",
      "67602  0.024616\n",
      "66983  0.024817\n",
      "13819  0.024836\n",
      "67544  0.024897\n",
      "66899  0.024936\n",
      "67104  0.024997\n",
      "67566  0.025177\n",
      "67213  0.025197\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283540  0.016716\n",
      "621283469  0.016726\n",
      "621283564  0.016774\n",
      "621283656  0.016826\n",
      "621283410  0.016836\n",
      "621283417  0.016958\n",
      "621283620  0.017013\n",
      "621283380  0.017105\n",
      "621283370  0.017165\n",
      "621283418  0.017199\n",
      "621283413  0.017201\n",
      "621283320  0.017313\n",
      "621283411  0.017373\n",
      "621283374  0.017407\n",
      "621283539  0.017504\n",
      "621283377  0.017554\n",
      "621283333  0.017772\n",
      "621283416  0.018502\n",
      "621283379  0.018578\n",
      "621283371  0.023271\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67566  0.020980\n",
      "67602  0.021262\n",
      "68726  0.021296\n",
      "67213  0.021844\n",
      "67280  0.022022\n",
      "66899  0.022188\n",
      "68873  0.022339\n",
      "13819  0.022744\n",
      "67104  0.022746\n",
      "67989  0.022846\n",
      "66983  0.022940\n",
      "67879  0.022992\n",
      "66855  0.023002\n",
      "66879  0.023052\n",
      "68102  0.023297\n",
      "67544  0.023301\n",
      "13756  0.023359\n",
      "13772  0.023411\n",
      "66862  0.023467\n",
      "68381  0.039951\n",
      ">>> Fastest Solution is rocblas 621283540 0.016715900599956514\n",
      "M N K dtype 3584 96 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 96 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286474  0.020867\n",
      "621283191  0.021674\n",
      "621286476  0.022846\n",
      "621283323  0.023875\n",
      "621283190  0.024280\n",
      "621283334  0.025112\n",
      "621286475  0.025393\n",
      "621283530  0.025884\n",
      "621283189  0.025954\n",
      "621283321  0.026209\n",
      "621283332  0.026279\n",
      "621283322  0.026310\n",
      "621283258  0.026380\n",
      "621283395  0.026561\n",
      "621283394  0.026680\n",
      "621283388  0.027302\n",
      "621283325  0.027342\n",
      "621283328  0.027442\n",
      "621283329  0.027682\n",
      "621283330  0.027703\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.024376\n",
      "13917  0.025698\n",
      "13852  0.026239\n",
      "13894  0.026239\n",
      "13893  0.026680\n",
      "13859  0.027042\n",
      "13883  0.027382\n",
      "13884  0.027442\n",
      "13896  0.027663\n",
      "13786  0.027949\n",
      "13855  0.028219\n",
      "13845  0.028264\n",
      "13778  0.028330\n",
      "67895  0.028464\n",
      "13743  0.028780\n",
      "13788  0.029206\n",
      "13790  0.029266\n",
      "67746  0.029267\n",
      "13848  0.029417\n",
      "13779  0.029527\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283191  0.018402\n",
      "621283190  0.018810\n",
      "621286475  0.018849\n",
      "621283189  0.019045\n",
      "621283334  0.019215\n",
      "621286476  0.019281\n",
      "621283321  0.019407\n",
      "621283322  0.019920\n",
      "621283323  0.020729\n",
      "621286474  0.020795\n",
      "621283530  0.023783\n",
      "621283330  0.024524\n",
      "621283328  0.024526\n",
      "621283388  0.024598\n",
      "621283329  0.024616\n",
      "621283325  0.025139\n",
      "621283395  0.025887\n",
      "621283332  0.026135\n",
      "621283394  0.026781\n",
      "621283258  0.031754\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13893  0.023961\n",
      "13896  0.023962\n",
      "13884  0.024486\n",
      "13883  0.024700\n",
      "13894  0.024927\n",
      "13852  0.025097\n",
      "13917  0.025646\n",
      "13855  0.025764\n",
      "13788  0.026515\n",
      "13859  0.026636\n",
      "67895  0.026805\n",
      "13848  0.026968\n",
      "13786  0.027007\n",
      "13845  0.027220\n",
      "13779  0.027236\n",
      "13743  0.027242\n",
      "13790  0.027436\n",
      "13778  0.027799\n",
      "67746  0.029260\n",
      "13897  0.032604\n",
      ">>> Fastest Solution is rocblas 621283191 0.018401700258255004\n",
      "M N K dtype 4096 96 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 96 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283337  0.020727\n",
      "621283236  0.021008\n",
      "621283531  0.021008\n",
      "621283398  0.021148\n",
      "621283399  0.021168\n",
      "621283392  0.021238\n",
      "621283335  0.021328\n",
      "621283532  0.021349\n",
      "621283332  0.021468\n",
      "621283189  0.021589\n",
      "621283617  0.021750\n",
      "621283327  0.021769\n",
      "621283324  0.021769\n",
      "621286474  0.021905\n",
      "621283533  0.021970\n",
      "621283246  0.022060\n",
      "621286472  0.022120\n",
      "621283340  0.022131\n",
      "621283339  0.022171\n",
      "621283191  0.022271\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13913  0.022561\n",
      "13860  0.024696\n",
      "13912  0.024857\n",
      "13844  0.024957\n",
      "67823  0.025839\n",
      "13873  0.025938\n",
      "13779  0.025979\n",
      "67738  0.026339\n",
      "67642  0.026360\n",
      "13843  0.026380\n",
      "13770  0.026400\n",
      "13849  0.026540\n",
      "67863  0.026901\n",
      "67903  0.027002\n",
      "13850  0.027142\n",
      "66998  0.027222\n",
      "67692  0.027262\n",
      "13842  0.027301\n",
      "67916  0.027322\n",
      "13907  0.027332\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283189  0.016453\n",
      "621283324  0.016750\n",
      "621283531  0.016770\n",
      "621283191  0.016933\n",
      "621283327  0.016952\n",
      "621283332  0.017331\n",
      "621283392  0.017515\n",
      "621286474  0.017850\n",
      "621283335  0.018047\n",
      "621286472  0.018306\n",
      "621283340  0.018310\n",
      "621283339  0.019021\n",
      "621283399  0.019290\n",
      "621283532  0.019416\n",
      "621283398  0.019454\n",
      "621283533  0.019645\n",
      "621283246  0.019671\n",
      "621283617  0.020082\n",
      "621283236  0.020611\n",
      "621283337  0.027264\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13913  0.020821\n",
      "13873  0.022258\n",
      "13849  0.022275\n",
      "13907  0.022353\n",
      "13850  0.022445\n",
      "13843  0.022580\n",
      "67916  0.022709\n",
      "13844  0.022786\n",
      "13842  0.022823\n",
      "13860  0.022962\n",
      "67738  0.023213\n",
      "67903  0.023219\n",
      "67823  0.023335\n",
      "13912  0.023708\n",
      "13770  0.023906\n",
      "13779  0.024180\n",
      "67692  0.024313\n",
      "67642  0.024907\n",
      "67863  0.025195\n",
      "66998  0.025887\n",
      ">>> Fastest Solution is rocblas 621283189 0.016453300416469575\n",
      "M N K dtype 768 88 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 88 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283244  0.019404\n",
      "621283393  0.019705\n",
      "621283530  0.020767\n",
      "621283191  0.020847\n",
      "621283187  0.020867\n",
      "621283319  0.020867\n",
      "621283320  0.020868\n",
      "621286475  0.021028\n",
      "621283190  0.021068\n",
      "621283334  0.021088\n",
      "621286473  0.021328\n",
      "621283325  0.021429\n",
      "621283392  0.021468\n",
      "621283322  0.021509\n",
      "621283328  0.021528\n",
      "621283321  0.021609\n",
      "621283388  0.021609\n",
      "621283249  0.021690\n",
      "621283247  0.021869\n",
      "621283395  0.021930\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13850  0.023894\n",
      "68760  0.024295\n",
      "67895  0.024376\n",
      "13884  0.024796\n",
      "13779  0.024876\n",
      "67808  0.024897\n",
      "13917  0.024997\n",
      "13897  0.025057\n",
      "13845  0.025057\n",
      "67678  0.025197\n",
      "13892  0.025277\n",
      "13883  0.025378\n",
      "66941  0.025498\n",
      "67865  0.025538\n",
      "13746  0.025719\n",
      "13852  0.025758\n",
      "13893  0.025819\n",
      "66939  0.025959\n",
      "13915  0.025959\n",
      "13886  0.025979\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283334  0.016205\n",
      "621283530  0.016511\n",
      "621283322  0.016748\n",
      "621286473  0.016756\n",
      "621283321  0.016906\n",
      "621283320  0.017089\n",
      "621283319  0.017311\n",
      "621286475  0.017444\n",
      "621283190  0.017550\n",
      "621283187  0.017810\n",
      "621283392  0.017820\n",
      "621283249  0.017830\n",
      "621283191  0.017853\n",
      "621283247  0.018536\n",
      "621283325  0.018859\n",
      "621283328  0.018929\n",
      "621283388  0.019306\n",
      "621283395  0.020208\n",
      "621283244  0.022888\n",
      "621283393  0.023243\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13897  0.020294\n",
      "13845  0.020334\n",
      "13852  0.020470\n",
      "13883  0.020787\n",
      "13884  0.020825\n",
      "13917  0.021180\n",
      "67808  0.021509\n",
      "13746  0.021870\n",
      "67678  0.021924\n",
      "13779  0.022014\n",
      "13892  0.022050\n",
      "13915  0.022166\n",
      "66939  0.022246\n",
      "67865  0.022273\n",
      "13886  0.022325\n",
      "66941  0.022485\n",
      "13893  0.022495\n",
      "67895  0.022924\n",
      "13850  0.027669\n",
      "68760  0.030489\n",
      ">>> Fastest Solution is rocblas 621283334 0.016204799711704253\n",
      "M N K dtype 4096 88 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 88 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283410  0.018943\n",
      "621283656  0.019023\n",
      "621283416  0.019183\n",
      "621283418  0.019183\n",
      "621283564  0.019304\n",
      "621283469  0.019384\n",
      "621283379  0.019444\n",
      "621283413  0.019544\n",
      "621283411  0.019605\n",
      "621283369  0.019624\n",
      "621283323  0.019764\n",
      "621283380  0.019765\n",
      "621283417  0.019805\n",
      "621283370  0.019905\n",
      "621283336  0.019965\n",
      "621283371  0.019965\n",
      "621283391  0.020065\n",
      "621283258  0.020126\n",
      "621283540  0.020126\n",
      "621283390  0.020226\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "66868  0.023032\n",
      "68873  0.023132\n",
      "13772  0.023413\n",
      "67016  0.023954\n",
      "68893  0.024095\n",
      "13911  0.024275\n",
      "66932  0.024295\n",
      "67527  0.024315\n",
      "66864  0.024315\n",
      "67086  0.024456\n",
      "13870  0.024696\n",
      "67410  0.024696\n",
      "67602  0.024736\n",
      "13866  0.024736\n",
      "67170  0.024876\n",
      "66899  0.024897\n",
      "67244  0.024997\n",
      "68299  0.025016\n",
      "13867  0.025117\n",
      "67594  0.025177\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283656  0.015962\n",
      "621283379  0.016367\n",
      "621283371  0.016465\n",
      "621283469  0.016582\n",
      "621283564  0.016606\n",
      "621283411  0.016864\n",
      "621283390  0.017015\n",
      "621283417  0.017129\n",
      "621283391  0.017165\n",
      "621283540  0.017165\n",
      "621283323  0.017249\n",
      "621283413  0.017251\n",
      "621283369  0.017289\n",
      "621283258  0.017538\n",
      "621283336  0.017736\n",
      "621283370  0.017788\n",
      "621283418  0.018087\n",
      "621283380  0.018530\n",
      "621283416  0.018610\n",
      "621283410  0.025209\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13870  0.020805\n",
      "68873  0.021360\n",
      "13866  0.021741\n",
      "67602  0.021779\n",
      "13867  0.021954\n",
      "66899  0.022164\n",
      "67244  0.022311\n",
      "67086  0.022615\n",
      "67594  0.022657\n",
      "67016  0.022786\n",
      "67410  0.022840\n",
      "67527  0.022906\n",
      "68299  0.022948\n",
      "13911  0.022948\n",
      "66864  0.023112\n",
      "66932  0.023160\n",
      "68893  0.023281\n",
      "13772  0.023589\n",
      "67170  0.023602\n",
      "66868  0.031088\n",
      ">>> Fastest Solution is rocblas 621283656 0.0159621998667717\n",
      "M N K dtype 3584 88 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 88 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286474  0.020316\n",
      "621286475  0.020932\n",
      "621283334  0.023704\n",
      "621283322  0.023899\n",
      "621283320  0.025308\n",
      "621283189  0.025342\n",
      "621286476  0.025347\n",
      "621283332  0.025357\n",
      "621283328  0.025518\n",
      "621283191  0.025523\n",
      "621283321  0.025683\n",
      "621283323  0.025979\n",
      "621283388  0.026641\n",
      "621283190  0.026811\n",
      "621283395  0.027142\n",
      "621283327  0.027222\n",
      "621283324  0.027242\n",
      "621283330  0.027423\n",
      "621283325  0.027523\n",
      "621283531  0.027783\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.024475\n",
      "13845  0.025217\n",
      "13746  0.025398\n",
      "13896  0.025739\n",
      "13850  0.025859\n",
      "13884  0.026160\n",
      "13916  0.027186\n",
      "67757  0.027442\n",
      "67895  0.027563\n",
      "13848  0.027928\n",
      "13883  0.028224\n",
      "67797  0.028289\n",
      "13894  0.028424\n",
      "13917  0.028464\n",
      "67865  0.028524\n",
      "13893  0.028585\n",
      "67837  0.028745\n",
      "13788  0.028871\n",
      "13886  0.029016\n",
      "13852  0.029106\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621286475  0.018765\n",
      "621286476  0.019704\n",
      "621283334  0.020025\n",
      "621283189  0.020042\n",
      "621286474  0.020344\n",
      "621283322  0.020759\n",
      "621283191  0.020847\n",
      "621283190  0.021804\n",
      "621283323  0.022479\n",
      "621283321  0.022725\n",
      "621283324  0.023964\n",
      "621283325  0.024407\n",
      "621283330  0.024676\n",
      "621283328  0.024794\n",
      "621283332  0.025007\n",
      "621283327  0.025097\n",
      "621283388  0.025287\n",
      "621283395  0.025604\n",
      "621283531  0.025982\n",
      "621283320  0.027105\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13896  0.023698\n",
      "13850  0.023822\n",
      "13884  0.023830\n",
      "13883  0.024119\n",
      "13852  0.024171\n",
      "13917  0.024289\n",
      "13894  0.025096\n",
      "67837  0.025201\n",
      "67757  0.025524\n",
      "13848  0.025913\n",
      "67895  0.026047\n",
      "13916  0.026261\n",
      "13845  0.026372\n",
      "13746  0.026536\n",
      "13886  0.026562\n",
      "13788  0.027340\n",
      "13893  0.027864\n",
      "67797  0.027961\n",
      "67865  0.028559\n",
      "13897  0.033113\n",
      ">>> Fastest Solution is rocblas 621286475 0.018764549493789674\n",
      "M N K dtype 4096 88 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 88 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283365  0.019324\n",
      "621283394  0.020908\n",
      "621283327  0.021008\n",
      "621283244  0.021283\n",
      "621283339  0.021349\n",
      "621283392  0.021453\n",
      "621283338  0.021629\n",
      "621283393  0.021769\n",
      "621283398  0.021809\n",
      "621283397  0.021870\n",
      "621283399  0.021949\n",
      "621283341  0.022030\n",
      "621283617  0.022131\n",
      "621283250  0.022150\n",
      "621283337  0.022150\n",
      "621283322  0.022161\n",
      "621283334  0.022436\n",
      "621283533  0.022451\n",
      "621283363  0.022752\n",
      "621283532  0.022812\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13851  0.024957\n",
      "13770  0.024957\n",
      "67505  0.025197\n",
      "13843  0.025648\n",
      "67777  0.025879\n",
      "13741  0.025898\n",
      "67642  0.025899\n",
      "13864  0.025899\n",
      "67903  0.025919\n",
      "67869  0.025919\n",
      "66941  0.026269\n",
      "13879  0.026405\n",
      "66962  0.026540\n",
      "67016  0.026580\n",
      "13831  0.026605\n",
      "13791  0.026680\n",
      "67951  0.026701\n",
      "13853  0.026720\n",
      "68547  0.026741\n",
      "13905  0.026816\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283327  0.016040\n",
      "621283394  0.016155\n",
      "621283322  0.017128\n",
      "621283244  0.017272\n",
      "621283392  0.017543\n",
      "621283334  0.017675\n",
      "621283338  0.017794\n",
      "621283393  0.017996\n",
      "621283363  0.018263\n",
      "621283339  0.018392\n",
      "621283250  0.018761\n",
      "621283337  0.018775\n",
      "621283397  0.018799\n",
      "621283533  0.018915\n",
      "621283399  0.018999\n",
      "621283398  0.019266\n",
      "621283532  0.019312\n",
      "621283341  0.019496\n",
      "621283617  0.019829\n",
      "621283365  0.024393\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67951  0.021739\n",
      "13864  0.021787\n",
      "13853  0.022088\n",
      "13879  0.022599\n",
      "13905  0.022601\n",
      "13843  0.022835\n",
      "66962  0.022854\n",
      "67903  0.022884\n",
      "67869  0.023253\n",
      "66941  0.023847\n",
      "13791  0.024024\n",
      "13831  0.024084\n",
      "67777  0.024247\n",
      "13770  0.024554\n",
      "67642  0.024831\n",
      "67016  0.024997\n",
      "13851  0.026187\n",
      "13741  0.027260\n",
      "67505  0.027617\n",
      "68547  0.028388\n",
      ">>> Fastest Solution is rocblas 621283327 0.01604039967060089\n",
      "M N K dtype 768 80 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 80 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.019645\n",
      "621283244  0.019965\n",
      "621283191  0.020867\n",
      "621283189  0.020908\n",
      "621283319  0.020947\n",
      "621283249  0.020968\n",
      "621286472  0.021027\n",
      "621283247  0.021208\n",
      "621286475  0.021268\n",
      "621283334  0.021349\n",
      "621283188  0.021529\n",
      "621283325  0.021529\n",
      "621283388  0.021650\n",
      "621286473  0.021709\n",
      "621283187  0.021729\n",
      "621283248  0.021790\n",
      "621283392  0.021809\n",
      "621286474  0.021850\n",
      "621283246  0.021869\n",
      "621283320  0.021870\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13845  0.023353\n",
      "13852  0.023473\n",
      "67746  0.024095\n",
      "67772  0.024154\n",
      "67757  0.024416\n",
      "13917  0.024616\n",
      "13884  0.024716\n",
      "13746  0.024876\n",
      "67738  0.024936\n",
      "13892  0.025017\n",
      "67837  0.025317\n",
      "67808  0.025438\n",
      "13790  0.025478\n",
      "13850  0.025518\n",
      "13894  0.025678\n",
      "13893  0.025679\n",
      "13915  0.025779\n",
      "66941  0.025819\n",
      "13855  0.025839\n",
      "13743  0.025859\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283191  0.014653\n",
      "621286474  0.016696\n",
      "621283246  0.016806\n",
      "621283189  0.016832\n",
      "621283319  0.016940\n",
      "621286472  0.017015\n",
      "621286475  0.017045\n",
      "621283187  0.017135\n",
      "621283249  0.017169\n",
      "621283320  0.017225\n",
      "621283188  0.017283\n",
      "621283334  0.017381\n",
      "621283392  0.017410\n",
      "621283247  0.017470\n",
      "621283248  0.017666\n",
      "621286473  0.017742\n",
      "621283325  0.018871\n",
      "621283244  0.018875\n",
      "621283388  0.018915\n",
      "621283530  0.023022\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.020685\n",
      "67808  0.020709\n",
      "13917  0.020807\n",
      "13850  0.020899\n",
      "67757  0.020968\n",
      "67837  0.021316\n",
      "13790  0.021763\n",
      "13894  0.021964\n",
      "13746  0.022044\n",
      "13892  0.022090\n",
      "13855  0.022236\n",
      "13915  0.022299\n",
      "67738  0.022333\n",
      "66941  0.022764\n",
      "13743  0.023128\n",
      "13893  0.023363\n",
      "67746  0.023720\n",
      "13852  0.024526\n",
      "13845  0.030317\n",
      "67772  0.030710\n",
      ">>> Fastest Solution is rocblas 621283191 0.014653199911117553\n",
      "M N K dtype 4096 80 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 80 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283656  0.018662\n",
      "621283378  0.019023\n",
      "621283373  0.019264\n",
      "621283369  0.019304\n",
      "621283411  0.019364\n",
      "621283620  0.019364\n",
      "621283379  0.019384\n",
      "621283564  0.019404\n",
      "621283469  0.019504\n",
      "621283413  0.019605\n",
      "621283380  0.019605\n",
      "621283417  0.019624\n",
      "621283539  0.019724\n",
      "621283534  0.019845\n",
      "621283331  0.019885\n",
      "621283416  0.020025\n",
      "621283238  0.020266\n",
      "621283245  0.020406\n",
      "621283335  0.020427\n",
      "621283391  0.020466\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13911  0.022331\n",
      "13772  0.023513\n",
      "13870  0.023734\n",
      "68873  0.023975\n",
      "66864  0.024095\n",
      "67566  0.024175\n",
      "66879  0.024175\n",
      "67544  0.024215\n",
      "67170  0.024335\n",
      "66868  0.024456\n",
      "67527  0.024516\n",
      "67086  0.024516\n",
      "67280  0.024556\n",
      "66855  0.024635\n",
      "13878  0.024717\n",
      "66899  0.024756\n",
      "13867  0.025016\n",
      "67213  0.025037\n",
      "66923  0.025076\n",
      "67244  0.025077\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283413  0.016397\n",
      "621283417  0.016437\n",
      "621283380  0.016445\n",
      "621283379  0.016511\n",
      "621283469  0.016552\n",
      "621283539  0.016598\n",
      "621283373  0.016906\n",
      "621283335  0.017003\n",
      "621283238  0.017053\n",
      "621283416  0.017167\n",
      "621283391  0.017177\n",
      "621283369  0.017271\n",
      "621283245  0.017319\n",
      "621283620  0.017403\n",
      "621283331  0.017742\n",
      "621283564  0.017864\n",
      "621283411  0.017949\n",
      "621283534  0.018141\n",
      "621283378  0.019552\n",
      "621283656  0.019813\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67566  0.021188\n",
      "67244  0.021473\n",
      "67213  0.021525\n",
      "66923  0.021551\n",
      "13867  0.021651\n",
      "66899  0.021755\n",
      "13870  0.022146\n",
      "13878  0.022278\n",
      "66879  0.022647\n",
      "67527  0.022653\n",
      "66868  0.022661\n",
      "67544  0.022722\n",
      "66855  0.022840\n",
      "68873  0.022944\n",
      "66864  0.023050\n",
      "67086  0.023056\n",
      "67280  0.023293\n",
      "13772  0.023299\n",
      "67170  0.023543\n",
      "13911  0.029204\n",
      ">>> Fastest Solution is rocblas 621283413 0.016397200524806976\n",
      "M N K dtype 3584 80 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 80 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283191  0.022687\n",
      "621286474  0.022832\n",
      "621283321  0.023312\n",
      "621283531  0.024535\n",
      "621283190  0.024551\n",
      "621286476  0.025172\n",
      "621283334  0.025528\n",
      "621283329  0.025879\n",
      "621283327  0.025999\n",
      "621283530  0.026230\n",
      "621283189  0.026235\n",
      "621283323  0.026249\n",
      "621283332  0.026420\n",
      "621283328  0.026520\n",
      "621283388  0.026560\n",
      "621283322  0.026841\n",
      "621283330  0.026881\n",
      "621283324  0.026901\n",
      "621283395  0.026941\n",
      "621283325  0.027001\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67757  0.025357\n",
      "13905  0.025789\n",
      "13920  0.025989\n",
      "13904  0.026129\n",
      "13884  0.026339\n",
      "13845  0.026420\n",
      "13907  0.026505\n",
      "13896  0.026540\n",
      "13842  0.026591\n",
      "13897  0.026620\n",
      "13846  0.026695\n",
      "13852  0.026841\n",
      "13883  0.027001\n",
      "13913  0.027377\n",
      "13912  0.027542\n",
      "13922  0.027553\n",
      "13917  0.027623\n",
      "13841  0.027632\n",
      "13740  0.027818\n",
      "13879  0.027949\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621286476  0.018926\n",
      "621283190  0.019085\n",
      "621283334  0.019086\n",
      "621283321  0.019156\n",
      "621286474  0.019508\n",
      "621283189  0.021162\n",
      "621283191  0.021238\n",
      "621283323  0.021584\n",
      "621283325  0.023501\n",
      "621283328  0.023652\n",
      "621283322  0.024017\n",
      "621283388  0.024028\n",
      "621283395  0.024107\n",
      "621283330  0.024389\n",
      "621283324  0.024826\n",
      "621283332  0.025103\n",
      "621283329  0.025374\n",
      "621283327  0.025542\n",
      "621283531  0.025997\n",
      "621283530  0.026017\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13896  0.023668\n",
      "13845  0.023880\n",
      "13917  0.023892\n",
      "13922  0.023985\n",
      "13883  0.024213\n",
      "13920  0.024332\n",
      "13842  0.024413\n",
      "13852  0.024469\n",
      "13879  0.024580\n",
      "13884  0.024752\n",
      "13897  0.024780\n",
      "13905  0.024808\n",
      "13912  0.025110\n",
      "13913  0.025345\n",
      "13904  0.025401\n",
      "13846  0.025758\n",
      "13907  0.026178\n",
      "13841  0.026682\n",
      "13740  0.027233\n",
      "67757  0.032642\n",
      ">>> Fastest Solution is rocblas 621286476 0.018926399946212768\n",
      "M N K dtype 4096 80 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 80 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283392  0.017560\n",
      "621283320  0.019725\n",
      "621283531  0.020065\n",
      "621283330  0.020146\n",
      "621283328  0.020186\n",
      "621283337  0.020286\n",
      "621283324  0.020646\n",
      "621283332  0.020987\n",
      "621283335  0.021048\n",
      "621283388  0.021088\n",
      "621283321  0.021143\n",
      "621283244  0.021168\n",
      "621283331  0.021368\n",
      "621283363  0.021368\n",
      "621283323  0.021413\n",
      "621283246  0.021428\n",
      "621283396  0.021428\n",
      "621283617  0.021529\n",
      "621283395  0.021769\n",
      "621283364  0.021869\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "68547  0.024395\n",
      "13759  0.024936\n",
      "13831  0.025091\n",
      "67856  0.025298\n",
      "66916  0.025719\n",
      "13856  0.025758\n",
      "13864  0.025838\n",
      "13791  0.025859\n",
      "13844  0.025879\n",
      "67951  0.025939\n",
      "13849  0.025999\n",
      "13770  0.026119\n",
      "66951  0.026160\n",
      "67825  0.026561\n",
      "13769  0.026661\n",
      "13843  0.026776\n",
      "67261  0.026780\n",
      "66853  0.026801\n",
      "67972  0.026820\n",
      "67863  0.026821\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283320  0.015209\n",
      "621283531  0.015836\n",
      "621283323  0.016575\n",
      "621283321  0.016579\n",
      "621283388  0.016606\n",
      "621283328  0.017061\n",
      "621283331  0.017315\n",
      "621283395  0.017488\n",
      "621283330  0.017538\n",
      "621283324  0.017660\n",
      "621283335  0.017702\n",
      "621283363  0.017740\n",
      "621283364  0.017873\n",
      "621283332  0.018145\n",
      "621283244  0.018355\n",
      "621283392  0.018386\n",
      "621283396  0.018985\n",
      "621283337  0.019099\n",
      "621283246  0.019699\n",
      "621283617  0.019757\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67951  0.021990\n",
      "13864  0.021994\n",
      "13844  0.022451\n",
      "13843  0.022502\n",
      "13759  0.022533\n",
      "67825  0.022599\n",
      "13849  0.022623\n",
      "13856  0.022631\n",
      "13770  0.022722\n",
      "13791  0.023151\n",
      "67863  0.023311\n",
      "13769  0.023361\n",
      "13831  0.023516\n",
      "67856  0.023990\n",
      "67972  0.024429\n",
      "66951  0.025033\n",
      "67261  0.026061\n",
      "66853  0.026284\n",
      "66916  0.027110\n",
      "68547  0.033961\n",
      ">>> Fastest Solution is rocblas 621283320 0.015208500623703002\n",
      "M N K dtype 768 72 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 768 72 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.019724\n",
      "621283530  0.020527\n",
      "621283191  0.021108\n",
      "621283247  0.021208\n",
      "621286476  0.021288\n",
      "621283328  0.021328\n",
      "621283322  0.021429\n",
      "621283248  0.021468\n",
      "621286473  0.021469\n",
      "621283388  0.021609\n",
      "621283319  0.021629\n",
      "621283392  0.021649\n",
      "621283323  0.021669\n",
      "621283320  0.021669\n",
      "621286474  0.021709\n",
      "621283188  0.021809\n",
      "621283189  0.021830\n",
      "621283334  0.021830\n",
      "621283330  0.021970\n",
      "621283325  0.021990\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13850  0.023434\n",
      "67746  0.024135\n",
      "13845  0.024395\n",
      "13883  0.024516\n",
      "13896  0.024556\n",
      "13790  0.024636\n",
      "67808  0.024656\n",
      "67865  0.024736\n",
      "13897  0.025037\n",
      "13859  0.025116\n",
      "13884  0.025137\n",
      "67837  0.025197\n",
      "67764  0.025277\n",
      "13779  0.025317\n",
      "67895  0.025317\n",
      "13892  0.025598\n",
      "66939  0.025719\n",
      "13893  0.025819\n",
      "13915  0.025838\n",
      "67757  0.025898\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283322  0.015806\n",
      "621283334  0.016191\n",
      "621286473  0.016431\n",
      "621283189  0.016542\n",
      "621283323  0.016546\n",
      "621286474  0.016610\n",
      "621283191  0.016752\n",
      "621283319  0.016826\n",
      "621286476  0.017049\n",
      "621283392  0.017183\n",
      "621283188  0.017321\n",
      "621283320  0.017935\n",
      "621283248  0.018111\n",
      "621283530  0.018257\n",
      "621283247  0.018520\n",
      "621283325  0.018783\n",
      "621283328  0.018983\n",
      "621283388  0.019161\n",
      "621283330  0.019372\n",
      "621283393  0.025708\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13897  0.020208\n",
      "13883  0.020496\n",
      "13896  0.020791\n",
      "67757  0.020799\n",
      "13884  0.020903\n",
      "67808  0.020957\n",
      "67837  0.021062\n",
      "13845  0.021248\n",
      "13859  0.021737\n",
      "67895  0.021781\n",
      "13779  0.021848\n",
      "13790  0.022002\n",
      "13915  0.022010\n",
      "13893  0.022395\n",
      "67865  0.022415\n",
      "13892  0.022545\n",
      "66939  0.022870\n",
      "67746  0.027033\n",
      "13850  0.029796\n",
      "67764  0.030772\n",
      ">>> Fastest Solution is rocblas 621283322 0.015805849432945253\n",
      "M N K dtype 4096 72 512 torch.bfloat16 >>> Total rocb solutions 444\n",
      "M N K bias dtype 4096 72 512 False torch.bfloat16 >>> Total hipb solutions 1636\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283378  0.017880\n",
      "621283379  0.018823\n",
      "621283388  0.019023\n",
      "621283412  0.019143\n",
      "621283418  0.019264\n",
      "621283620  0.019444\n",
      "621283417  0.019464\n",
      "621283469  0.019524\n",
      "621283380  0.019705\n",
      "621283370  0.019745\n",
      "621283372  0.019785\n",
      "621283259  0.020046\n",
      "621283564  0.020126\n",
      "621283410  0.020186\n",
      "621283539  0.020206\n",
      "621283416  0.020366\n",
      "621283323  0.020487\n",
      "621283240  0.020506\n",
      "621283338  0.020546\n",
      "621283404  0.020607\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67544  0.023072\n",
      "68668  0.023413\n",
      "13870  0.023834\n",
      "67280  0.023854\n",
      "67156  0.024175\n",
      "13862  0.024295\n",
      "68873  0.024335\n",
      "67566  0.024556\n",
      "67602  0.024596\n",
      "13772  0.024696\n",
      "13866  0.024817\n",
      "66932  0.024876\n",
      "67244  0.024957\n",
      "66923  0.024957\n",
      "67213  0.024997\n",
      "67527  0.025237\n",
      "13779  0.025237\n",
      "67361  0.025277\n",
      "13756  0.025317\n",
      "67104  0.025357\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283418  0.016265\n",
      "621283620  0.016337\n",
      "621283410  0.016361\n",
      "621283380  0.016387\n",
      "621283564  0.016499\n",
      "621283417  0.016634\n",
      "621283469  0.016682\n",
      "621283416  0.016770\n",
      "621283372  0.017195\n",
      "621283388  0.017195\n",
      "621283412  0.017213\n",
      "621283323  0.017317\n",
      "621283539  0.017417\n",
      "621283370  0.017462\n",
      "621283338  0.017558\n",
      "621283404  0.017694\n",
      "621283240  0.017708\n",
      "621283259  0.017754\n",
      "621283379  0.020041\n",
      "621283378  0.021555\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67602  0.021254\n",
      "67244  0.021427\n",
      "67213  0.021647\n",
      "13866  0.021709\n",
      "13870  0.021771\n",
      "67566  0.021811\n",
      "67527  0.021837\n",
      "66923  0.021884\n",
      "13862  0.022569\n",
      "66932  0.022597\n",
      "13756  0.022659\n",
      "67156  0.022671\n",
      "67361  0.022709\n",
      "67104  0.022786\n",
      "67280  0.022858\n",
      "13779  0.023048\n",
      "68873  0.023096\n",
      "13772  0.023413\n",
      "68668  0.023449\n",
      "67544  0.029316\n",
      ">>> Fastest Solution is rocblas 621283418 0.0162649005651474\n",
      "M N K dtype 3584 72 4096 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 3584 72 4096 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283321  0.019715\n",
      "621283323  0.022651\n",
      "621283189  0.022867\n",
      "621283191  0.023223\n",
      "621283322  0.023664\n",
      "621286476  0.024050\n",
      "621283334  0.024105\n",
      "621283395  0.024155\n",
      "621283190  0.024505\n",
      "621286474  0.025202\n",
      "621286475  0.025934\n",
      "621283328  0.026160\n",
      "621283531  0.026400\n",
      "621283388  0.026540\n",
      "621283325  0.026560\n",
      "621283332  0.026561\n",
      "621283327  0.026841\n",
      "621283530  0.026901\n",
      "621283330  0.026901\n",
      "621283329  0.026961\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13897  0.024035\n",
      "67757  0.024175\n",
      "13850  0.024616\n",
      "13842  0.025393\n",
      "13896  0.025779\n",
      "13886  0.025979\n",
      "67808  0.026475\n",
      "13907  0.026641\n",
      "13883  0.027002\n",
      "13924  0.027292\n",
      "13852  0.027302\n",
      "13884  0.027542\n",
      "13740  0.027593\n",
      "13904  0.027753\n",
      "67797  0.027843\n",
      "67865  0.027864\n",
      "13847  0.027878\n",
      "13917  0.027983\n",
      "13841  0.028009\n",
      "13905  0.028235\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283189  0.017217\n",
      "621286476  0.017612\n",
      "621286474  0.018508\n",
      "621283334  0.018666\n",
      "621283191  0.018869\n",
      "621283323  0.018873\n",
      "621283190  0.019116\n",
      "621283322  0.020157\n",
      "621286475  0.020308\n",
      "621283321  0.020402\n",
      "621283325  0.023858\n",
      "621283332  0.023916\n",
      "621283327  0.024043\n",
      "621283328  0.024299\n",
      "621283388  0.024506\n",
      "621283330  0.024514\n",
      "621283329  0.024622\n",
      "621283531  0.024638\n",
      "621283395  0.025291\n",
      "621283530  0.025987\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13884  0.023191\n",
      "13905  0.023436\n",
      "13917  0.023463\n",
      "13852  0.023628\n",
      "13896  0.023698\n",
      "13850  0.023834\n",
      "13924  0.024126\n",
      "13883  0.024349\n",
      "13904  0.024491\n",
      "13907  0.025353\n",
      "13847  0.025492\n",
      "67808  0.025800\n",
      "13886  0.025913\n",
      "67757  0.026159\n",
      "13841  0.026353\n",
      "13842  0.026461\n",
      "13740  0.026767\n",
      "67797  0.026914\n",
      "67865  0.028142\n",
      "13897  0.030840\n",
      ">>> Fastest Solution is rocblas 621283189 0.01721705049276352\n",
      "M N K dtype 4096 72 1792 torch.bfloat16 >>> Total rocb solutions 446\n",
      "M N K bias dtype 4096 72 1792 False torch.bfloat16 >>> Total hipb solutions 2242\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283363  0.019164\n",
      "621283238  0.020467\n",
      "621283246  0.020853\n",
      "621283532  0.020887\n",
      "621283328  0.020947\n",
      "621286475  0.020947\n",
      "621283394  0.020987\n",
      "621286476  0.021008\n",
      "621283530  0.021042\n",
      "621283339  0.021068\n",
      "621283330  0.021108\n",
      "621283335  0.021128\n",
      "621283324  0.021128\n",
      "621283334  0.021128\n",
      "621283189  0.021148\n",
      "621283327  0.021249\n",
      "621283531  0.021589\n",
      "621283341  0.021649\n",
      "621283617  0.021649\n",
      "621283533  0.021729\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67694  0.024079\n",
      "66983  0.024857\n",
      "13791  0.025097\n",
      "67825  0.025298\n",
      "67692  0.025698\n",
      "13739  0.025999\n",
      "66842  0.025999\n",
      "13920  0.026024\n",
      "13875  0.026079\n",
      "13755  0.026239\n",
      "13843  0.026324\n",
      "13864  0.026460\n",
      "13885  0.026540\n",
      "67661  0.026550\n",
      "13873  0.026620\n",
      "67903  0.026641\n",
      "13740  0.026771\n",
      "67817  0.026801\n",
      "13849  0.026901\n",
      "67642  0.026951\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283330  0.016702\n",
      "621283246  0.016852\n",
      "621283324  0.017055\n",
      "621283328  0.017119\n",
      "621283334  0.017284\n",
      "621286476  0.017318\n",
      "621283327  0.017319\n",
      "621286475  0.017388\n",
      "621283530  0.017507\n",
      "621283189  0.017622\n",
      "621283335  0.017666\n",
      "621283531  0.017710\n",
      "621283394  0.018474\n",
      "621283341  0.018492\n",
      "621283339  0.018542\n",
      "621283532  0.018743\n",
      "621283617  0.019534\n",
      "621283533  0.019753\n",
      "621283238  0.024782\n",
      "621283363  0.025506\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13843  0.022196\n",
      "13864  0.022234\n",
      "13849  0.022246\n",
      "13875  0.022317\n",
      "67903  0.022599\n",
      "13873  0.022631\n",
      "67825  0.022700\n",
      "13791  0.022784\n",
      "13920  0.023287\n",
      "67817  0.023433\n",
      "13739  0.023447\n",
      "13755  0.023570\n",
      "67694  0.023736\n",
      "13740  0.023799\n",
      "67692  0.024021\n",
      "67642  0.024284\n",
      "67661  0.025375\n",
      "66842  0.026276\n",
      "13885  0.032233\n",
      "66983  0.032911\n",
      ">>> Fastest Solution is rocblas 621283330 0.016701899468898773\n",
      "M N K dtype 768 64 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 768 64 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283307  0.019304\n",
      "621283393  0.019504\n",
      "621283223  0.020045\n",
      "621283311  0.020527\n",
      "621283220  0.020527\n",
      "621283218  0.020567\n",
      "621283225  0.020586\n",
      "621283224  0.020606\n",
      "621283227  0.020627\n",
      "621283226  0.020667\n",
      "621283320  0.020687\n",
      "621283322  0.020727\n",
      "621283305  0.020787\n",
      "621283189  0.020787\n",
      "621283191  0.020787\n",
      "621283317  0.020827\n",
      "621283244  0.020847\n",
      "621283248  0.020868\n",
      "621283308  0.020908\n",
      "621283222  0.020927\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67837  0.024015\n",
      "13897  0.024094\n",
      "13896  0.024155\n",
      "67746  0.024596\n",
      "13883  0.024676\n",
      "67865  0.025056\n",
      "67757  0.025097\n",
      "13845  0.025177\n",
      "13884  0.025298\n",
      "13894  0.025378\n",
      "13735  0.025398\n",
      "13916  0.025438\n",
      "13855  0.025498\n",
      "13738  0.025538\n",
      "13886  0.025739\n",
      "13752  0.025758\n",
      "13915  0.025858\n",
      "13846  0.025959\n",
      "13764  0.025999\n",
      "13893  0.026039\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283311  0.015878\n",
      "621283227  0.015920\n",
      "621283244  0.016034\n",
      "621283224  0.016616\n",
      "621283223  0.016698\n",
      "621283322  0.016708\n",
      "621283226  0.016764\n",
      "621283191  0.016846\n",
      "621283248  0.016946\n",
      "621283220  0.017053\n",
      "621283225  0.017103\n",
      "621283320  0.017125\n",
      "621283189  0.017125\n",
      "621283317  0.017287\n",
      "621283218  0.017319\n",
      "621283308  0.017680\n",
      "621283222  0.018009\n",
      "621283305  0.018231\n",
      "621283393  0.022487\n",
      "621283307  0.025520\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13845  0.020250\n",
      "67757  0.020773\n",
      "13883  0.021004\n",
      "13884  0.021038\n",
      "13764  0.021302\n",
      "13738  0.021407\n",
      "13896  0.021577\n",
      "13855  0.021691\n",
      "13893  0.021761\n",
      "13916  0.021815\n",
      "67746  0.021848\n",
      "13735  0.021970\n",
      "13846  0.022048\n",
      "67865  0.022052\n",
      "13886  0.022291\n",
      "13894  0.022367\n",
      "13915  0.022391\n",
      "13752  0.023854\n",
      "13897  0.025386\n",
      "67837  0.030884\n",
      ">>> Fastest Solution is rocblas 621283311 0.015877999365329742\n",
      "M N K dtype 4096 64 512 torch.bfloat16 >>> Total rocb solutions 488\n",
      "M N K bias dtype 4096 64 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283379  0.019063\n",
      "621283378  0.019124\n",
      "621283540  0.019183\n",
      "621283418  0.019384\n",
      "621283564  0.019464\n",
      "621283410  0.019484\n",
      "621283411  0.019484\n",
      "621283340  0.019484\n",
      "621283417  0.019684\n",
      "621283539  0.019705\n",
      "621283620  0.019785\n",
      "621283413  0.019965\n",
      "621283361  0.020025\n",
      "621283392  0.020025\n",
      "621283333  0.020046\n",
      "621283416  0.020046\n",
      "621283366  0.020065\n",
      "621283240  0.020126\n",
      "621283236  0.020146\n",
      "621283189  0.020165\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13772  0.023153\n",
      "67280  0.023313\n",
      "66923  0.023553\n",
      "67086  0.024054\n",
      "66839  0.024054\n",
      "66862  0.024095\n",
      "67544  0.024175\n",
      "67527  0.024175\n",
      "67566  0.024295\n",
      "67361  0.024315\n",
      "13925  0.024495\n",
      "67037  0.024575\n",
      "13878  0.024576\n",
      "68378  0.024596\n",
      "66849  0.024716\n",
      "13906  0.024816\n",
      "67602  0.024816\n",
      "67213  0.024816\n",
      "67410  0.024857\n",
      "66879  0.024937\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283418  0.016086\n",
      "621283540  0.016117\n",
      "621283539  0.016247\n",
      "621283620  0.016265\n",
      "621283411  0.016331\n",
      "621283410  0.016425\n",
      "621283413  0.016562\n",
      "621283417  0.016820\n",
      "621283366  0.016973\n",
      "621283361  0.017117\n",
      "621283340  0.017173\n",
      "621283333  0.017179\n",
      "621283240  0.017245\n",
      "621283564  0.017357\n",
      "621283236  0.017403\n",
      "621283189  0.017446\n",
      "621283392  0.017464\n",
      "621283416  0.018159\n",
      "621283378  0.019103\n",
      "621283379  0.025247\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13925  0.020555\n",
      "13906  0.020777\n",
      "67566  0.020783\n",
      "67602  0.021162\n",
      "67213  0.021597\n",
      "66879  0.021976\n",
      "13878  0.022118\n",
      "67361  0.022377\n",
      "66923  0.022479\n",
      "68378  0.022485\n",
      "67410  0.022517\n",
      "67086  0.022734\n",
      "67527  0.022888\n",
      "66849  0.022960\n",
      "67544  0.023253\n",
      "67037  0.023381\n",
      "66839  0.023587\n",
      "66862  0.023626\n",
      "67280  0.023782\n",
      "13772  0.027959\n",
      ">>> Fastest Solution is rocblas 621283418 0.016086450219154357\n",
      "M N K dtype 3584 64 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 3584 64 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283313  0.020927\n",
      "621283530  0.021148\n",
      "621286473  0.021694\n",
      "621283189  0.021760\n",
      "621283212  0.021850\n",
      "621283224  0.022080\n",
      "621283310  0.022095\n",
      "621283392  0.022331\n",
      "621283213  0.022496\n",
      "621283190  0.022842\n",
      "621283227  0.022932\n",
      "621283219  0.023002\n",
      "621283393  0.023293\n",
      "621283226  0.023413\n",
      "621286472  0.023864\n",
      "621283218  0.023869\n",
      "621283323  0.023935\n",
      "621283305  0.023954\n",
      "621286474  0.024024\n",
      "621283316  0.024035\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13845  0.023714\n",
      "13850  0.025298\n",
      "67694  0.025794\n",
      "13740  0.025884\n",
      "13886  0.025889\n",
      "13847  0.026265\n",
      "13841  0.026360\n",
      "13842  0.026405\n",
      "67777  0.026485\n",
      "13907  0.026580\n",
      "13893  0.026646\n",
      "13765  0.026845\n",
      "67738  0.027142\n",
      "13855  0.027207\n",
      "67757  0.027282\n",
      "67746  0.027643\n",
      "13788  0.027683\n",
      "67837  0.027743\n",
      "13917  0.027743\n",
      "13894  0.027903\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621286474  0.016786\n",
      "621283190  0.017130\n",
      "621283189  0.017520\n",
      "621283310  0.017688\n",
      "621283305  0.018722\n",
      "621283316  0.018787\n",
      "621283323  0.019474\n",
      "621283313  0.020256\n",
      "621283213  0.020271\n",
      "621283219  0.020573\n",
      "621283530  0.020591\n",
      "621283392  0.020717\n",
      "621283224  0.020908\n",
      "621286472  0.021095\n",
      "621283212  0.021488\n",
      "621283226  0.022281\n",
      "621283227  0.022339\n",
      "621286473  0.022456\n",
      "621283393  0.022705\n",
      "621283218  0.022916\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13894  0.021860\n",
      "13893  0.021971\n",
      "13917  0.022275\n",
      "13855  0.022549\n",
      "13886  0.022746\n",
      "67837  0.024209\n",
      "67777  0.024453\n",
      "13907  0.024673\n",
      "67757  0.024764\n",
      "67738  0.024772\n",
      "13850  0.024900\n",
      "13841  0.025140\n",
      "13842  0.025189\n",
      "13788  0.025301\n",
      "67694  0.025370\n",
      "13847  0.025750\n",
      "13765  0.026492\n",
      "13740  0.026640\n",
      "67746  0.028398\n",
      "13845  0.029262\n",
      ">>> Fastest Solution is rocblas 621286474 0.016785599291324615\n",
      "M N K dtype 4096 64 1792 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 4096 64 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283247  0.018963\n",
      "621283362  0.019785\n",
      "621283249  0.019805\n",
      "621283222  0.020206\n",
      "621283399  0.020427\n",
      "621283336  0.020427\n",
      "621283334  0.020446\n",
      "621283248  0.020467\n",
      "621283330  0.020527\n",
      "621283303  0.020567\n",
      "621283189  0.020747\n",
      "621283225  0.020787\n",
      "621283305  0.020868\n",
      "621283395  0.020868\n",
      "621283223  0.020887\n",
      "621283257  0.020927\n",
      "621283393  0.020947\n",
      "621283306  0.020968\n",
      "621283338  0.020987\n",
      "621283220  0.020988\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "66910  0.024175\n",
      "13847  0.024255\n",
      "67692  0.024596\n",
      "13927  0.025057\n",
      "13873  0.025317\n",
      "13730  0.025398\n",
      "13841  0.025403\n",
      "67951  0.025538\n",
      "66916  0.025739\n",
      "13831  0.025819\n",
      "13728  0.025898\n",
      "13863  0.025919\n",
      "13857  0.026019\n",
      "67504  0.026340\n",
      "13906  0.026400\n",
      "13895  0.026420\n",
      "13876  0.026540\n",
      "67817  0.026680\n",
      "67863  0.026760\n",
      "67825  0.026861\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283330  0.016682\n",
      "621283305  0.016832\n",
      "621283334  0.016906\n",
      "621283223  0.016948\n",
      "621283225  0.017083\n",
      "621283222  0.017087\n",
      "621283306  0.017287\n",
      "621283257  0.017426\n",
      "621283395  0.017508\n",
      "621283189  0.017560\n",
      "621283220  0.017578\n",
      "621283303  0.017649\n",
      "621283393  0.017738\n",
      "621283249  0.017740\n",
      "621283248  0.017856\n",
      "621283338  0.018502\n",
      "621283399  0.018600\n",
      "621283362  0.019829\n",
      "621283247  0.021024\n",
      "621283336  0.022238\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13876  0.021437\n",
      "13728  0.021922\n",
      "67951  0.022012\n",
      "13831  0.022068\n",
      "13895  0.022154\n",
      "67825  0.022373\n",
      "13873  0.022379\n",
      "13841  0.022723\n",
      "13863  0.022944\n",
      "67817  0.023058\n",
      "67504  0.023078\n",
      "13730  0.023092\n",
      "13927  0.023124\n",
      "67863  0.023169\n",
      "13906  0.023491\n",
      "13847  0.023728\n",
      "67692  0.023788\n",
      "13857  0.024756\n",
      "66916  0.027304\n",
      "66910  0.034310\n",
      ">>> Fastest Solution is rocblas 621283330 0.01668184995651245\n",
      "M N K dtype 768 56 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 768 56 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283393  0.019063\n",
      "621283307  0.019143\n",
      "621283226  0.019505\n",
      "621283316  0.019724\n",
      "621283227  0.019805\n",
      "621283244  0.019825\n",
      "621283530  0.020186\n",
      "621283245  0.020226\n",
      "621283303  0.020586\n",
      "621283249  0.020607\n",
      "621283319  0.020667\n",
      "621283213  0.020727\n",
      "621283314  0.020747\n",
      "621283306  0.020927\n",
      "621283246  0.020987\n",
      "621283191  0.021008\n",
      "621283304  0.021087\n",
      "621283305  0.021088\n",
      "621283190  0.021088\n",
      "621283219  0.021228\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "68104  0.021208\n",
      "68064  0.022411\n",
      "66972  0.022551\n",
      "68760  0.023193\n",
      "13897  0.023293\n",
      "68747  0.023353\n",
      "67140  0.023413\n",
      "67746  0.023573\n",
      "67902  0.023714\n",
      "13852  0.023734\n",
      "13883  0.023754\n",
      "68465  0.023874\n",
      "67064  0.023935\n",
      "67808  0.024215\n",
      "13884  0.024235\n",
      "13921  0.024255\n",
      "67865  0.024295\n",
      "13896  0.024335\n",
      "67678  0.024395\n",
      "67710  0.024395\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283244  0.015613\n",
      "621283530  0.015645\n",
      "621283245  0.016251\n",
      "621283246  0.016668\n",
      "621283316  0.016700\n",
      "621283249  0.016718\n",
      "621283190  0.016732\n",
      "621283227  0.016742\n",
      "621283305  0.016776\n",
      "621283213  0.016970\n",
      "621283191  0.016985\n",
      "621283303  0.017029\n",
      "621283314  0.017165\n",
      "621283219  0.017293\n",
      "621283319  0.017540\n",
      "621283304  0.017552\n",
      "621283306  0.017712\n",
      "621283226  0.018999\n",
      "621283307  0.021489\n",
      "621283393  0.026059\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13883  0.020334\n",
      "13852  0.020430\n",
      "13884  0.020665\n",
      "13896  0.020917\n",
      "67808  0.021254\n",
      "67746  0.021441\n",
      "67865  0.021978\n",
      "13897  0.022062\n",
      "67678  0.022090\n",
      "68104  0.022148\n",
      "13921  0.022768\n",
      "67064  0.029453\n",
      "68465  0.029541\n",
      "67710  0.029824\n",
      "67140  0.030385\n",
      "68760  0.030746\n",
      "67902  0.030814\n",
      "68064  0.031255\n",
      "66972  0.031684\n",
      "68747  0.032283\n",
      ">>> Fastest Solution is rocblas 621283244 0.01561339944601059\n",
      "M N K dtype 4096 56 512 torch.bfloat16 >>> Total rocb solutions 488\n",
      "M N K bias dtype 4096 56 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283532  0.017420\n",
      "621283371  0.018261\n",
      "621283540  0.018522\n",
      "621283417  0.018702\n",
      "621283620  0.018742\n",
      "621283656  0.018823\n",
      "621283418  0.018963\n",
      "621283369  0.019063\n",
      "621283564  0.019264\n",
      "621283410  0.019304\n",
      "621283240  0.019444\n",
      "621283330  0.019544\n",
      "621283416  0.019885\n",
      "621283335  0.020025\n",
      "621283415  0.020025\n",
      "621283379  0.020065\n",
      "621283542  0.020126\n",
      "621283414  0.020146\n",
      "621283412  0.020165\n",
      "621283220  0.020226\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13861  0.022290\n",
      "13916  0.023012\n",
      "13911  0.023053\n",
      "13870  0.023393\n",
      "66941  0.023874\n",
      "67544  0.023954\n",
      "67086  0.024054\n",
      "67916  0.024195\n",
      "67566  0.024275\n",
      "68057  0.024395\n",
      "67602  0.024416\n",
      "13903  0.024476\n",
      "67877  0.024556\n",
      "13772  0.024576\n",
      "67162  0.024596\n",
      "66864  0.024616\n",
      "66879  0.024636\n",
      "68893  0.024656\n",
      "68299  0.024796\n",
      "13925  0.024796\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283532  0.012156\n",
      "621283379  0.015826\n",
      "621283410  0.016062\n",
      "621283418  0.016313\n",
      "621283240  0.016728\n",
      "621283656  0.016987\n",
      "621283542  0.017003\n",
      "621283620  0.017029\n",
      "621283335  0.017053\n",
      "621283416  0.017077\n",
      "621283369  0.017091\n",
      "621283540  0.017109\n",
      "621283220  0.017269\n",
      "621283414  0.017315\n",
      "621283412  0.017494\n",
      "621283330  0.017498\n",
      "621283415  0.017536\n",
      "621283564  0.018265\n",
      "621283417  0.018390\n",
      "621283371  0.022589\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13925  0.020737\n",
      "67566  0.021272\n",
      "67162  0.021383\n",
      "13870  0.021647\n",
      "13911  0.021723\n",
      "67602  0.022162\n",
      "67086  0.022178\n",
      "13903  0.022206\n",
      "13916  0.022325\n",
      "67916  0.022689\n",
      "66864  0.022754\n",
      "68057  0.022838\n",
      "67877  0.022920\n",
      "68893  0.022998\n",
      "68299  0.023052\n",
      "66879  0.023175\n",
      "67544  0.023483\n",
      "66941  0.023608\n",
      "13772  0.023854\n",
      "13861  0.025660\n",
      ">>> Fastest Solution is rocblas 621283532 0.012155549973249436\n",
      "M N K dtype 3584 56 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 3584 56 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283334  0.019905\n",
      "621283393  0.020647\n",
      "621286476  0.020917\n",
      "621283224  0.021138\n",
      "621283188  0.021544\n",
      "621283302  0.021769\n",
      "621283530  0.021870\n",
      "621283392  0.022050\n",
      "621283191  0.022496\n",
      "621283213  0.022561\n",
      "621286472  0.022601\n",
      "621283304  0.022806\n",
      "621283305  0.023122\n",
      "621283245  0.023153\n",
      "621283221  0.023162\n",
      "621283320  0.023212\n",
      "621283322  0.023338\n",
      "621283218  0.023548\n",
      "621283219  0.023604\n",
      "621286473  0.023734\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13917  0.025679\n",
      "13848  0.025809\n",
      "67757  0.025979\n",
      "13845  0.026059\n",
      "13892  0.026245\n",
      "67694  0.026305\n",
      "13765  0.026530\n",
      "13894  0.027006\n",
      "13920  0.027101\n",
      "13858  0.027157\n",
      "67837  0.027282\n",
      "13788  0.027302\n",
      "67746  0.027322\n",
      "13842  0.027362\n",
      "13926  0.027392\n",
      "13850  0.027422\n",
      "13886  0.027483\n",
      "13855  0.027517\n",
      "13786  0.027583\n",
      "13830  0.027728\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621286476  0.016427\n",
      "621283322  0.018219\n",
      "621283305  0.018629\n",
      "621283191  0.019218\n",
      "621283224  0.019484\n",
      "621283320  0.019494\n",
      "621283221  0.019587\n",
      "621283304  0.019820\n",
      "621286472  0.019996\n",
      "621283530  0.020175\n",
      "621283245  0.020288\n",
      "621283213  0.020739\n",
      "621283392  0.020838\n",
      "621283302  0.021303\n",
      "621283334  0.021639\n",
      "621283219  0.021678\n",
      "621283188  0.022574\n",
      "621283218  0.022670\n",
      "621286473  0.022774\n",
      "621283393  0.024953\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13848  0.020462\n",
      "13886  0.021900\n",
      "13894  0.022095\n",
      "13892  0.022303\n",
      "13855  0.022729\n",
      "13850  0.022896\n",
      "13845  0.023253\n",
      "13788  0.023854\n",
      "13926  0.023861\n",
      "13920  0.023946\n",
      "13830  0.024010\n",
      "13786  0.024353\n",
      "67837  0.024666\n",
      "13765  0.024937\n",
      "67757  0.024953\n",
      "67694  0.025418\n",
      "13842  0.026414\n",
      "13858  0.026614\n",
      "67746  0.027400\n",
      "13917  0.030603\n",
      ">>> Fastest Solution is rocblas 621286476 0.01642725020647049\n",
      "M N K dtype 4096 56 1792 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 4096 56 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283359  0.018221\n",
      "621283189  0.018742\n",
      "621283365  0.019524\n",
      "621283363  0.019544\n",
      "621283362  0.019825\n",
      "621283314  0.019845\n",
      "621283322  0.020286\n",
      "621283245  0.020306\n",
      "621283229  0.020326\n",
      "621283323  0.020446\n",
      "621283391  0.020567\n",
      "621283530  0.020607\n",
      "621283312  0.020607\n",
      "621283234  0.020647\n",
      "621283531  0.020687\n",
      "621283316  0.020747\n",
      "621283306  0.020747\n",
      "621286475  0.020807\n",
      "621283227  0.020807\n",
      "621283317  0.020847\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13882  0.023653\n",
      "13728  0.024636\n",
      "67951  0.024756\n",
      "13847  0.024836\n",
      "13864  0.025056\n",
      "13858  0.025257\n",
      "13844  0.025337\n",
      "13841  0.025347\n",
      "13849  0.025398\n",
      "67661  0.025593\n",
      "13873  0.025638\n",
      "66828  0.025758\n",
      "13925  0.025879\n",
      "13857  0.025899\n",
      "67833  0.025979\n",
      "13906  0.026019\n",
      "13860  0.026039\n",
      "13876  0.026060\n",
      "66939  0.026199\n",
      "13759  0.026300\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283314  0.016425\n",
      "621283189  0.016658\n",
      "621283362  0.016690\n",
      "621283229  0.016702\n",
      "621286475  0.016842\n",
      "621283245  0.016846\n",
      "621283322  0.016900\n",
      "621283227  0.016962\n",
      "621283317  0.017035\n",
      "621283530  0.017053\n",
      "621283306  0.017097\n",
      "621283316  0.017245\n",
      "621283323  0.017259\n",
      "621283363  0.017401\n",
      "621283531  0.017522\n",
      "621283312  0.017688\n",
      "621283391  0.018420\n",
      "621283365  0.019262\n",
      "621283234  0.019983\n",
      "621283359  0.021170\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13858  0.021824\n",
      "13844  0.022008\n",
      "13873  0.022042\n",
      "13860  0.022276\n",
      "67951  0.022371\n",
      "13876  0.022429\n",
      "13864  0.022790\n",
      "67833  0.022814\n",
      "13841  0.022917\n",
      "13849  0.022942\n",
      "13906  0.023092\n",
      "13925  0.023148\n",
      "13759  0.023387\n",
      "67661  0.023408\n",
      "66939  0.023658\n",
      "13847  0.023672\n",
      "66828  0.024572\n",
      "13857  0.024720\n",
      "13728  0.025091\n",
      "13882  0.028990\n",
      ">>> Fastest Solution is rocblas 621283314 0.016425250470638274\n",
      "M N K dtype 768 48 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 768 48 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283307  0.019324\n",
      "621283530  0.019504\n",
      "621283226  0.020105\n",
      "621283393  0.020105\n",
      "621283311  0.020427\n",
      "621283392  0.020586\n",
      "621283218  0.020607\n",
      "621283308  0.020647\n",
      "621283224  0.020747\n",
      "621283189  0.020847\n",
      "621283303  0.020908\n",
      "621283322  0.020968\n",
      "621283313  0.020968\n",
      "621283321  0.020968\n",
      "621283310  0.021027\n",
      "621283319  0.021048\n",
      "621283191  0.021208\n",
      "621283228  0.021249\n",
      "621286473  0.021308\n",
      "621283355  0.021308\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67064  0.021228\n",
      "67141  0.021428\n",
      "67036  0.022130\n",
      "67753  0.022551\n",
      "13850  0.023674\n",
      "13883  0.023774\n",
      "13897  0.024134\n",
      "13852  0.024415\n",
      "67808  0.024596\n",
      "13845  0.024676\n",
      "13884  0.024676\n",
      "13896  0.024776\n",
      "67865  0.024957\n",
      "13790  0.024997\n",
      "13746  0.025037\n",
      "13779  0.025077\n",
      "67746  0.025317\n",
      "67041  0.025457\n",
      "13893  0.025478\n",
      "13886  0.025478\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283393  0.016359\n",
      "621283322  0.016654\n",
      "621283189  0.016778\n",
      "621283310  0.016954\n",
      "621283224  0.016984\n",
      "621283311  0.017145\n",
      "621283321  0.017219\n",
      "621283303  0.017225\n",
      "621283319  0.017239\n",
      "621286473  0.017408\n",
      "621283308  0.017418\n",
      "621283218  0.017440\n",
      "621283530  0.017582\n",
      "621283191  0.017632\n",
      "621283228  0.017746\n",
      "621283313  0.017784\n",
      "621283392  0.017989\n",
      "621283355  0.018504\n",
      "621283226  0.018562\n",
      "621283307  0.024872\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13845  0.020152\n",
      "13884  0.020282\n",
      "13852  0.020286\n",
      "13896  0.020300\n",
      "13883  0.020396\n",
      "13850  0.020585\n",
      "13897  0.020994\n",
      "67746  0.020994\n",
      "13779  0.021870\n",
      "13886  0.022042\n",
      "13746  0.022090\n",
      "13790  0.022100\n",
      "67808  0.022154\n",
      "13893  0.022222\n",
      "67865  0.022563\n",
      "67064  0.023634\n",
      "67041  0.030233\n",
      "67036  0.031862\n",
      "67141  0.032161\n",
      "67753  0.032746\n",
      ">>> Fastest Solution is rocblas 621283393 0.01635909974575043\n",
      "M N K dtype 4096 48 512 torch.bfloat16 >>> Total rocb solutions 488\n",
      "M N K bias dtype 4096 48 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283306  0.018582\n",
      "621283378  0.018662\n",
      "621283410  0.019083\n",
      "621283564  0.019284\n",
      "621283540  0.019404\n",
      "621283380  0.019404\n",
      "621283417  0.019424\n",
      "621283328  0.019444\n",
      "621283390  0.019885\n",
      "621283418  0.019946\n",
      "621283469  0.020025\n",
      "621283416  0.020025\n",
      "621283400  0.020046\n",
      "621283542  0.020065\n",
      "621283415  0.020146\n",
      "621283240  0.020306\n",
      "621283413  0.020366\n",
      "621283620  0.020487\n",
      "621283412  0.020527\n",
      "621283316  0.020587\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13870  0.023754\n",
      "68873  0.023854\n",
      "67505  0.023914\n",
      "13772  0.023994\n",
      "66868  0.024075\n",
      "66932  0.024075\n",
      "67602  0.024235\n",
      "67527  0.024255\n",
      "67566  0.024315\n",
      "67280  0.024616\n",
      "67170  0.024696\n",
      "68102  0.024776\n",
      "67213  0.024796\n",
      "66848  0.024857\n",
      "67361  0.024916\n",
      "67086  0.024976\n",
      "13903  0.024977\n",
      "67104  0.025157\n",
      "68299  0.025338\n",
      "13919  0.025357\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283306  0.007174\n",
      "621283418  0.015824\n",
      "621283240  0.016127\n",
      "621283620  0.016209\n",
      "621283469  0.016622\n",
      "621283328  0.016638\n",
      "621283417  0.016854\n",
      "621283412  0.016866\n",
      "621283416  0.016944\n",
      "621283380  0.016989\n",
      "621283542  0.017083\n",
      "621283415  0.017095\n",
      "621283564  0.017099\n",
      "621283390  0.017149\n",
      "621283316  0.017263\n",
      "621283410  0.017315\n",
      "621283413  0.018043\n",
      "621283400  0.018107\n",
      "621283540  0.018656\n",
      "621283378  0.019196\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67566  0.020723\n",
      "67602  0.021044\n",
      "13919  0.021064\n",
      "66868  0.021421\n",
      "67213  0.021503\n",
      "68102  0.021759\n",
      "68873  0.021769\n",
      "67086  0.021896\n",
      "13903  0.021950\n",
      "67280  0.022357\n",
      "67361  0.022435\n",
      "13772  0.022439\n",
      "68299  0.022599\n",
      "67527  0.022712\n",
      "67104  0.022728\n",
      "66848  0.022924\n",
      "66932  0.022994\n",
      "67170  0.023179\n",
      "67505  0.024173\n",
      "13870  0.029389\n",
      ">>> Fastest Solution is rocblas 621283306 0.007174299657344818\n",
      "M N K dtype 3584 48 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 3584 48 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283219  0.020607\n",
      "621283223  0.020908\n",
      "621283220  0.021379\n",
      "621283304  0.021489\n",
      "621283187  0.021774\n",
      "621283530  0.022165\n",
      "621283308  0.022416\n",
      "621286472  0.022527\n",
      "621283225  0.022631\n",
      "621283317  0.022701\n",
      "621283307  0.022752\n",
      "621283188  0.022797\n",
      "621283392  0.022887\n",
      "621286473  0.023122\n",
      "621283245  0.023172\n",
      "621286476  0.023193\n",
      "621283334  0.023213\n",
      "621283212  0.023338\n",
      "621283314  0.023342\n",
      "621283302  0.023423\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13848  0.025031\n",
      "13895  0.025387\n",
      "13892  0.025453\n",
      "13855  0.025558\n",
      "13893  0.025638\n",
      "13847  0.025749\n",
      "13845  0.025919\n",
      "13897  0.026199\n",
      "13778  0.026224\n",
      "13841  0.026279\n",
      "13894  0.026300\n",
      "67661  0.026390\n",
      "13896  0.026600\n",
      "13846  0.026661\n",
      "13755  0.026695\n",
      "13886  0.026731\n",
      "13852  0.026801\n",
      "13859  0.027342\n",
      "13740  0.027412\n",
      "13830  0.027642\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283225  0.016450\n",
      "621283304  0.016465\n",
      "621286476  0.017142\n",
      "621283220  0.017608\n",
      "621283308  0.017631\n",
      "621283187  0.018069\n",
      "621283334  0.018521\n",
      "621283317  0.018531\n",
      "621283223  0.018761\n",
      "621283314  0.018898\n",
      "621286472  0.019097\n",
      "621283530  0.019596\n",
      "621283212  0.019933\n",
      "621283245  0.020158\n",
      "621283392  0.020689\n",
      "621283307  0.020984\n",
      "621283302  0.021079\n",
      "621283188  0.021593\n",
      "621286473  0.021897\n",
      "621283219  0.022553\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13892  0.020974\n",
      "13859  0.021806\n",
      "13894  0.021884\n",
      "13893  0.021972\n",
      "13886  0.022102\n",
      "13855  0.022240\n",
      "13845  0.022623\n",
      "13778  0.022832\n",
      "67661  0.023378\n",
      "13755  0.023553\n",
      "13846  0.023834\n",
      "13830  0.023906\n",
      "13895  0.024191\n",
      "13896  0.024413\n",
      "13841  0.024493\n",
      "13852  0.024580\n",
      "13897  0.024682\n",
      "13847  0.025291\n",
      "13848  0.025524\n",
      "13740  0.027026\n",
      ">>> Fastest Solution is rocblas 621283225 0.016449800133705138\n",
      "M N K dtype 4096 48 1792 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 4096 48 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283359  0.019544\n",
      "621283305  0.019654\n",
      "621283362  0.019684\n",
      "621283397  0.019965\n",
      "621283333  0.020467\n",
      "621283334  0.020527\n",
      "621283304  0.020606\n",
      "621283320  0.020747\n",
      "621283222  0.020747\n",
      "621283213  0.020777\n",
      "621283312  0.020968\n",
      "621283245  0.020987\n",
      "621286475  0.021008\n",
      "621283223  0.021028\n",
      "621283323  0.021108\n",
      "621283337  0.021128\n",
      "621283340  0.021168\n",
      "621283363  0.021188\n",
      "621283330  0.021188\n",
      "621283338  0.021248\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13853  0.024395\n",
      "13847  0.024435\n",
      "67755  0.025598\n",
      "13857  0.025638\n",
      "67469  0.025658\n",
      "13851  0.025819\n",
      "13858  0.025838\n",
      "13849  0.025838\n",
      "13927  0.025879\n",
      "13925  0.026039\n",
      "13841  0.026160\n",
      "67825  0.026199\n",
      "67454  0.026741\n",
      "13730  0.026760\n",
      "13845  0.026780\n",
      "67903  0.026841\n",
      "13882  0.026881\n",
      "67592  0.026901\n",
      "67116  0.026901\n",
      "66939  0.026982\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283362  0.016281\n",
      "621283312  0.016409\n",
      "621283323  0.016556\n",
      "621283223  0.016586\n",
      "621283213  0.016686\n",
      "621286475  0.016784\n",
      "621283304  0.016828\n",
      "621283330  0.017009\n",
      "621283334  0.017011\n",
      "621283245  0.017323\n",
      "621283222  0.017331\n",
      "621283320  0.017375\n",
      "621283305  0.017422\n",
      "621283340  0.017512\n",
      "621283333  0.017544\n",
      "621283338  0.017953\n",
      "621283363  0.018059\n",
      "621283337  0.018135\n",
      "621283397  0.019097\n",
      "621283359  0.021519\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13858  0.020741\n",
      "13841  0.020923\n",
      "13853  0.021643\n",
      "67755  0.021779\n",
      "13882  0.022070\n",
      "13845  0.022096\n",
      "13925  0.022303\n",
      "67825  0.022391\n",
      "13851  0.022487\n",
      "13849  0.022519\n",
      "13927  0.022772\n",
      "13730  0.023094\n",
      "67116  0.023303\n",
      "67903  0.023309\n",
      "13847  0.023391\n",
      "66939  0.023894\n",
      "13857  0.024004\n",
      "67454  0.025871\n",
      "67592  0.028184\n",
      "67469  0.029669\n",
      ">>> Fastest Solution is rocblas 621283362 0.016280899941921233\n",
      "M N K dtype 768 40 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 768 40 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283227  0.018762\n",
      "621283307  0.018783\n",
      "621283530  0.018923\n",
      "621283393  0.019424\n",
      "621283311  0.019645\n",
      "621283226  0.019945\n",
      "621283191  0.020025\n",
      "621283322  0.020126\n",
      "621283334  0.020206\n",
      "621283306  0.020246\n",
      "621283225  0.020326\n",
      "621283312  0.020366\n",
      "621283187  0.020567\n",
      "621283320  0.020586\n",
      "621283316  0.020627\n",
      "621283304  0.020647\n",
      "621283310  0.020667\n",
      "621283228  0.020687\n",
      "621283190  0.020707\n",
      "621283246  0.020727\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67821  0.018963\n",
      "67854  0.019063\n",
      "66975  0.019384\n",
      "67893  0.020186\n",
      "68718  0.020406\n",
      "67807  0.020727\n",
      "67076  0.021249\n",
      "67022  0.021609\n",
      "68060  0.021809\n",
      "68402  0.021950\n",
      "67155  0.022070\n",
      "67675  0.022351\n",
      "13897  0.023072\n",
      "13852  0.023113\n",
      "13917  0.023232\n",
      "13850  0.023413\n",
      "13915  0.023493\n",
      "13896  0.023494\n",
      "13883  0.023634\n",
      "67848  0.023674\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283226  0.015954\n",
      "621283530  0.016315\n",
      "621283334  0.016562\n",
      "621283311  0.016714\n",
      "621283316  0.016722\n",
      "621283306  0.016744\n",
      "621283191  0.016800\n",
      "621283322  0.016828\n",
      "621283187  0.016896\n",
      "621283225  0.016906\n",
      "621283312  0.016922\n",
      "621283190  0.017093\n",
      "621283310  0.017153\n",
      "621283246  0.017301\n",
      "621283304  0.017598\n",
      "621283320  0.017658\n",
      "621283228  0.017885\n",
      "621283393  0.018306\n",
      "621283307  0.021158\n",
      "621283227  0.025069\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13850  0.019863\n",
      "13852  0.020258\n",
      "13896  0.020294\n",
      "13883  0.020294\n",
      "13897  0.020322\n",
      "67821  0.020533\n",
      "13917  0.020753\n",
      "13915  0.021715\n",
      "67155  0.027859\n",
      "68060  0.027891\n",
      "66975  0.028196\n",
      "68402  0.028262\n",
      "68718  0.028438\n",
      "67675  0.029689\n",
      "67848  0.029796\n",
      "67076  0.030022\n",
      "67893  0.030353\n",
      "67022  0.030363\n",
      "67854  0.030515\n",
      "67807  0.030583\n",
      ">>> Fastest Solution is rocblas 621283226 0.01595419943332672\n",
      "M N K dtype 4096 40 512 torch.bfloat16 >>> Total rocb solutions 488\n",
      "M N K bias dtype 4096 40 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283371  0.018261\n",
      "621283379  0.018482\n",
      "621283411  0.018562\n",
      "621283410  0.018562\n",
      "621283539  0.018983\n",
      "621283620  0.019143\n",
      "621283564  0.019183\n",
      "621283469  0.019204\n",
      "621283417  0.019344\n",
      "621283314  0.019524\n",
      "621283225  0.019584\n",
      "621283540  0.019605\n",
      "621283380  0.019645\n",
      "621283413  0.019764\n",
      "621283240  0.019765\n",
      "621283366  0.019805\n",
      "621283250  0.019865\n",
      "621283253  0.019986\n",
      "621283306  0.020005\n",
      "621283213  0.020105\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "68378  0.023794\n",
      "66932  0.023794\n",
      "13926  0.023794\n",
      "13866  0.024075\n",
      "68873  0.024094\n",
      "13772  0.024115\n",
      "13870  0.024175\n",
      "66899  0.024175\n",
      "66864  0.024195\n",
      "66849  0.024275\n",
      "67086  0.024355\n",
      "67162  0.024416\n",
      "67410  0.024475\n",
      "66855  0.024535\n",
      "66868  0.024756\n",
      "13752  0.024776\n",
      "67566  0.024836\n",
      "13839  0.024997\n",
      "13918  0.024997\n",
      "67501  0.025037\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283411  0.015106\n",
      "621283240  0.015866\n",
      "621283417  0.016255\n",
      "621283380  0.016393\n",
      "621283469  0.016445\n",
      "621283306  0.016556\n",
      "621283539  0.016574\n",
      "621283620  0.016646\n",
      "621283366  0.016688\n",
      "621283540  0.016802\n",
      "621283564  0.016830\n",
      "621283410  0.016884\n",
      "621283314  0.016904\n",
      "621283413  0.017113\n",
      "621283253  0.017147\n",
      "621283225  0.017189\n",
      "621283250  0.017257\n",
      "621283213  0.017279\n",
      "621283379  0.021970\n",
      "621283371  0.025508\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "68378  0.018736\n",
      "13870  0.019486\n",
      "67566  0.020679\n",
      "66899  0.020893\n",
      "67162  0.021286\n",
      "13918  0.021298\n",
      "13752  0.021922\n",
      "68873  0.022146\n",
      "66864  0.022186\n",
      "67501  0.022212\n",
      "13839  0.022335\n",
      "66855  0.022545\n",
      "66868  0.022559\n",
      "66932  0.022623\n",
      "67410  0.022669\n",
      "13926  0.022738\n",
      "67086  0.022826\n",
      "13772  0.022864\n",
      "13866  0.023026\n",
      "66849  0.023397\n",
      ">>> Fastest Solution is rocblas 621283411 0.01510625034570694\n",
      "M N K dtype 3584 40 4096 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 3584 40 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283226  0.020686\n",
      "621286472  0.020872\n",
      "621283392  0.021008\n",
      "621283225  0.021409\n",
      "621283319  0.021649\n",
      "621283306  0.021990\n",
      "621283316  0.022050\n",
      "621283189  0.022216\n",
      "621283245  0.022250\n",
      "621283314  0.022286\n",
      "621283187  0.022290\n",
      "621283244  0.022331\n",
      "621283304  0.022396\n",
      "621283388  0.022431\n",
      "621283323  0.022636\n",
      "621283213  0.022806\n",
      "621286475  0.022852\n",
      "621283212  0.023032\n",
      "621283313  0.023168\n",
      "621286473  0.023248\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13859  0.024696\n",
      "67738  0.025497\n",
      "13855  0.025658\n",
      "13858  0.025743\n",
      "67717  0.025874\n",
      "13845  0.026019\n",
      "13850  0.026019\n",
      "13893  0.026099\n",
      "13897  0.026120\n",
      "13883  0.026601\n",
      "13886  0.026641\n",
      "13896  0.026760\n",
      "67837  0.026781\n",
      "67661  0.026805\n",
      "13793  0.026871\n",
      "13884  0.026881\n",
      "13892  0.027262\n",
      "13765  0.027312\n",
      "13927  0.027653\n",
      "67895  0.027723\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283306  0.016002\n",
      "621283225  0.016812\n",
      "621283304  0.017431\n",
      "621286472  0.017435\n",
      "621283189  0.017456\n",
      "621283187  0.017457\n",
      "621283319  0.017751\n",
      "621283323  0.017899\n",
      "621283245  0.018511\n",
      "621283316  0.018733\n",
      "621283314  0.018872\n",
      "621283392  0.018898\n",
      "621283212  0.019196\n",
      "621283213  0.019341\n",
      "621283313  0.019356\n",
      "621283244  0.019448\n",
      "621286475  0.019646\n",
      "621286473  0.021371\n",
      "621283388  0.025361\n",
      "621283226  0.026596\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13892  0.021917\n",
      "13886  0.022217\n",
      "13850  0.022275\n",
      "13845  0.022341\n",
      "13855  0.022353\n",
      "67661  0.022462\n",
      "13893  0.022493\n",
      "13859  0.022974\n",
      "67837  0.023138\n",
      "13883  0.023758\n",
      "13896  0.023790\n",
      "67717  0.024138\n",
      "13897  0.024209\n",
      "13884  0.024417\n",
      "13765  0.024578\n",
      "67738  0.025057\n",
      "67895  0.025496\n",
      "13793  0.025850\n",
      "13927  0.025971\n",
      "13858  0.026011\n",
      ">>> Fastest Solution is rocblas 621283306 0.01600230038166046\n",
      "M N K dtype 4096 40 1792 torch.bfloat16 >>> Total rocb solutions 491\n",
      "M N K bias dtype 4096 40 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283359  0.018321\n",
      "621283363  0.018543\n",
      "621283339  0.019264\n",
      "621283362  0.020025\n",
      "621286472  0.020326\n",
      "621283335  0.020667\n",
      "621283303  0.020707\n",
      "621283308  0.020787\n",
      "621283325  0.020807\n",
      "621283304  0.020807\n",
      "621283229  0.020827\n",
      "621283220  0.020827\n",
      "621283191  0.020887\n",
      "621283329  0.020908\n",
      "621283224  0.020942\n",
      "621286476  0.020947\n",
      "621283334  0.020987\n",
      "621283323  0.021008\n",
      "621283321  0.021087\n",
      "621283354  0.021108\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67952  0.024195\n",
      "67903  0.024335\n",
      "68091  0.025117\n",
      "13857  0.025157\n",
      "13858  0.025177\n",
      "67856  0.025718\n",
      "13929  0.025719\n",
      "13906  0.025879\n",
      "67260  0.026079\n",
      "67823  0.026160\n",
      "13899  0.026239\n",
      "13771  0.026239\n",
      "67045  0.026500\n",
      "67863  0.026601\n",
      "13925  0.026641\n",
      "13777  0.026741\n",
      "13873  0.026780\n",
      "13853  0.026780\n",
      "67755  0.026901\n",
      "66839  0.026961\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283362  0.015690\n",
      "621283303  0.016391\n",
      "621283224  0.016459\n",
      "621283323  0.016469\n",
      "621283220  0.016627\n",
      "621283321  0.016630\n",
      "621283308  0.016772\n",
      "621283325  0.016788\n",
      "621283304  0.016834\n",
      "621286472  0.016836\n",
      "621283329  0.016966\n",
      "621283334  0.017103\n",
      "621286476  0.017151\n",
      "621283191  0.017181\n",
      "621283354  0.017566\n",
      "621283335  0.017834\n",
      "621283229  0.017844\n",
      "621283339  0.019031\n",
      "621283363  0.020553\n",
      "621283359  0.021565\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13858  0.020735\n",
      "13899  0.021683\n",
      "13906  0.021912\n",
      "13873  0.021964\n",
      "13853  0.022252\n",
      "13925  0.022254\n",
      "13777  0.022507\n",
      "67856  0.022629\n",
      "67755  0.022994\n",
      "67863  0.023153\n",
      "67823  0.023313\n",
      "13857  0.023437\n",
      "67045  0.023584\n",
      "67260  0.023858\n",
      "67903  0.024020\n",
      "68091  0.024786\n",
      "66839  0.025770\n",
      "13771  0.028336\n",
      "13929  0.028685\n",
      "67952  0.030112\n",
      ">>> Fastest Solution is rocblas 621283362 0.015689550340175627\n",
      "M N K dtype 768 32 4096 torch.bfloat16 >>> Total rocb solutions 538\n",
      "M N K bias dtype 768 32 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283350  0.018923\n",
      "621283393  0.019024\n",
      "621283227  0.019364\n",
      "621283307  0.019424\n",
      "621283530  0.019624\n",
      "621283226  0.019986\n",
      "621283188  0.020046\n",
      "621283311  0.020186\n",
      "621283223  0.020206\n",
      "621283187  0.020286\n",
      "621283228  0.020366\n",
      "621283313  0.020406\n",
      "621286469  0.020446\n",
      "621283323  0.020446\n",
      "621286474  0.020687\n",
      "621283321  0.020707\n",
      "621283190  0.020747\n",
      "621286475  0.020747\n",
      "621283191  0.020787\n",
      "621283220  0.020867\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67009  0.017179\n",
      "66978  0.018041\n",
      "68034  0.018061\n",
      "68476  0.018101\n",
      "67710  0.018762\n",
      "68140  0.019264\n",
      "67029  0.019645\n",
      "68071  0.020687\n",
      "67094  0.020887\n",
      "68525  0.020947\n",
      "67737  0.021228\n",
      "67637  0.021428\n",
      "67297  0.021729\n",
      "13884  0.023153\n",
      "67757  0.023253\n",
      "13897  0.023834\n",
      "13896  0.023854\n",
      "13917  0.024014\n",
      "67837  0.024115\n",
      "67641  0.024175\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.016177\n",
      "621283311  0.016273\n",
      "621286475  0.016423\n",
      "621283226  0.016523\n",
      "621283313  0.016554\n",
      "621283191  0.016650\n",
      "621283321  0.016750\n",
      "621286474  0.016814\n",
      "621283323  0.016934\n",
      "621283188  0.017151\n",
      "621283223  0.017205\n",
      "621283190  0.017255\n",
      "621283187  0.017369\n",
      "621283228  0.017514\n",
      "621286469  0.017592\n",
      "621283220  0.018023\n",
      "621283307  0.018063\n",
      "621283227  0.020577\n",
      "621283393  0.023880\n",
      "621283350  0.024874\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67009  0.018690\n",
      "13896  0.020130\n",
      "13897  0.020306\n",
      "13884  0.020990\n",
      "13917  0.021000\n",
      "67837  0.021150\n",
      "67757  0.021589\n",
      "67637  0.030096\n",
      "67641  0.030259\n",
      "67710  0.030309\n",
      "67297  0.030511\n",
      "68140  0.030772\n",
      "68071  0.030898\n",
      "67029  0.031050\n",
      "68476  0.031273\n",
      "67737  0.031534\n",
      "68525  0.031724\n",
      "68034  0.031734\n",
      "67094  0.032686\n",
      "66978  0.032895\n",
      ">>> Fastest Solution is rocblas 621283530 0.016176700592041016\n",
      "M N K dtype 4096 32 512 torch.bfloat16 >>> Total rocb solutions 533\n",
      "M N K bias dtype 4096 32 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283656  0.017981\n",
      "621283417  0.018442\n",
      "621283418  0.018842\n",
      "621283411  0.018863\n",
      "621283410  0.018943\n",
      "621283380  0.018983\n",
      "621283378  0.019023\n",
      "621283416  0.019143\n",
      "621283540  0.019143\n",
      "621283620  0.019164\n",
      "621283539  0.019224\n",
      "621283311  0.019243\n",
      "621283469  0.019404\n",
      "621283412  0.019464\n",
      "621283321  0.019624\n",
      "621283369  0.019705\n",
      "621283335  0.019865\n",
      "621283254  0.019885\n",
      "621283362  0.019905\n",
      "621283371  0.019985\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13870  0.022992\n",
      "13772  0.023032\n",
      "67544  0.023353\n",
      "13911  0.023493\n",
      "67280  0.023994\n",
      "67566  0.024094\n",
      "66899  0.024435\n",
      "68873  0.024435\n",
      "13763  0.024676\n",
      "67244  0.024776\n",
      "13867  0.024797\n",
      "13761  0.024817\n",
      "67213  0.024817\n",
      "67162  0.024876\n",
      "66868  0.024897\n",
      "68893  0.024997\n",
      "13820  0.025017\n",
      "13919  0.025037\n",
      "67037  0.025137\n",
      "13750  0.025157\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283469  0.016187\n",
      "621283311  0.016379\n",
      "621283620  0.016411\n",
      "621283380  0.016519\n",
      "621283335  0.016550\n",
      "621283539  0.016700\n",
      "621283410  0.016886\n",
      "621283321  0.016900\n",
      "621283369  0.016978\n",
      "621283416  0.017065\n",
      "621283378  0.017221\n",
      "621283254  0.017355\n",
      "621283362  0.017438\n",
      "621283412  0.017740\n",
      "621283411  0.017822\n",
      "621283540  0.017935\n",
      "621283371  0.018009\n",
      "621283418  0.018430\n",
      "621283656  0.019911\n",
      "621283417  0.020933\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13919  0.020905\n",
      "13867  0.020992\n",
      "67566  0.021130\n",
      "67162  0.021136\n",
      "67244  0.021290\n",
      "67213  0.021555\n",
      "66868  0.021729\n",
      "13763  0.021844\n",
      "66899  0.022074\n",
      "13761  0.022216\n",
      "13820  0.022497\n",
      "13772  0.022701\n",
      "68893  0.022704\n",
      "13750  0.022704\n",
      "67280  0.022740\n",
      "67544  0.023339\n",
      "68873  0.023469\n",
      "13911  0.023501\n",
      "67037  0.024047\n",
      "13870  0.028741\n",
      ">>> Fastest Solution is rocblas 621283469 0.016186749935150145\n",
      "M N K dtype 3584 32 4096 torch.bfloat16 >>> Total rocb solutions 538\n",
      "M N K bias dtype 3584 32 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286465  0.019419\n",
      "621283207  0.019709\n",
      "621286474  0.020446\n",
      "621283321  0.020612\n",
      "621283347  0.020627\n",
      "621283350  0.020647\n",
      "621283530  0.020867\n",
      "621283208  0.020942\n",
      "621283322  0.021288\n",
      "621283186  0.021389\n",
      "621283183  0.021389\n",
      "621283319  0.021509\n",
      "621283191  0.021549\n",
      "621286467  0.021674\n",
      "621283384  0.021769\n",
      "621286475  0.021850\n",
      "621283385  0.021909\n",
      "621283310  0.022030\n",
      "621283316  0.022070\n",
      "621283304  0.022080\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13845  0.024976\n",
      "67631  0.025177\n",
      "13850  0.025798\n",
      "67865  0.025838\n",
      "13859  0.025879\n",
      "67642  0.025938\n",
      "13894  0.025979\n",
      "13893  0.025979\n",
      "13855  0.026060\n",
      "13884  0.026139\n",
      "13852  0.026159\n",
      "13886  0.026160\n",
      "67837  0.026199\n",
      "13906  0.026245\n",
      "67797  0.026400\n",
      "13917  0.026680\n",
      "13848  0.026701\n",
      "67694  0.027362\n",
      "13778  0.027362\n",
      "13896  0.027602\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283321  0.015934\n",
      "621283207  0.016343\n",
      "621283186  0.016674\n",
      "621283319  0.016754\n",
      "621283530  0.016884\n",
      "621283347  0.016997\n",
      "621286465  0.017059\n",
      "621283208  0.017254\n",
      "621283384  0.017346\n",
      "621283183  0.017431\n",
      "621283191  0.017434\n",
      "621283322  0.017475\n",
      "621283310  0.017574\n",
      "621286475  0.017774\n",
      "621283385  0.017837\n",
      "621286467  0.018300\n",
      "621283304  0.018714\n",
      "621286474  0.018769\n",
      "621283316  0.019110\n",
      "621283350  0.021948\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67642  0.020581\n",
      "13850  0.022565\n",
      "13859  0.022633\n",
      "13894  0.022655\n",
      "13886  0.022744\n",
      "67837  0.022826\n",
      "13848  0.022842\n",
      "13855  0.022972\n",
      "13893  0.023197\n",
      "67694  0.023221\n",
      "13778  0.023403\n",
      "13852  0.023475\n",
      "67797  0.023533\n",
      "13884  0.024013\n",
      "13896  0.024016\n",
      "67631  0.024047\n",
      "13917  0.024415\n",
      "67865  0.024734\n",
      "13906  0.025761\n",
      "13845  0.030435\n",
      ">>> Fastest Solution is rocblas 621283321 0.0159341499209404\n",
      "M N K dtype 4096 32 1792 torch.bfloat16 >>> Total rocb solutions 538\n",
      "M N K bias dtype 4096 32 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283362  0.018723\n",
      "621283321  0.019204\n",
      "621283397  0.019384\n",
      "621283359  0.019564\n",
      "621283186  0.019664\n",
      "621283297  0.019825\n",
      "621283293  0.019945\n",
      "621283182  0.020005\n",
      "621283221  0.020126\n",
      "621283320  0.020186\n",
      "621283212  0.020226\n",
      "621283315  0.020226\n",
      "621283352  0.020286\n",
      "621283314  0.020306\n",
      "621283217  0.020326\n",
      "621283350  0.020326\n",
      "621283338  0.020346\n",
      "621283181  0.020366\n",
      "621283322  0.020387\n",
      "621283224  0.020406\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "36974  0.022872\n",
      "13765  0.023032\n",
      "67856  0.024495\n",
      "13895  0.024616\n",
      "67952  0.024716\n",
      "67863  0.024957\n",
      "67825  0.025157\n",
      "67951  0.025578\n",
      "67469  0.025698\n",
      "66939  0.025718\n",
      "66983  0.025738\n",
      "13860  0.025739\n",
      "66853  0.025779\n",
      "13864  0.025779\n",
      "67188  0.025999\n",
      "13794  0.026039\n",
      "67879  0.026139\n",
      "67405  0.026199\n",
      "13791  0.026220\n",
      "13912  0.026260\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283359  0.016503\n",
      "621283224  0.016668\n",
      "621283315  0.016680\n",
      "621283297  0.016718\n",
      "621283221  0.016792\n",
      "621283322  0.017025\n",
      "621283212  0.017095\n",
      "621283314  0.017237\n",
      "621283320  0.017241\n",
      "621283350  0.017420\n",
      "621283321  0.017422\n",
      "621283186  0.017448\n",
      "621283217  0.017869\n",
      "621283338  0.018314\n",
      "621283182  0.018370\n",
      "621283293  0.018382\n",
      "621283181  0.018442\n",
      "621283352  0.018458\n",
      "621283397  0.019867\n",
      "621283362  0.020807\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "36974  0.010750\n",
      "67856  0.021048\n",
      "67951  0.021421\n",
      "67952  0.021687\n",
      "13860  0.022164\n",
      "13912  0.022667\n",
      "67405  0.022697\n",
      "13864  0.022734\n",
      "13791  0.022896\n",
      "67188  0.023016\n",
      "66939  0.023066\n",
      "13895  0.023195\n",
      "67879  0.023499\n",
      "13765  0.023662\n",
      "67863  0.023758\n",
      "67825  0.024135\n",
      "13794  0.024828\n",
      "66853  0.026723\n",
      "67469  0.029539\n",
      "66983  0.029655\n",
      ">>> Fastest Solution is hipblaslt 36974 0.01075040027499199\n",
      "M N K dtype 768 24 4096 torch.bfloat16 >>> Total rocb solutions 538\n",
      "M N K bias dtype 768 24 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283350  0.017199\n",
      "621283307  0.019364\n",
      "621283530  0.019464\n",
      "621283393  0.019564\n",
      "621283227  0.019905\n",
      "621283226  0.019986\n",
      "621283294  0.020105\n",
      "621283303  0.020467\n",
      "621283182  0.020487\n",
      "621283187  0.020527\n",
      "621286464  0.020527\n",
      "621283323  0.020627\n",
      "621283191  0.020647\n",
      "621286475  0.020687\n",
      "621283190  0.020687\n",
      "621283383  0.020747\n",
      "621283219  0.020787\n",
      "621286469  0.020827\n",
      "621283319  0.020827\n",
      "621286472  0.020847\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "68107  0.019264\n",
      "67772  0.020246\n",
      "67676  0.020246\n",
      "68071  0.021990\n",
      "13845  0.022932\n",
      "67675  0.023093\n",
      "67553  0.023172\n",
      "13850  0.023232\n",
      "13852  0.023333\n",
      "68764  0.023494\n",
      "13883  0.023714\n",
      "67837  0.023875\n",
      "67865  0.023994\n",
      "67164  0.024275\n",
      "13882  0.024355\n",
      "67746  0.024596\n",
      "67808  0.024696\n",
      "67678  0.024757\n",
      "66998  0.024816\n",
      "13894  0.025177\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.016411\n",
      "621283227  0.016427\n",
      "621283190  0.016520\n",
      "621283303  0.016588\n",
      "621283294  0.016642\n",
      "621286464  0.016740\n",
      "621283383  0.016828\n",
      "621283182  0.016846\n",
      "621286475  0.016948\n",
      "621286469  0.016964\n",
      "621283187  0.016999\n",
      "621283319  0.017085\n",
      "621283226  0.017237\n",
      "621283323  0.017343\n",
      "621283219  0.017415\n",
      "621286472  0.017877\n",
      "621283191  0.018171\n",
      "621283393  0.018532\n",
      "621283307  0.025041\n",
      "621283350  0.025564\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13850  0.019807\n",
      "67808  0.020104\n",
      "13845  0.020128\n",
      "13883  0.020186\n",
      "13852  0.020212\n",
      "68107  0.020825\n",
      "67837  0.020915\n",
      "67865  0.021431\n",
      "67746  0.021649\n",
      "67678  0.021695\n",
      "13882  0.022256\n",
      "13894  0.022291\n",
      "66998  0.023247\n",
      "67164  0.029525\n",
      "67553  0.029577\n",
      "67675  0.030028\n",
      "67676  0.030826\n",
      "68764  0.030850\n",
      "68071  0.031215\n",
      "67772  0.031347\n",
      ">>> Fastest Solution is rocblas 621283530 0.016411200165748596\n",
      "M N K dtype 4096 24 512 torch.bfloat16 >>> Total rocb solutions 533\n",
      "M N K bias dtype 4096 24 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283371  0.018342\n",
      "621283320  0.018823\n",
      "621283540  0.019003\n",
      "621283564  0.019003\n",
      "621283400  0.019124\n",
      "621283620  0.019264\n",
      "621283469  0.019283\n",
      "621283417  0.019384\n",
      "621283380  0.019384\n",
      "621283656  0.019404\n",
      "621283418  0.019424\n",
      "621283410  0.019444\n",
      "621283542  0.019484\n",
      "621283539  0.019805\n",
      "621283241  0.019825\n",
      "621283416  0.019825\n",
      "621283379  0.019905\n",
      "621283412  0.020065\n",
      "621283414  0.020105\n",
      "621283236  0.020166\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13842  0.022310\n",
      "67086  0.023333\n",
      "13906  0.023453\n",
      "67602  0.023453\n",
      "66899  0.023594\n",
      "13772  0.023753\n",
      "68873  0.023874\n",
      "66973  0.023894\n",
      "13870  0.024015\n",
      "66868  0.024035\n",
      "66968  0.024095\n",
      "67156  0.024295\n",
      "67170  0.024335\n",
      "67566  0.024395\n",
      "66864  0.024416\n",
      "13756  0.024475\n",
      "67361  0.024535\n",
      "67410  0.024656\n",
      "67544  0.024716\n",
      "66855  0.024736\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283379  0.015732\n",
      "621283418  0.015928\n",
      "621283469  0.016056\n",
      "621283540  0.016387\n",
      "621283416  0.016471\n",
      "621283620  0.016485\n",
      "621283417  0.016547\n",
      "621283241  0.016554\n",
      "621283564  0.016886\n",
      "621283380  0.016892\n",
      "621283656  0.016916\n",
      "621283410  0.016952\n",
      "621283542  0.016997\n",
      "621283414  0.017027\n",
      "621283412  0.017235\n",
      "621283539  0.017237\n",
      "621283320  0.017654\n",
      "621283236  0.017752\n",
      "621283400  0.017973\n",
      "621283371  0.024650\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13842  0.006142\n",
      "67566  0.020647\n",
      "67602  0.020942\n",
      "13870  0.021334\n",
      "66899  0.021589\n",
      "66864  0.022357\n",
      "66855  0.022623\n",
      "66868  0.022768\n",
      "13756  0.022844\n",
      "68873  0.022874\n",
      "67410  0.023006\n",
      "67170  0.023020\n",
      "67361  0.023165\n",
      "13906  0.023343\n",
      "66973  0.023557\n",
      "67544  0.023561\n",
      "13772  0.023567\n",
      "67156  0.023906\n",
      "67086  0.028330\n",
      "66968  0.036250\n",
      ">>> Fastest Solution is hipblaslt 13842 0.006141949817538262\n",
      "M N K dtype 3584 24 4096 torch.bfloat16 >>> Total rocb solutions 538\n",
      "M N K bias dtype 3584 24 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.019945\n",
      "621283385  0.020046\n",
      "621283322  0.020366\n",
      "621286471  0.020987\n",
      "621283310  0.021008\n",
      "621283302  0.021008\n",
      "621283216  0.021224\n",
      "621283334  0.021248\n",
      "621283312  0.021328\n",
      "621283308  0.021398\n",
      "621283225  0.021650\n",
      "621283189  0.021769\n",
      "621283384  0.021930\n",
      "621286475  0.022010\n",
      "621283317  0.022130\n",
      "621283210  0.022150\n",
      "621286476  0.022171\n",
      "621286474  0.022210\n",
      "621283319  0.022331\n",
      "621283320  0.022331\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67642  0.023694\n",
      "13850  0.024676\n",
      "13855  0.024776\n",
      "13831  0.025393\n",
      "13845  0.025719\n",
      "13884  0.025819\n",
      "13886  0.025838\n",
      "67837  0.025939\n",
      "13883  0.025959\n",
      "13881  0.026065\n",
      "13859  0.026339\n",
      "13857  0.026345\n",
      "13893  0.026400\n",
      "13848  0.026500\n",
      "13894  0.026580\n",
      "13892  0.026601\n",
      "67865  0.026741\n",
      "13839  0.026801\n",
      "13925  0.026951\n",
      "13730  0.027016\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283322  0.016622\n",
      "621283225  0.016920\n",
      "621283189  0.017067\n",
      "621283334  0.017117\n",
      "621283320  0.017149\n",
      "621283319  0.017375\n",
      "621283310  0.017379\n",
      "621283384  0.017426\n",
      "621286475  0.017472\n",
      "621283317  0.017510\n",
      "621286471  0.017544\n",
      "621286476  0.017548\n",
      "621286474  0.017594\n",
      "621283312  0.017668\n",
      "621283385  0.017778\n",
      "621283216  0.017981\n",
      "621283302  0.018041\n",
      "621283308  0.019580\n",
      "621283210  0.021789\n",
      "621283530  0.024995\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13857  0.019948\n",
      "13845  0.021517\n",
      "13855  0.021813\n",
      "13892  0.021966\n",
      "13894  0.022385\n",
      "13893  0.022483\n",
      "13831  0.022643\n",
      "13848  0.022718\n",
      "13886  0.022728\n",
      "67865  0.022825\n",
      "67837  0.022900\n",
      "13925  0.023275\n",
      "13881  0.023308\n",
      "13883  0.023505\n",
      "13884  0.023810\n",
      "13859  0.023830\n",
      "67642  0.025093\n",
      "13730  0.025277\n",
      "13839  0.026006\n",
      "13850  0.026320\n",
      ">>> Fastest Solution is rocblas 621283322 0.016621699929237364\n",
      "M N K dtype 4096 24 1792 torch.bfloat16 >>> Total rocb solutions 538\n",
      "M N K bias dtype 4096 24 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283351  0.018903\n",
      "621283362  0.018923\n",
      "621283335  0.019083\n",
      "621283350  0.019484\n",
      "621283318  0.019524\n",
      "621283207  0.019885\n",
      "621283340  0.019946\n",
      "621283359  0.020146\n",
      "621283182  0.020226\n",
      "621286476  0.020286\n",
      "621286471  0.020366\n",
      "621283363  0.020406\n",
      "621283209  0.020427\n",
      "621283347  0.020527\n",
      "621283185  0.020546\n",
      "621283308  0.020647\n",
      "621283229  0.020647\n",
      "621286465  0.020657\n",
      "621283293  0.020667\n",
      "621283219  0.020687\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67952  0.024235\n",
      "13770  0.024656\n",
      "67692  0.025077\n",
      "67973  0.025317\n",
      "67592  0.025658\n",
      "13839  0.026199\n",
      "13771  0.026199\n",
      "66939  0.026199\n",
      "67405  0.026260\n",
      "13876  0.026400\n",
      "13907  0.026400\n",
      "67879  0.026440\n",
      "67856  0.026500\n",
      "68399  0.026540\n",
      "13851  0.026841\n",
      "13789  0.026861\n",
      "13860  0.026921\n",
      "13844  0.026982\n",
      "67919  0.027042\n",
      "66966  0.027322\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283359  0.016016\n",
      "621283185  0.016610\n",
      "621286476  0.016770\n",
      "621283308  0.016770\n",
      "621283219  0.016870\n",
      "621283207  0.016880\n",
      "621283229  0.016976\n",
      "621283318  0.017047\n",
      "621286465  0.017079\n",
      "621283209  0.017107\n",
      "621286471  0.017279\n",
      "621283347  0.017317\n",
      "621283350  0.017412\n",
      "621283293  0.017582\n",
      "621283340  0.017818\n",
      "621283182  0.018163\n",
      "621283363  0.018189\n",
      "621283351  0.018275\n",
      "621283335  0.019939\n",
      "621283362  0.020005\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13907  0.022108\n",
      "67592  0.022122\n",
      "13839  0.022178\n",
      "13844  0.022182\n",
      "67973  0.022343\n",
      "13876  0.022473\n",
      "13860  0.022533\n",
      "67405  0.022868\n",
      "67879  0.022902\n",
      "67919  0.022910\n",
      "13851  0.023058\n",
      "66939  0.023082\n",
      "66966  0.023207\n",
      "13770  0.023407\n",
      "67856  0.023702\n",
      "67692  0.024399\n",
      "68399  0.024460\n",
      "13771  0.027637\n",
      "13789  0.030082\n",
      "67952  0.030208\n",
      ">>> Fastest Solution is rocblas 621283359 0.016016350686550142\n",
      "M N K dtype 768 16 4096 torch.bfloat16 >>> Total rocb solutions 549\n",
      "M N K bias dtype 768 16 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283345  0.016598\n",
      "621283350  0.017199\n",
      "621283344  0.018382\n",
      "621283227  0.019384\n",
      "621283184  0.019424\n",
      "621283307  0.019524\n",
      "621283226  0.019645\n",
      "621283347  0.019945\n",
      "621283313  0.020046\n",
      "621283292  0.020186\n",
      "621283308  0.020226\n",
      "621283530  0.020286\n",
      "621283188  0.020286\n",
      "621283187  0.020306\n",
      "621283311  0.020346\n",
      "621283183  0.020366\n",
      "621283310  0.020387\n",
      "621286469  0.020487\n",
      "621283346  0.020527\n",
      "621283206  0.020547\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13884  0.023032\n",
      "13850  0.023413\n",
      "13883  0.023473\n",
      "68525  0.023613\n",
      "13896  0.023734\n",
      "13897  0.023794\n",
      "67757  0.023814\n",
      "67837  0.023935\n",
      "13852  0.024495\n",
      "67865  0.024636\n",
      "67895  0.024876\n",
      "68079  0.024997\n",
      "66947  0.024997\n",
      "67808  0.025057\n",
      "13917  0.025097\n",
      "13790  0.025197\n",
      "13894  0.025238\n",
      "13779  0.025277\n",
      "13893  0.025357\n",
      "67746  0.025497\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283346  0.015559\n",
      "621283311  0.016093\n",
      "621283530  0.016433\n",
      "621283226  0.016662\n",
      "621286469  0.016700\n",
      "621283347  0.016714\n",
      "621283310  0.016842\n",
      "621283183  0.016914\n",
      "621283313  0.016999\n",
      "621283206  0.017001\n",
      "621283292  0.017017\n",
      "621283187  0.017021\n",
      "621283308  0.017061\n",
      "621283184  0.017165\n",
      "621283307  0.017424\n",
      "621283188  0.017746\n",
      "621283344  0.020190\n",
      "621283227  0.022062\n",
      "621283350  0.022565\n",
      "621283345  0.023054\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13852  0.020577\n",
      "67808  0.020727\n",
      "67757  0.021292\n",
      "13917  0.021389\n",
      "67746  0.021401\n",
      "67837  0.021483\n",
      "13897  0.021639\n",
      "13896  0.021824\n",
      "67895  0.021944\n",
      "13779  0.021982\n",
      "13893  0.022024\n",
      "13790  0.022124\n",
      "13883  0.022204\n",
      "67865  0.022216\n",
      "13850  0.022361\n",
      "13894  0.022511\n",
      "66947  0.025039\n",
      "13884  0.030277\n",
      "68079  0.030381\n",
      "68525  0.032470\n",
      ">>> Fastest Solution is rocblas 621283346 0.015559299290180207\n",
      "M N K dtype 4096 16 512 torch.bfloat16 >>> Total rocb solutions 544\n",
      "M N K bias dtype 4096 16 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283371  0.018643\n",
      "621283411  0.018702\n",
      "621283418  0.018903\n",
      "621283379  0.019003\n",
      "621283288  0.019103\n",
      "621283417  0.019143\n",
      "621283620  0.019243\n",
      "621283564  0.019243\n",
      "621283228  0.019283\n",
      "621283309  0.019424\n",
      "621283362  0.019444\n",
      "621286469  0.019565\n",
      "621283540  0.019624\n",
      "621283412  0.019645\n",
      "621283416  0.019664\n",
      "621283378  0.019845\n",
      "621283370  0.019885\n",
      "621283323  0.020025\n",
      "621283542  0.020065\n",
      "621283380  0.020065\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67280  0.023373\n",
      "13772  0.023734\n",
      "68873  0.023754\n",
      "67086  0.023835\n",
      "67602  0.023854\n",
      "13856  0.023894\n",
      "66880  0.023954\n",
      "13891  0.024015\n",
      "67361  0.024015\n",
      "67566  0.024054\n",
      "67661  0.024416\n",
      "67162  0.024416\n",
      "66879  0.024475\n",
      "66864  0.024676\n",
      "66899  0.024756\n",
      "67527  0.024816\n",
      "13756  0.024936\n",
      "67244  0.024936\n",
      "67544  0.025037\n",
      "67170  0.025137\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283564  0.016241\n",
      "621283620  0.016694\n",
      "621283416  0.016766\n",
      "621283417  0.016850\n",
      "621283379  0.016862\n",
      "621283309  0.016892\n",
      "621283380  0.017075\n",
      "621283378  0.017305\n",
      "621286469  0.017387\n",
      "621283228  0.017395\n",
      "621283362  0.017776\n",
      "621283542  0.017965\n",
      "621283323  0.017975\n",
      "621283288  0.018045\n",
      "621283370  0.018143\n",
      "621283418  0.018159\n",
      "621283412  0.018237\n",
      "621283540  0.018769\n",
      "621283411  0.019236\n",
      "621283371  0.024311\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67602  0.020988\n",
      "67566  0.021421\n",
      "66899  0.021799\n",
      "66880  0.021888\n",
      "67162  0.021974\n",
      "13891  0.022006\n",
      "67244  0.022327\n",
      "13772  0.022347\n",
      "67361  0.022349\n",
      "67527  0.022561\n",
      "13756  0.022663\n",
      "68873  0.022958\n",
      "66879  0.023064\n",
      "67170  0.023297\n",
      "66864  0.023323\n",
      "67086  0.023333\n",
      "67661  0.023383\n",
      "13856  0.023429\n",
      "67544  0.023511\n",
      "67280  0.027989\n",
      ">>> Fastest Solution is rocblas 621283564 0.016240799427032472\n",
      "M N K dtype 3584 16 4096 torch.bfloat16 >>> Total rocb solutions 549\n",
      "M N K bias dtype 3584 16 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283350  0.017520\n",
      "621283347  0.017560\n",
      "621283530  0.019905\n",
      "621283186  0.019946\n",
      "621283382  0.020346\n",
      "621283383  0.020446\n",
      "621283189  0.020887\n",
      "621283385  0.020947\n",
      "621283322  0.021188\n",
      "621283306  0.021228\n",
      "621283291  0.021349\n",
      "621283207  0.021389\n",
      "621283316  0.021549\n",
      "621283218  0.021599\n",
      "621283321  0.021729\n",
      "621283290  0.021769\n",
      "621283190  0.021789\n",
      "621283384  0.021790\n",
      "621283188  0.021809\n",
      "621283312  0.021890\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13845  0.024696\n",
      "67837  0.024757\n",
      "13895  0.024897\n",
      "67865  0.025357\n",
      "13886  0.025478\n",
      "13883  0.025678\n",
      "13892  0.025799\n",
      "13855  0.025999\n",
      "67808  0.026019\n",
      "13894  0.026019\n",
      "13893  0.026039\n",
      "13738  0.026339\n",
      "13848  0.026761\n",
      "13735  0.026821\n",
      "13917  0.026921\n",
      "13852  0.027121\n",
      "13859  0.027222\n",
      "13778  0.027262\n",
      "67797  0.027482\n",
      "13779  0.027502\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283306  0.016461\n",
      "621283291  0.016862\n",
      "621283316  0.016946\n",
      "621283186  0.016976\n",
      "621283218  0.017043\n",
      "621283384  0.017145\n",
      "621283382  0.017147\n",
      "621283189  0.017157\n",
      "621283290  0.017181\n",
      "621283321  0.017203\n",
      "621283190  0.017305\n",
      "621283207  0.017343\n",
      "621283385  0.017353\n",
      "621283322  0.017500\n",
      "621283188  0.017561\n",
      "621283312  0.017642\n",
      "621283383  0.018243\n",
      "621283530  0.018905\n",
      "621283347  0.020847\n",
      "621283350  0.025686\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13859  0.021910\n",
      "13848  0.022056\n",
      "13894  0.022122\n",
      "13893  0.022289\n",
      "67808  0.022453\n",
      "13886  0.022583\n",
      "13892  0.022613\n",
      "13855  0.022842\n",
      "13895  0.022945\n",
      "13917  0.022984\n",
      "67797  0.023259\n",
      "13778  0.023289\n",
      "67865  0.023401\n",
      "13852  0.023519\n",
      "13883  0.023563\n",
      "13735  0.024913\n",
      "13738  0.025626\n",
      "67837  0.025817\n",
      "13779  0.025913\n",
      "13845  0.032131\n",
      ">>> Fastest Solution is rocblas 621283306 0.016461299359798433\n",
      "M N K dtype 4096 16 1792 torch.bfloat16 >>> Total rocb solutions 549\n",
      "M N K bias dtype 4096 16 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283351  0.018623\n",
      "621283362  0.019064\n",
      "621283359  0.019304\n",
      "621283321  0.019605\n",
      "621283304  0.020005\n",
      "621283227  0.020126\n",
      "621283303  0.020146\n",
      "621283306  0.020206\n",
      "621283225  0.020226\n",
      "621283317  0.020226\n",
      "621283350  0.020266\n",
      "621283363  0.020266\n",
      "621283397  0.020306\n",
      "621283305  0.020326\n",
      "621283345  0.020346\n",
      "621283335  0.020366\n",
      "621286472  0.020366\n",
      "621283308  0.020487\n",
      "621283357  0.020487\n",
      "621283220  0.020506\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13860  0.023894\n",
      "67952  0.024355\n",
      "67951  0.024676\n",
      "68882  0.024916\n",
      "67973  0.025057\n",
      "13902  0.025237\n",
      "67459  0.025658\n",
      "67260  0.025838\n",
      "67188  0.025838\n",
      "67592  0.025939\n",
      "13873  0.026079\n",
      "67849  0.026199\n",
      "67972  0.026220\n",
      "67190  0.026480\n",
      "13849  0.026520\n",
      "67652  0.026580\n",
      "67118  0.026701\n",
      "67863  0.026801\n",
      "13735  0.026801\n",
      "13739  0.026961\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283304  0.016339\n",
      "621283305  0.016547\n",
      "621283345  0.016748\n",
      "621283308  0.016974\n",
      "621283227  0.017001\n",
      "621283317  0.017029\n",
      "621283350  0.017051\n",
      "621283363  0.017105\n",
      "621283225  0.017131\n",
      "621286472  0.017191\n",
      "621283306  0.017227\n",
      "621283303  0.017357\n",
      "621283335  0.017512\n",
      "621283220  0.017618\n",
      "621283321  0.017634\n",
      "621283357  0.017642\n",
      "621283359  0.017746\n",
      "621283351  0.018019\n",
      "621283397  0.019204\n",
      "621283362  0.019869\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67951  0.021272\n",
      "67973  0.022289\n",
      "67592  0.022375\n",
      "13849  0.022605\n",
      "67459  0.022692\n",
      "13873  0.022766\n",
      "13739  0.023080\n",
      "13735  0.023086\n",
      "67118  0.023153\n",
      "67863  0.023181\n",
      "67188  0.023515\n",
      "67260  0.023545\n",
      "67972  0.023630\n",
      "67849  0.023788\n",
      "67952  0.024421\n",
      "13860  0.024710\n",
      "67190  0.027542\n",
      "13902  0.028194\n",
      "67652  0.029551\n",
      "68882  0.035915\n",
      ">>> Fastest Solution is rocblas 621283304 0.016339050233364107\n",
      "M N K dtype 768 8 4096 torch.bfloat16 >>> Total rocb solutions 549\n",
      "M N K bias dtype 768 8 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283345  0.016578\n",
      "621283346  0.016938\n",
      "621283350  0.016958\n",
      "621283344  0.017981\n",
      "621283530  0.019143\n",
      "621283307  0.019204\n",
      "621283232  0.019785\n",
      "621283244  0.019885\n",
      "621283347  0.020126\n",
      "621286466  0.020186\n",
      "621286467  0.020226\n",
      "621283225  0.020366\n",
      "621286471  0.020506\n",
      "621286473  0.020527\n",
      "621283226  0.020527\n",
      "621283304  0.020546\n",
      "621283219  0.020567\n",
      "621286476  0.020586\n",
      "621283220  0.020587\n",
      "621283297  0.020607\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13850  0.022952\n",
      "13897  0.023093\n",
      "67865  0.024054\n",
      "67746  0.024075\n",
      "13896  0.024135\n",
      "67837  0.024556\n",
      "13917  0.024556\n",
      "13883  0.024716\n",
      "67678  0.024736\n",
      "13790  0.024736\n",
      "13884  0.024876\n",
      "13746  0.024916\n",
      "13779  0.025056\n",
      "13859  0.025137\n",
      "13893  0.025217\n",
      "13894  0.025237\n",
      "13915  0.025739\n",
      "13846  0.025879\n",
      "13886  0.025919\n",
      "13848  0.025939\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283226  0.015571\n",
      "621283530  0.016151\n",
      "621283244  0.016295\n",
      "621283225  0.016365\n",
      "621283347  0.016520\n",
      "621283219  0.016530\n",
      "621286476  0.016648\n",
      "621286467  0.016806\n",
      "621286471  0.016878\n",
      "621283232  0.016914\n",
      "621286473  0.017077\n",
      "621283220  0.017235\n",
      "621283297  0.017323\n",
      "621283304  0.017592\n",
      "621286466  0.017800\n",
      "621283344  0.019550\n",
      "621283350  0.020158\n",
      "621283307  0.020226\n",
      "621283346  0.021242\n",
      "621283345  0.023599\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13883  0.020420\n",
      "13896  0.020428\n",
      "13884  0.020559\n",
      "13917  0.021004\n",
      "67837  0.021362\n",
      "13915  0.021687\n",
      "13779  0.021743\n",
      "13859  0.021803\n",
      "13893  0.021882\n",
      "13746  0.021954\n",
      "13846  0.021988\n",
      "13790  0.021996\n",
      "67865  0.022008\n",
      "13848  0.022014\n",
      "13894  0.022148\n",
      "13886  0.022190\n",
      "67678  0.022633\n",
      "67746  0.022754\n",
      "13897  0.025081\n",
      "13850  0.030108\n",
      ">>> Fastest Solution is rocblas 621283226 0.015571300685405732\n",
      "M N K dtype 4096 8 512 torch.bfloat16 >>> Total rocb solutions 544\n",
      "M N K bias dtype 4096 8 512 False torch.bfloat16 >>> Total hipb solutions 1646\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283530  0.017479\n",
      "621283564  0.019003\n",
      "621283620  0.019404\n",
      "621283319  0.019464\n",
      "621283411  0.019624\n",
      "621283469  0.019645\n",
      "621283288  0.019664\n",
      "621283378  0.019705\n",
      "621283296  0.019785\n",
      "621283298  0.019785\n",
      "621283345  0.019845\n",
      "621283241  0.019865\n",
      "621283410  0.019945\n",
      "621283542  0.019965\n",
      "621283207  0.019986\n",
      "621283330  0.020025\n",
      "621283414  0.020126\n",
      "621283412  0.020186\n",
      "621286471  0.020226\n",
      "621283353  0.020226\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13774  0.017038\n",
      "13782  0.019024\n",
      "13929  0.022631\n",
      "67280  0.022972\n",
      "68873  0.023413\n",
      "13870  0.023473\n",
      "67602  0.023494\n",
      "66899  0.023573\n",
      "13911  0.023694\n",
      "67566  0.024035\n",
      "67334  0.024075\n",
      "13756  0.024175\n",
      "66864  0.024376\n",
      "67162  0.024516\n",
      "13862  0.024516\n",
      "68102  0.024535\n",
      "67086  0.024556\n",
      "67915  0.024556\n",
      "68893  0.024716\n",
      "67213  0.024776\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.007032\n",
      "621283469  0.016628\n",
      "621286471  0.016974\n",
      "621283298  0.017079\n",
      "621283288  0.017107\n",
      "621283330  0.017205\n",
      "621283414  0.017287\n",
      "621283207  0.017440\n",
      "621283345  0.017444\n",
      "621283412  0.017564\n",
      "621283410  0.017584\n",
      "621283319  0.017646\n",
      "621283378  0.017674\n",
      "621283241  0.017724\n",
      "621283296  0.017746\n",
      "621283353  0.017899\n",
      "621283542  0.018223\n",
      "621283411  0.018935\n",
      "621283620  0.019119\n",
      "621283564  0.024806\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "67566  0.020647\n",
      "67602  0.020964\n",
      "13774  0.020980\n",
      "67162  0.021090\n",
      "67213  0.021417\n",
      "68102  0.021842\n",
      "13862  0.022054\n",
      "66899  0.022100\n",
      "13870  0.022232\n",
      "66864  0.022477\n",
      "68893  0.022513\n",
      "13911  0.022537\n",
      "67915  0.022583\n",
      "68873  0.022774\n",
      "67086  0.022846\n",
      "67280  0.022898\n",
      "13756  0.023141\n",
      "67334  0.023205\n",
      "13929  0.023439\n",
      "13782  0.030668\n",
      ">>> Fastest Solution is rocblas 621283530 0.007031950354576111\n",
      "M N K dtype 3584 8 4096 torch.bfloat16 >>> Total rocb solutions 549\n",
      "M N K bias dtype 3584 8 4096 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286472  0.017540\n",
      "621283218  0.019794\n",
      "621283530  0.020245\n",
      "621283183  0.020527\n",
      "621283344  0.020607\n",
      "621283347  0.020827\n",
      "621283385  0.020847\n",
      "621283302  0.021053\n",
      "621283384  0.021108\n",
      "621283292  0.021148\n",
      "621286465  0.021248\n",
      "621283392  0.021248\n",
      "621283355  0.021288\n",
      "621283288  0.021288\n",
      "621283232  0.021288\n",
      "621283287  0.021294\n",
      "621283303  0.021328\n",
      "621286468  0.021428\n",
      "621286476  0.021428\n",
      "621283313  0.021629\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13845  0.024496\n",
      "67865  0.025298\n",
      "13859  0.025498\n",
      "67837  0.025618\n",
      "13850  0.025739\n",
      "13917  0.025858\n",
      "13893  0.025939\n",
      "13855  0.026199\n",
      "13883  0.026460\n",
      "67808  0.026480\n",
      "13892  0.026560\n",
      "13886  0.026580\n",
      "13894  0.026601\n",
      "13848  0.027402\n",
      "13896  0.027422\n",
      "13765  0.027452\n",
      "13895  0.027482\n",
      "13790  0.027483\n",
      "13852  0.027483\n",
      "13884  0.027502\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283183  0.016656\n",
      "621283292  0.016836\n",
      "621283384  0.016900\n",
      "621283303  0.016926\n",
      "621283287  0.016945\n",
      "621286465  0.017019\n",
      "621283302  0.017153\n",
      "621283313  0.017257\n",
      "621283385  0.017281\n",
      "621283218  0.017405\n",
      "621283347  0.017427\n",
      "621283392  0.017470\n",
      "621286476  0.017474\n",
      "621286468  0.017846\n",
      "621283288  0.017930\n",
      "621283232  0.018334\n",
      "621286472  0.018468\n",
      "621283530  0.018757\n",
      "621283344  0.019562\n",
      "621283355  0.023134\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13850  0.021665\n",
      "13894  0.021787\n",
      "13892  0.022038\n",
      "13859  0.022170\n",
      "13848  0.022204\n",
      "13886  0.022263\n",
      "67808  0.022473\n",
      "13855  0.022489\n",
      "13893  0.022545\n",
      "67837  0.022561\n",
      "13852  0.023209\n",
      "13765  0.023277\n",
      "13896  0.023287\n",
      "13883  0.023405\n",
      "13895  0.023635\n",
      "13884  0.023698\n",
      "13917  0.023850\n",
      "67865  0.024361\n",
      "13790  0.025636\n",
      "13845  0.029673\n",
      ">>> Fastest Solution is rocblas 621283183 0.016655750572681427\n",
      "M N K dtype 4096 8 1792 torch.bfloat16 >>> Total rocb solutions 549\n",
      "M N K bias dtype 4096 8 1792 False torch.bfloat16 >>> Total hipb solutions 2252\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283362  0.017801\n",
      "621283351  0.018762\n",
      "621283258  0.018963\n",
      "621283359  0.019444\n",
      "621283185  0.019624\n",
      "621283345  0.019745\n",
      "621283204  0.019865\n",
      "621283397  0.019906\n",
      "621283215  0.020005\n",
      "621283212  0.020165\n",
      "621283222  0.020206\n",
      "621283332  0.020246\n",
      "621283208  0.020306\n",
      "621283352  0.020346\n",
      "621283360  0.020366\n",
      "621283228  0.020366\n",
      "621283207  0.020366\n",
      "621286469  0.020427\n",
      "621283341  0.020446\n",
      "621283217  0.020467\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67744  0.022231\n",
      "67952  0.024255\n",
      "67879  0.025197\n",
      "67260  0.025378\n",
      "67262  0.025378\n",
      "67592  0.025598\n",
      "66860  0.025639\n",
      "13735  0.025678\n",
      "13851  0.025879\n",
      "67504  0.025979\n",
      "67756  0.026039\n",
      "67972  0.026060\n",
      "13856  0.026280\n",
      "13738  0.026280\n",
      "13849  0.026300\n",
      "67405  0.026339\n",
      "67856  0.026360\n",
      "67825  0.026400\n",
      "67755  0.026420\n",
      "67833  0.026520\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283359  0.016171\n",
      "621283204  0.016231\n",
      "621283351  0.016509\n",
      "621283345  0.016521\n",
      "621283212  0.016646\n",
      "621283208  0.016912\n",
      "621283207  0.017033\n",
      "621283332  0.017097\n",
      "621283185  0.017145\n",
      "621283228  0.017355\n",
      "621286469  0.017524\n",
      "621283217  0.017630\n",
      "621283222  0.017802\n",
      "621283352  0.017861\n",
      "621283258  0.018456\n",
      "621283341  0.018470\n",
      "621283397  0.018931\n",
      "621283360  0.018981\n",
      "621283215  0.019701\n",
      "621283362  0.020286\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13851  0.022383\n",
      "67262  0.022485\n",
      "67833  0.023040\n",
      "67260  0.023094\n",
      "13849  0.023096\n",
      "67972  0.023223\n",
      "67825  0.023265\n",
      "13738  0.023303\n",
      "67592  0.023403\n",
      "13735  0.023573\n",
      "67879  0.023581\n",
      "67405  0.023648\n",
      "13856  0.023870\n",
      "67755  0.023918\n",
      "67856  0.024061\n",
      "67504  0.024155\n",
      "67952  0.025065\n",
      "67744  0.025782\n",
      "66860  0.026907\n",
      "67756  0.031105\n",
      ">>> Fastest Solution is rocblas 621283359 0.016170699894428254\n",
      "M N K dtype 768 4 4096 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 768 4 4096 False torch.bfloat16 >>> Total hipb solutions 2154\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283346  0.016097\n",
      "621283345  0.016477\n",
      "621283350  0.017038\n",
      "621283344  0.018582\n",
      "621283313  0.018723\n",
      "621283307  0.019544\n",
      "621283393  0.019564\n",
      "621283227  0.019725\n",
      "621283226  0.019906\n",
      "621283187  0.020186\n",
      "621283311  0.020286\n",
      "621283216  0.020306\n",
      "621286467  0.020306\n",
      "621283530  0.020346\n",
      "621283209  0.020467\n",
      "621283208  0.020546\n",
      "621283188  0.020547\n",
      "621283186  0.020567\n",
      "621283304  0.020586\n",
      "621286468  0.020606\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.018321\n",
      "67757  0.023634\n",
      "67837  0.023814\n",
      "67808  0.024015\n",
      "67865  0.024134\n",
      "67746  0.024135\n",
      "67895  0.024736\n",
      "13790  0.025478\n",
      "13779  0.025478\n",
      "67116  0.026039\n",
      "13753  0.026199\n",
      "13921  0.026239\n",
      "67916  0.026340\n",
      "13755  0.026380\n",
      "67004  0.026440\n",
      "13924  0.026520\n",
      "13778  0.026620\n",
      "67043  0.026701\n",
      "13786  0.026961\n",
      "67678  0.026961\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283226  0.015930\n",
      "621283530  0.016423\n",
      "621286467  0.016802\n",
      "621283311  0.016818\n",
      "621283216  0.016938\n",
      "621283208  0.017003\n",
      "621286468  0.017149\n",
      "621283187  0.017219\n",
      "621283304  0.017281\n",
      "621283209  0.017311\n",
      "621283313  0.017560\n",
      "621283227  0.017584\n",
      "621283188  0.017584\n",
      "621283186  0.017682\n",
      "621283393  0.018013\n",
      "621283307  0.018205\n",
      "621283344  0.019115\n",
      "621283350  0.019659\n",
      "621283345  0.021415\n",
      "621283346  0.023100\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.014816\n",
      "67865  0.021304\n",
      "67808  0.021425\n",
      "67678  0.021820\n",
      "67746  0.021882\n",
      "67895  0.021920\n",
      "13779  0.021940\n",
      "13790  0.022150\n",
      "67837  0.022234\n",
      "13921  0.022417\n",
      "13924  0.022814\n",
      "67116  0.023018\n",
      "13778  0.023341\n",
      "67916  0.023375\n",
      "13786  0.023391\n",
      "13755  0.023399\n",
      "67043  0.023543\n",
      "67004  0.023950\n",
      "13753  0.024590\n",
      "67757  0.028242\n",
      ">>> Fastest Solution is hipblaslt 13830 0.014815600216388702\n",
      "M N K dtype 4096 4 512 torch.bfloat16 >>> Total rocb solutions 524\n",
      "M N K bias dtype 4096 4 512 False torch.bfloat16 >>> Total hipb solutions 1548\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283618  0.017740\n",
      "621283540  0.017981\n",
      "621283378  0.018362\n",
      "621283418  0.018462\n",
      "621283656  0.018542\n",
      "621283539  0.018842\n",
      "621283564  0.019003\n",
      "621283410  0.019043\n",
      "621283411  0.019083\n",
      "621283469  0.019124\n",
      "621283417  0.019243\n",
      "621283181  0.019244\n",
      "621283620  0.019444\n",
      "621283413  0.019465\n",
      "621283254  0.019524\n",
      "621283391  0.019624\n",
      "621283380  0.019645\n",
      "621283235  0.019905\n",
      "621283542  0.019985\n",
      "621283414  0.020105\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.010604\n",
      "13831  0.016377\n",
      "67361  0.023172\n",
      "66931  0.023232\n",
      "67602  0.023393\n",
      "66957  0.023714\n",
      "66879  0.023774\n",
      "67086  0.023914\n",
      "67544  0.023975\n",
      "13772  0.024075\n",
      "66864  0.024135\n",
      "67527  0.024275\n",
      "67037  0.024536\n",
      "68102  0.024556\n",
      "66891  0.024576\n",
      "13928  0.024776\n",
      "66855  0.024816\n",
      "67486  0.024876\n",
      "67410  0.024916\n",
      "67156  0.024936\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283469  0.016048\n",
      "621283418  0.016070\n",
      "621283417  0.016133\n",
      "621283410  0.016149\n",
      "621283378  0.016167\n",
      "621283620  0.016365\n",
      "621283564  0.016377\n",
      "621283380  0.016419\n",
      "621283391  0.016888\n",
      "621283542  0.016934\n",
      "621283181  0.016969\n",
      "621283411  0.017003\n",
      "621283618  0.017081\n",
      "621283414  0.017155\n",
      "621283413  0.017391\n",
      "621283656  0.017718\n",
      "621283235  0.017867\n",
      "621283540  0.017889\n",
      "621283539  0.017931\n",
      "621283254  0.017939\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.008303\n",
      "67602  0.020362\n",
      "66891  0.021206\n",
      "67361  0.021613\n",
      "13928  0.021829\n",
      "68102  0.021998\n",
      "66879  0.022060\n",
      "13831  0.022154\n",
      "67086  0.022214\n",
      "67544  0.022246\n",
      "67527  0.022413\n",
      "66855  0.022591\n",
      "66864  0.022609\n",
      "13772  0.022784\n",
      "67486  0.022812\n",
      "67410  0.022822\n",
      "66957  0.022844\n",
      "67156  0.023387\n",
      "66931  0.023475\n",
      "67037  0.023844\n",
      ">>> Fastest Solution is hipblaslt 13830 0.008302850276231765\n",
      "M N K dtype 3584 4 4096 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 3584 4 4096 False torch.bfloat16 >>> Total hipb solutions 2154\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283188  0.016979\n",
      "621283530  0.019845\n",
      "621283384  0.020487\n",
      "621283294  0.020512\n",
      "621283316  0.020647\n",
      "621283190  0.020727\n",
      "621283383  0.020767\n",
      "621283346  0.020787\n",
      "621283347  0.020887\n",
      "621283287  0.020968\n",
      "621283208  0.020987\n",
      "621283305  0.021087\n",
      "621283184  0.021108\n",
      "621283191  0.021128\n",
      "621286473  0.021328\n",
      "621283182  0.021468\n",
      "621283328  0.021489\n",
      "621283313  0.021489\n",
      "621286462  0.021549\n",
      "621283306  0.021649\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.022030\n",
      "67808  0.024556\n",
      "67865  0.024656\n",
      "68230  0.025157\n",
      "67837  0.025338\n",
      "66939  0.025678\n",
      "13924  0.025718\n",
      "13922  0.026320\n",
      "13738  0.026601\n",
      "13778  0.026901\n",
      "66975  0.027342\n",
      "68443  0.027823\n",
      "13786  0.027983\n",
      "67757  0.028004\n",
      "13735  0.028024\n",
      "67746  0.028344\n",
      "67916  0.028445\n",
      "67694  0.028545\n",
      "13764  0.028545\n",
      "67895  0.028605\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283188  0.014799\n",
      "621283316  0.015896\n",
      "621283384  0.016251\n",
      "621283294  0.016452\n",
      "621283184  0.016525\n",
      "621283287  0.016611\n",
      "621283182  0.016642\n",
      "621283190  0.016790\n",
      "621283383  0.016798\n",
      "621286462  0.016842\n",
      "621286473  0.016894\n",
      "621283305  0.016940\n",
      "621283313  0.017021\n",
      "621283191  0.017065\n",
      "621283208  0.017093\n",
      "621283306  0.017319\n",
      "621283347  0.017720\n",
      "621283346  0.019624\n",
      "621283530  0.021503\n",
      "621283328  0.022704\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.017077\n",
      "67865  0.022393\n",
      "13922  0.022587\n",
      "67837  0.022657\n",
      "13924  0.023157\n",
      "67694  0.023343\n",
      "13778  0.023487\n",
      "67757  0.024093\n",
      "13786  0.024393\n",
      "67916  0.024708\n",
      "13764  0.024911\n",
      "13735  0.025145\n",
      "67895  0.025452\n",
      "13738  0.025452\n",
      "67746  0.027498\n",
      "67808  0.027725\n",
      "66939  0.029190\n",
      "66975  0.029647\n",
      "68443  0.030536\n",
      "68230  0.031259\n",
      ">>> Fastest Solution is rocblas 621283188 0.01479949951171875\n",
      "M N K dtype 4096 4 1792 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 4096 4 1792 False torch.bfloat16 >>> Total hipb solutions 2154\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283362  0.017199\n",
      "621283359  0.017500\n",
      "621283351  0.018101\n",
      "621283397  0.018963\n",
      "621283209  0.019064\n",
      "621283303  0.019103\n",
      "621283188  0.019384\n",
      "621283335  0.019645\n",
      "621283185  0.019664\n",
      "621283363  0.019705\n",
      "621283361  0.019965\n",
      "621283324  0.020186\n",
      "621283208  0.020206\n",
      "621283306  0.020226\n",
      "621283365  0.020306\n",
      "621283345  0.020366\n",
      "621283333  0.020366\n",
      "621283340  0.020386\n",
      "621283328  0.020466\n",
      "621283352  0.020506\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.014092\n",
      "13782  0.024134\n",
      "67973  0.024916\n",
      "67919  0.024997\n",
      "67952  0.025017\n",
      "67262  0.025117\n",
      "67592  0.025217\n",
      "13735  0.025819\n",
      "67863  0.026139\n",
      "67261  0.026360\n",
      "67469  0.026379\n",
      "67856  0.026440\n",
      "67833  0.026620\n",
      "66860  0.026620\n",
      "67260  0.026680\n",
      "67825  0.026701\n",
      "67405  0.026720\n",
      "13732  0.026741\n",
      "67879  0.026901\n",
      "67885  0.026961\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283351  0.016507\n",
      "621283306  0.016642\n",
      "621283335  0.016790\n",
      "621283185  0.016840\n",
      "621283324  0.016912\n",
      "621283363  0.016936\n",
      "621283333  0.016987\n",
      "621283209  0.017071\n",
      "621283208  0.017087\n",
      "621283303  0.017209\n",
      "621283352  0.017309\n",
      "621283340  0.017335\n",
      "621283328  0.017476\n",
      "621283188  0.017710\n",
      "621283345  0.017937\n",
      "621283365  0.017985\n",
      "621283361  0.019077\n",
      "621283362  0.019117\n",
      "621283397  0.019943\n",
      "621283359  0.020230\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.011446\n",
      "67952  0.021653\n",
      "67592  0.021805\n",
      "67863  0.022218\n",
      "67405  0.022307\n",
      "67262  0.022473\n",
      "67260  0.022707\n",
      "67833  0.022712\n",
      "67261  0.022762\n",
      "13732  0.022766\n",
      "67919  0.022910\n",
      "67879  0.022968\n",
      "67885  0.023343\n",
      "67973  0.023561\n",
      "13735  0.023828\n",
      "67856  0.023882\n",
      "67825  0.024067\n",
      "67469  0.026867\n",
      "66860  0.027106\n",
      "13782  0.031145\n",
      ">>> Fastest Solution is hipblaslt 13830 0.011445949971675872\n",
      "M N K dtype 768 2 4096 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 768 2 4096 False torch.bfloat16 >>> Total hipb solutions 2154\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283345  0.016417\n",
      "621283350  0.016839\n",
      "621283344  0.017500\n",
      "621283346  0.018602\n",
      "621283307  0.019584\n",
      "621283226  0.019624\n",
      "621283245  0.020025\n",
      "621283311  0.020146\n",
      "621283530  0.020226\n",
      "621283298  0.020226\n",
      "621283295  0.020366\n",
      "621283187  0.020386\n",
      "621283290  0.020406\n",
      "621283289  0.020467\n",
      "621283227  0.020586\n",
      "621283303  0.020606\n",
      "621283228  0.020646\n",
      "621286467  0.020646\n",
      "621283317  0.020667\n",
      "621283225  0.020667\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.018361\n",
      "67746  0.024335\n",
      "67757  0.024596\n",
      "67678  0.024676\n",
      "13779  0.024796\n",
      "13790  0.025056\n",
      "67808  0.025257\n",
      "13922  0.025698\n",
      "67865  0.025798\n",
      "67837  0.026560\n",
      "67043  0.026561\n",
      "67895  0.026781\n",
      "13921  0.026901\n",
      "67797  0.026981\n",
      "67738  0.027001\n",
      "13755  0.027182\n",
      "67116  0.027282\n",
      "67004  0.027482\n",
      "66979  0.027643\n",
      "67916  0.027663\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.015575\n",
      "621283227  0.015846\n",
      "621283311  0.016586\n",
      "621286467  0.016644\n",
      "621283295  0.016668\n",
      "621283187  0.016706\n",
      "621283228  0.016734\n",
      "621283245  0.016792\n",
      "621283225  0.016806\n",
      "621283317  0.016969\n",
      "621283298  0.017143\n",
      "621283289  0.017149\n",
      "621283303  0.017231\n",
      "621283290  0.017325\n",
      "621283344  0.018584\n",
      "621283346  0.018618\n",
      "621283226  0.019023\n",
      "621283307  0.019685\n",
      "621283350  0.022194\n",
      "621283345  0.023331\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.013994\n",
      "67837  0.020833\n",
      "67808  0.021184\n",
      "13922  0.021769\n",
      "67895  0.021892\n",
      "13779  0.021926\n",
      "13790  0.022058\n",
      "13921  0.022385\n",
      "67757  0.022571\n",
      "67865  0.022625\n",
      "67116  0.022962\n",
      "67797  0.023066\n",
      "67738  0.023199\n",
      "67004  0.023347\n",
      "67916  0.023369\n",
      "13755  0.023453\n",
      "67043  0.023491\n",
      "67678  0.024514\n",
      "66979  0.024652\n",
      "67746  0.030200\n",
      ">>> Fastest Solution is hipblaslt 13830 0.013993750512599944\n",
      "M N K dtype 4096 2 512 torch.bfloat16 >>> Total rocb solutions 524\n",
      "M N K bias dtype 4096 2 512 False torch.bfloat16 >>> Total hipb solutions 1548\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283410  0.017961\n",
      "621283540  0.018422\n",
      "621283656  0.018502\n",
      "621283371  0.018783\n",
      "621283564  0.019243\n",
      "621283404  0.019283\n",
      "621283620  0.019383\n",
      "621283418  0.019383\n",
      "621283411  0.019664\n",
      "621283380  0.019865\n",
      "621283414  0.019945\n",
      "621283379  0.019965\n",
      "621283416  0.020086\n",
      "621283346  0.020105\n",
      "621283353  0.020106\n",
      "621283208  0.020165\n",
      "621283212  0.020186\n",
      "621283541  0.020206\n",
      "621283385  0.020246\n",
      "621283319  0.020266\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.011005\n",
      "67280  0.022932\n",
      "68893  0.022952\n",
      "67086  0.023753\n",
      "13772  0.023894\n",
      "67361  0.023975\n",
      "67566  0.024015\n",
      "66879  0.024094\n",
      "66880  0.024215\n",
      "66868  0.024335\n",
      "68873  0.024355\n",
      "66854  0.024475\n",
      "66923  0.024495\n",
      "66899  0.024696\n",
      "67352  0.024836\n",
      "67410  0.024876\n",
      "66855  0.024976\n",
      "67162  0.025037\n",
      "67501  0.025177\n",
      "67877  0.025237\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283379  0.015836\n",
      "621283656  0.016325\n",
      "621283416  0.016525\n",
      "621283380  0.016696\n",
      "621283620  0.016756\n",
      "621283208  0.016846\n",
      "621283319  0.016924\n",
      "621283404  0.016962\n",
      "621283418  0.016966\n",
      "621283414  0.016989\n",
      "621283212  0.017163\n",
      "621283541  0.017203\n",
      "621283411  0.017285\n",
      "621283353  0.017289\n",
      "621283385  0.017317\n",
      "621283371  0.017412\n",
      "621283346  0.018045\n",
      "621283564  0.018145\n",
      "621283540  0.019324\n",
      "621283410  0.022575\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.007575\n",
      "67566  0.021038\n",
      "66880  0.021340\n",
      "67162  0.021545\n",
      "66899  0.022303\n",
      "66923  0.022307\n",
      "68893  0.022433\n",
      "67361  0.022437\n",
      "67501  0.022479\n",
      "68873  0.022667\n",
      "67280  0.022758\n",
      "66854  0.022808\n",
      "66855  0.022826\n",
      "67877  0.022908\n",
      "67086  0.022954\n",
      "66879  0.022984\n",
      "66868  0.023010\n",
      "67410  0.023030\n",
      "67352  0.023138\n",
      "13772  0.023217\n",
      ">>> Fastest Solution is hipblaslt 13830 0.007575199753046036\n",
      "M N K dtype 3584 2 4096 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 3584 2 4096 False torch.bfloat16 >>> Total hipb solutions 2154\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283347  0.020446\n",
      "621283186  0.020627\n",
      "621283293  0.020867\n",
      "621286471  0.020887\n",
      "621283530  0.021048\n",
      "621283212  0.021248\n",
      "621283224  0.021288\n",
      "621283322  0.021328\n",
      "621283310  0.021368\n",
      "621283298  0.021389\n",
      "621286476  0.021508\n",
      "621283204  0.021549\n",
      "621283316  0.021549\n",
      "621286463  0.021589\n",
      "621283384  0.021589\n",
      "621283294  0.021614\n",
      "621283345  0.021649\n",
      "621286475  0.021650\n",
      "621283346  0.021669\n",
      "621283213  0.021709\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.020686\n",
      "13735  0.025538\n",
      "67865  0.025638\n",
      "13921  0.025739\n",
      "67837  0.025779\n",
      "13922  0.025979\n",
      "67808  0.026199\n",
      "13738  0.026821\n",
      "13778  0.026901\n",
      "13746  0.026981\n",
      "68164  0.027201\n",
      "67916  0.027462\n",
      "13790  0.027462\n",
      "67757  0.027682\n",
      "13764  0.027983\n",
      "67797  0.028064\n",
      "67895  0.028505\n",
      "67738  0.028705\n",
      "13779  0.028825\n",
      "67694  0.028865\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283316  0.016363\n",
      "621283212  0.016513\n",
      "621283322  0.016544\n",
      "621283530  0.016616\n",
      "621283204  0.016698\n",
      "621286475  0.016710\n",
      "621283224  0.016894\n",
      "621283294  0.016910\n",
      "621283213  0.016966\n",
      "621283384  0.016971\n",
      "621283186  0.017143\n",
      "621283298  0.017157\n",
      "621283310  0.017173\n",
      "621286471  0.017203\n",
      "621283293  0.017203\n",
      "621286476  0.017452\n",
      "621286463  0.017915\n",
      "621283346  0.018354\n",
      "621283345  0.018598\n",
      "621283347  0.021687\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.016978\n",
      "13922  0.021735\n",
      "67808  0.022303\n",
      "67837  0.022882\n",
      "13921  0.022894\n",
      "67797  0.023012\n",
      "67865  0.023024\n",
      "67694  0.023179\n",
      "13778  0.023595\n",
      "67757  0.023954\n",
      "67738  0.024467\n",
      "13764  0.024526\n",
      "13738  0.024594\n",
      "13779  0.024694\n",
      "13746  0.024935\n",
      "67916  0.025031\n",
      "67895  0.025500\n",
      "13790  0.025833\n",
      "13735  0.029278\n",
      "68164  0.030958\n",
      ">>> Fastest Solution is rocblas 621283316 0.016363100707530977\n",
      "M N K dtype 4096 2 1792 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 4096 2 1792 False torch.bfloat16 >>> Total hipb solutions 2154\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283359  0.017360\n",
      "621283362  0.019124\n",
      "621283310  0.019384\n",
      "621283351  0.019444\n",
      "621283363  0.019745\n",
      "621283365  0.019805\n",
      "621283212  0.020226\n",
      "621283336  0.020266\n",
      "621283384  0.020406\n",
      "621283204  0.020427\n",
      "621283186  0.020446\n",
      "621283185  0.020467\n",
      "621286474  0.020487\n",
      "621283335  0.020546\n",
      "621286476  0.020606\n",
      "621283308  0.020667\n",
      "621283203  0.020687\n",
      "621283224  0.020787\n",
      "621283319  0.020827\n",
      "621286472  0.020867\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13830  0.013851\n",
      "67505  0.024235\n",
      "67856  0.024276\n",
      "67952  0.024836\n",
      "67592  0.024936\n",
      "13735  0.025518\n",
      "67262  0.025578\n",
      "13781  0.025739\n",
      "67869  0.025798\n",
      "67879  0.025919\n",
      "13759  0.026440\n",
      "67863  0.026520\n",
      "67973  0.026741\n",
      "67817  0.026741\n",
      "67972  0.026781\n",
      "13770  0.026801\n",
      "67849  0.026821\n",
      "67118  0.026841\n",
      "67692  0.026881\n",
      "13750  0.027001\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283351  0.015597\n",
      "621283185  0.016253\n",
      "621286474  0.016596\n",
      "621283212  0.016640\n",
      "621286476  0.016748\n",
      "621283310  0.016754\n",
      "621283384  0.016874\n",
      "621283203  0.016884\n",
      "621283319  0.016918\n",
      "621283335  0.016932\n",
      "621283186  0.016944\n",
      "621283204  0.016997\n",
      "621283363  0.017035\n",
      "621286472  0.017179\n",
      "621283308  0.017265\n",
      "621283224  0.017305\n",
      "621283365  0.017562\n",
      "621283362  0.018177\n",
      "621283336  0.018219\n",
      "621283359  0.020855\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13830  0.011697\n",
      "67856  0.021004\n",
      "67952  0.022166\n",
      "67592  0.022329\n",
      "67262  0.022439\n",
      "67973  0.022593\n",
      "67879  0.022681\n",
      "13759  0.022758\n",
      "67817  0.022992\n",
      "67869  0.023062\n",
      "13735  0.023110\n",
      "13750  0.023173\n",
      "67849  0.023233\n",
      "67118  0.023305\n",
      "13770  0.023361\n",
      "67972  0.023610\n",
      "67692  0.023748\n",
      "67863  0.024033\n",
      "67505  0.024632\n",
      "13781  0.026783\n",
      ">>> Fastest Solution is hipblaslt 13830 0.011696500331163406\n",
      "M N K dtype 768 1 4096 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 768 1 4096 False torch.bfloat16 >>> Total hipb solutions 2152\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283346  0.016136\n",
      "621283344  0.016618\n",
      "621283350  0.016919\n",
      "621286476  0.018402\n",
      "621283307  0.019003\n",
      "621283227  0.019464\n",
      "621283345  0.019865\n",
      "621286472  0.019965\n",
      "621283216  0.020086\n",
      "621283188  0.020105\n",
      "621283530  0.020165\n",
      "621283347  0.020205\n",
      "621283310  0.020387\n",
      "621283321  0.020467\n",
      "621283187  0.020506\n",
      "621283311  0.020546\n",
      "621283302  0.020567\n",
      "621283208  0.020627\n",
      "621283299  0.020647\n",
      "621283295  0.020667\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13831  0.017320\n",
      "67757  0.023534\n",
      "67808  0.023734\n",
      "67837  0.023834\n",
      "67746  0.024095\n",
      "67865  0.024275\n",
      "67678  0.024475\n",
      "67895  0.024676\n",
      "13790  0.024756\n",
      "67797  0.025317\n",
      "13779  0.025678\n",
      "13922  0.025999\n",
      "13924  0.026139\n",
      "67916  0.026279\n",
      "13750  0.026380\n",
      "67004  0.026480\n",
      "67043  0.026620\n",
      "13743  0.026701\n",
      "13746  0.026801\n",
      "66962  0.026881\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283530  0.015738\n",
      "621283208  0.016708\n",
      "621283345  0.016762\n",
      "621283216  0.016995\n",
      "621286472  0.016999\n",
      "621283311  0.017055\n",
      "621283188  0.017161\n",
      "621283295  0.017263\n",
      "621283347  0.017335\n",
      "621283321  0.017339\n",
      "621283187  0.017369\n",
      "621283310  0.017478\n",
      "621286476  0.017686\n",
      "621283302  0.017790\n",
      "621283299  0.017876\n",
      "621283227  0.017963\n",
      "621283307  0.018093\n",
      "621283350  0.019891\n",
      "621283344  0.021014\n",
      "621283346  0.022403\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13831  0.014070\n",
      "67808  0.020498\n",
      "67837  0.021675\n",
      "13790  0.021844\n",
      "13779  0.021860\n",
      "67678  0.021876\n",
      "13746  0.021882\n",
      "67895  0.021900\n",
      "13924  0.022176\n",
      "67746  0.022208\n",
      "13922  0.022511\n",
      "67865  0.022860\n",
      "67797  0.023098\n",
      "67004  0.023261\n",
      "13743  0.023283\n",
      "67916  0.023315\n",
      "67043  0.023491\n",
      "66962  0.025153\n",
      "13750  0.025418\n",
      "67757  0.026861\n",
      ">>> Fastest Solution is hipblaslt 13831 0.014069899916648865\n",
      "M N K dtype 4096 1 512 torch.bfloat16 >>> Total rocb solutions 524\n",
      "M N K bias dtype 4096 1 512 False torch.bfloat16 >>> Total hipb solutions 1546\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283417  0.018502\n",
      "621283410  0.019183\n",
      "621283540  0.019183\n",
      "621283620  0.019344\n",
      "621283564  0.019444\n",
      "621283411  0.019504\n",
      "621283418  0.019705\n",
      "621283320  0.019764\n",
      "621283373  0.019905\n",
      "621283469  0.019925\n",
      "621283371  0.019965\n",
      "621283415  0.019965\n",
      "621283414  0.020086\n",
      "621283203  0.020165\n",
      "621283191  0.020186\n",
      "621286476  0.020286\n",
      "621283230  0.020306\n",
      "621283541  0.020326\n",
      "621283336  0.020346\n",
      "621283297  0.020366\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13831  0.010304\n",
      "66879  0.023493\n",
      "66855  0.023653\n",
      "67280  0.023734\n",
      "67527  0.023914\n",
      "66899  0.023954\n",
      "67602  0.024055\n",
      "68873  0.024134\n",
      "66919  0.024195\n",
      "67410  0.024276\n",
      "66868  0.024355\n",
      "13773  0.024376\n",
      "13928  0.024636\n",
      "66923  0.024757\n",
      "67104  0.024836\n",
      "68102  0.025017\n",
      "67244  0.025057\n",
      "67086  0.025117\n",
      "67162  0.025197\n",
      "66913  0.025237\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283540  0.015549\n",
      "621283418  0.016131\n",
      "621283469  0.016159\n",
      "621283371  0.016427\n",
      "621283411  0.016445\n",
      "621283203  0.016882\n",
      "621283620  0.016936\n",
      "621283564  0.016971\n",
      "621283320  0.016984\n",
      "621283297  0.017003\n",
      "621286476  0.017049\n",
      "621283541  0.017173\n",
      "621283373  0.017454\n",
      "621283230  0.017504\n",
      "621283336  0.017520\n",
      "621283191  0.017704\n",
      "621283414  0.017782\n",
      "621283415  0.017792\n",
      "621283410  0.019901\n",
      "621283417  0.026416\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13831  0.006320\n",
      "66923  0.021371\n",
      "67162  0.021384\n",
      "67602  0.021386\n",
      "68102  0.021493\n",
      "67244  0.021555\n",
      "66899  0.021651\n",
      "67086  0.021775\n",
      "67104  0.022098\n",
      "66919  0.022154\n",
      "68873  0.022363\n",
      "13928  0.022461\n",
      "66868  0.022661\n",
      "13773  0.023012\n",
      "67410  0.023042\n",
      "66913  0.023156\n",
      "66855  0.023163\n",
      "67527  0.023379\n",
      "67280  0.023594\n",
      "66879  0.024688\n",
      ">>> Fastest Solution is hipblaslt 13831 0.006320349872112274\n",
      "M N K dtype 3584 1 4096 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 3584 1 4096 False torch.bfloat16 >>> Total hipb solutions 2152\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286470  0.019183\n",
      "621283393  0.019364\n",
      "621283308  0.020506\n",
      "621283347  0.020687\n",
      "621283385  0.020747\n",
      "621286466  0.020867\n",
      "621283187  0.021008\n",
      "621283384  0.021048\n",
      "621283323  0.021108\n",
      "621283184  0.021148\n",
      "621283345  0.021188\n",
      "621283322  0.021268\n",
      "621283212  0.021268\n",
      "621283225  0.021368\n",
      "621283224  0.021428\n",
      "621283292  0.021509\n",
      "621286469  0.021569\n",
      "621283530  0.021589\n",
      "621286476  0.021650\n",
      "621283346  0.021690\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13831  0.019825\n",
      "67865  0.024796\n",
      "67808  0.025357\n",
      "13924  0.026059\n",
      "67895  0.026320\n",
      "67837  0.026560\n",
      "13764  0.026820\n",
      "13779  0.026921\n",
      "13922  0.027362\n",
      "13755  0.027523\n",
      "67757  0.027943\n",
      "67916  0.028023\n",
      "13738  0.028344\n",
      "13746  0.028344\n",
      "13735  0.028384\n",
      "67797  0.028725\n",
      "13921  0.028965\n",
      "67631  0.029166\n",
      "13778  0.029446\n",
      "13745  0.029446\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283308  0.016054\n",
      "621283322  0.016066\n",
      "621283292  0.016622\n",
      "621283530  0.016632\n",
      "621283212  0.016720\n",
      "621283224  0.016806\n",
      "621286469  0.016878\n",
      "621286476  0.016882\n",
      "621283184  0.017127\n",
      "621283225  0.017203\n",
      "621283323  0.017213\n",
      "621283187  0.017352\n",
      "621283385  0.017496\n",
      "621286466  0.017714\n",
      "621283384  0.017754\n",
      "621283347  0.018300\n",
      "621283345  0.018534\n",
      "621283346  0.018706\n",
      "621286470  0.022114\n",
      "621283393  0.024828\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13831  0.015395\n",
      "13921  0.021982\n",
      "13924  0.022224\n",
      "13922  0.022331\n",
      "67837  0.022623\n",
      "13778  0.022738\n",
      "67631  0.022912\n",
      "13755  0.023489\n",
      "67808  0.023587\n",
      "67797  0.023740\n",
      "13738  0.023808\n",
      "67757  0.023890\n",
      "13745  0.023986\n",
      "13764  0.024021\n",
      "13735  0.024221\n",
      "67916  0.024696\n",
      "13746  0.025063\n",
      "67895  0.025189\n",
      "13779  0.025253\n",
      "67865  0.026388\n",
      ">>> Fastest Solution is hipblaslt 13831 0.015394900739192963\n",
      "M N K dtype 4096 1 1792 torch.bfloat16 >>> Total rocb solutions 529\n",
      "M N K bias dtype 4096 1 1792 False torch.bfloat16 >>> Total hipb solutions 2152\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283300  0.018081\n",
      "621283360  0.018843\n",
      "621283362  0.018923\n",
      "621283222  0.019183\n",
      "621283340  0.019344\n",
      "621283533  0.019404\n",
      "621283321  0.019705\n",
      "621283232  0.019785\n",
      "621283363  0.019965\n",
      "621283351  0.020105\n",
      "621283531  0.020186\n",
      "621283245  0.020226\n",
      "621283327  0.020427\n",
      "621283333  0.020427\n",
      "621283344  0.020427\n",
      "621283335  0.020446\n",
      "621283384  0.020467\n",
      "621283292  0.020506\n",
      "621286471  0.020567\n",
      "621286470  0.020627\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13831  0.013611\n",
      "67952  0.024054\n",
      "67856  0.024255\n",
      "67757  0.024636\n",
      "67262  0.025076\n",
      "67592  0.025277\n",
      "67755  0.025298\n",
      "67505  0.025457\n",
      "67973  0.025678\n",
      "67879  0.025779\n",
      "67777  0.025819\n",
      "67863  0.025879\n",
      "67692  0.025979\n",
      "13739  0.026119\n",
      "67869  0.026239\n",
      "67972  0.026279\n",
      "67504  0.026440\n",
      "67865  0.026601\n",
      "67261  0.026721\n",
      "67825  0.026881\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283300  0.008930\n",
      "621283351  0.016044\n",
      "621283363  0.016397\n",
      "621283384  0.016858\n",
      "621283333  0.016866\n",
      "621286470  0.016868\n",
      "621283232  0.016872\n",
      "621283327  0.016874\n",
      "621283362  0.016966\n",
      "621283222  0.017005\n",
      "621283292  0.017017\n",
      "621286471  0.017039\n",
      "621283321  0.017069\n",
      "621283335  0.017093\n",
      "621283340  0.017311\n",
      "621283245  0.017375\n",
      "621283344  0.017620\n",
      "621283531  0.017678\n",
      "621283533  0.019538\n",
      "621283360  0.022675\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13831  0.009951\n",
      "67856  0.021789\n",
      "67592  0.021962\n",
      "67504  0.022531\n",
      "67973  0.022537\n",
      "67261  0.022722\n",
      "67863  0.022786\n",
      "67879  0.022864\n",
      "67262  0.022990\n",
      "67777  0.023133\n",
      "13739  0.023253\n",
      "67757  0.023297\n",
      "67972  0.023455\n",
      "67825  0.023461\n",
      "67692  0.023485\n",
      "67755  0.023487\n",
      "67869  0.023573\n",
      "67865  0.023602\n",
      "67505  0.024692\n",
      "67952  0.026747\n",
      ">>> Fastest Solution is rocblas 621283300 0.008930200338363647\n",
      "M N K dtype 768 32768 4096 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 768 32768 4096 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283631  0.346465\n",
      "621283625  0.351638\n",
      "621283639  0.361726\n",
      "621283634  0.389223\n",
      "621283552  0.391809\n",
      "621283437  0.393733\n",
      "621283655  0.399727\n",
      "621283644  0.400728\n",
      "621283647  0.403275\n",
      "621283590  0.405700\n",
      "621283424  0.406041\n",
      "621283591  0.408085\n",
      "621283442  0.408687\n",
      "621283441  0.411253\n",
      "621283621  0.411453\n",
      "621283632  0.411613\n",
      "621283645  0.412416\n",
      "621283553  0.414039\n",
      "621283627  0.415642\n",
      "621283444  0.416364\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13887  0.392150\n",
      "13829  0.404678\n",
      "13919  0.406612\n",
      "13918  0.407925\n",
      "13826  0.410832\n",
      "67382  0.418429\n",
      "67255  0.419311\n",
      "13822  0.426006\n",
      "67621  0.430631\n",
      "13824  0.434686\n",
      "13891  0.434987\n",
      "67224  0.435573\n",
      "13823  0.437532\n",
      "13901  0.438745\n",
      "67298  0.439176\n",
      "13819  0.439276\n",
      "13878  0.439336\n",
      "67400  0.439898\n",
      "67585  0.440539\n",
      "67582  0.440955\n",
      ">>> rocblas Solidx 621283437 FAILED reference test\n",
      ">>> rocblas Solidx 621283655 FAILED reference test\n",
      ">>> rocblas Solidx 621283442 FAILED reference test\n",
      ">>> rocblas Solidx 621283632 FAILED reference test\n",
      ">>> rocblas Solidx 621283645 FAILED reference test\n",
      ">>> rocblas Solidx 621283553 FAILED reference test\n",
      ">>> rocblas Solidx 621283444 FAILED reference test\n",
      ">>> hipblaslt Solidx 13829 FAILED reference test\n",
      ">>> hipblaslt Solidx 13918 FAILED reference test\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283625  0.379843\n",
      "621283639  0.381297\n",
      "621283631  0.390161\n",
      "621283627  0.398157\n",
      "621283552  0.400813\n",
      "621283644  0.404030\n",
      "621283441  0.405415\n",
      "621283634  0.412095\n",
      "621283621  0.424304\n",
      "621283591  0.426964\n",
      "621283647  0.433369\n",
      "621283424  0.440706\n",
      "621283590  0.447705\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13919  0.433716\n",
      "13824  0.434858\n",
      "67224  0.435333\n",
      "13819  0.436211\n",
      "67255  0.436237\n",
      "67382  0.436889\n",
      "67298  0.436947\n",
      "67621  0.438755\n",
      "13823  0.446342\n",
      "67582  0.446841\n",
      "13826  0.447645\n",
      "13887  0.450101\n",
      "13891  0.454691\n",
      "67585  0.457700\n",
      "67400  0.460324\n",
      "13878  0.463758\n",
      "13822  0.474909\n",
      "13901  0.475422\n",
      ">>> Fastest Solution is rocblas 621283625 0.3798434972763062\n",
      "M N K dtype 4096 32768 512 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 4096 32768 512 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283267  0.294899\n",
      "621283522  0.297134\n",
      "621283560  0.297194\n",
      "621283519  0.304390\n",
      "621283453  0.306415\n",
      "621283157  0.307999\n",
      "621283160  0.308039\n",
      "621283627  0.308059\n",
      "621283561  0.310003\n",
      "621283614  0.312248\n",
      "621283433  0.314312\n",
      "621283430  0.314493\n",
      "621283591  0.314694\n",
      "621283454  0.314874\n",
      "621286441  0.315175\n",
      "621286437  0.315776\n",
      "621283459  0.315856\n",
      "621283284  0.316433\n",
      "621283487  0.317019\n",
      "621283262  0.318177\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13874  0.298618\n",
      "13824  0.298717\n",
      "13869  0.301003\n",
      "13823  0.301564\n",
      "13829  0.307077\n",
      "13828  0.308565\n",
      "13737  0.319986\n",
      "13861  0.321308\n",
      "13854  0.322351\n",
      "13862  0.324531\n",
      "13888  0.329583\n",
      "13872  0.333676\n",
      "13760  0.335200\n",
      "13792  0.335420\n",
      "13820  0.341444\n",
      "13878  0.343539\n",
      "13868  0.347804\n",
      "13903  0.347904\n",
      "66934  0.351236\n",
      "13825  0.355927\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621286441  0.304527\n",
      "621283627  0.305014\n",
      "621283591  0.308963\n",
      "621283157  0.311314\n",
      "621283519  0.315385\n",
      "621283614  0.315416\n",
      "621283561  0.315494\n",
      "621283487  0.315682\n",
      "621283522  0.318007\n",
      "621283284  0.320118\n",
      "621283430  0.321317\n",
      "621283560  0.322351\n",
      "621283453  0.323917\n",
      "621283433  0.324189\n",
      "621283454  0.325823\n",
      "621283262  0.328726\n",
      "621283459  0.329944\n",
      "621283267  0.330802\n",
      "621286437  0.334334\n",
      "621283160  0.336052\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13829  0.306200\n",
      "13869  0.307808\n",
      "13823  0.311853\n",
      "13824  0.312799\n",
      "13737  0.318041\n",
      "13888  0.319022\n",
      "13828  0.319088\n",
      "13854  0.321285\n",
      "13874  0.321770\n",
      "13862  0.323466\n",
      "13903  0.324426\n",
      "13820  0.325121\n",
      "13878  0.328012\n",
      "13760  0.328758\n",
      "13861  0.331718\n",
      "13792  0.334420\n",
      "13868  0.336986\n",
      "13872  0.337634\n",
      "13825  0.340065\n",
      "66934  0.370655\n",
      ">>> Fastest Solution is rocblas 621286441 0.3045266389846802\n",
      "M N K dtype 3584 32768 4096 torch.bfloat16 >>> Total rocb solutions 390\n",
      "M N K bias dtype 3584 32768 4096 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286477  1.629315\n",
      "621283628  1.632302\n",
      "621283200  1.635128\n",
      "621283443  1.641543\n",
      "621283196  1.644209\n",
      "621283437  1.650223\n",
      "621283554  1.652869\n",
      "621283444  1.655394\n",
      "621286461  1.656257\n",
      "621283553  1.657680\n",
      "621283439  1.659384\n",
      "621283556  1.660867\n",
      "621283636  1.661448\n",
      "621283197  1.662851\n",
      "621283627  1.667663\n",
      "621283634  1.672454\n",
      "621283632  1.674397\n",
      "621283637  1.678447\n",
      "621283442  1.681674\n",
      "621283641  1.684481\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13900  1.648544\n",
      "13874  1.680812\n",
      "13908  1.683368\n",
      "13821  1.734233\n",
      "13840  1.737040\n",
      "13828  1.751572\n",
      "13869  1.773903\n",
      "13792  1.779456\n",
      "13888  1.785770\n",
      "13822  1.808381\n",
      "13758  1.812271\n",
      "13829  1.812992\n",
      "13837  1.815237\n",
      "13832  1.821692\n",
      "13836  1.825841\n",
      "13918  1.826683\n",
      "13826  1.828788\n",
      "67627  1.831234\n",
      "13823  1.831514\n",
      "13903  1.833418\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621286477  1.641535\n",
      "621283634  1.641583\n",
      "621283553  1.646987\n",
      "621283443  1.647017\n",
      "621283627  1.648439\n",
      "621283200  1.649778\n",
      "621283632  1.652985\n",
      "621283444  1.653633\n",
      "621283437  1.654019\n",
      "621283628  1.654198\n",
      "621283439  1.656401\n",
      "621283556  1.665782\n",
      "621283554  1.666414\n",
      "621283196  1.666794\n",
      "621283637  1.667737\n",
      "621286461  1.670266\n",
      "621283636  1.671537\n",
      "621283442  1.680147\n",
      "621283197  1.792048\n",
      "621283641  1.822123\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13874  1.670116\n",
      "13821  1.682931\n",
      "13792  1.689821\n",
      "13828  1.693072\n",
      "13822  1.706873\n",
      "13829  1.711197\n",
      "13869  1.716916\n",
      "13823  1.721452\n",
      "13837  1.726764\n",
      "13840  1.747764\n",
      "13832  1.755812\n",
      "13888  1.789579\n",
      "13758  1.801845\n",
      "13903  1.808145\n",
      "13908  1.809895\n",
      "13836  1.824875\n",
      "13900  1.841070\n",
      "67627  1.850463\n",
      "13826  1.869059\n",
      "13918  1.894898\n",
      ">>> Fastest Solution is rocblas 621286477 1.641535186767578\n",
      "M N K dtype 4096 32768 1792 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 4096 32768 1792 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283192  0.822505\n",
      "621283198  0.835154\n",
      "621286477  0.835796\n",
      "621283627  0.839965\n",
      "621286461  0.847261\n",
      "621283643  0.849206\n",
      "621283200  0.854418\n",
      "621283285  0.854719\n",
      "621283628  0.856382\n",
      "621283634  0.857244\n",
      "621283613  0.861995\n",
      "621283593  0.862516\n",
      "621283590  0.863900\n",
      "621283591  0.864461\n",
      "621283650  0.866786\n",
      "621283196  0.867067\n",
      "621283197  0.869613\n",
      "621283284  0.870334\n",
      "621283641  0.872840\n",
      "621283637  0.878874\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13828  0.849246\n",
      "13832  0.864821\n",
      "13869  0.866024\n",
      "13887  0.870655\n",
      "13874  0.874804\n",
      "13837  0.885468\n",
      "13862  0.892704\n",
      "13888  0.906717\n",
      "13726  0.909302\n",
      "13737  0.912004\n",
      "13824  0.920548\n",
      "13823  0.923474\n",
      "13836  0.925640\n",
      "13758  0.936343\n",
      "13829  0.936644\n",
      "13820  0.938549\n",
      "13918  0.940132\n",
      "13760  0.940814\n",
      "13840  0.942759\n",
      "13792  0.947810\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283634  0.852590\n",
      "621283192  0.855741\n",
      "621283198  0.858134\n",
      "621286477  0.858682\n",
      "621283643  0.859750\n",
      "621283200  0.862125\n",
      "621283627  0.864797\n",
      "621283196  0.865922\n",
      "621283197  0.867393\n",
      "621283628  0.868307\n",
      "621283285  0.871206\n",
      "621283591  0.871284\n",
      "621283593  0.871805\n",
      "621283284  0.875760\n",
      "621283637  0.876694\n",
      "621283590  0.878510\n",
      "621283650  0.881363\n",
      "621283613  0.883642\n",
      "621283641  0.885023\n",
      "621286461  0.892975\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13888  0.878216\n",
      "13828  0.879384\n",
      "13837  0.882850\n",
      "13869  0.884380\n",
      "13874  0.890319\n",
      "13829  0.891163\n",
      "13836  0.891612\n",
      "13726  0.903712\n",
      "13792  0.905011\n",
      "13862  0.905959\n",
      "13824  0.906721\n",
      "13887  0.908033\n",
      "13823  0.909960\n",
      "13760  0.911732\n",
      "13758  0.916759\n",
      "13737  0.916761\n",
      "13832  0.923276\n",
      "13840  0.933569\n",
      "13820  0.933722\n",
      "13918  0.970664\n",
      ">>> Fastest Solution is rocblas 621283634 0.8525897979736328\n",
      "M N K dtype 768 29696 4096 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 768 29696 4096 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283625  0.339770\n",
      "621283639  0.343424\n",
      "621283583  0.344822\n",
      "621283646  0.350334\n",
      "621283434  0.353181\n",
      "621283428  0.353202\n",
      "621283472  0.353557\n",
      "621283566  0.354344\n",
      "621283549  0.354484\n",
      "621283584  0.354624\n",
      "621283605  0.355547\n",
      "621283551  0.356889\n",
      "621283429  0.357550\n",
      "621283433  0.360938\n",
      "621283631  0.362973\n",
      "621283432  0.366852\n",
      "621283507  0.371864\n",
      "621283473  0.372505\n",
      "621283430  0.375351\n",
      "621283554  0.378318\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "67338  0.380643\n",
      "13887  0.389423\n",
      "13918  0.389564\n",
      "13902  0.391603\n",
      "13901  0.393041\n",
      "13772  0.395757\n",
      "67616  0.396358\n",
      "67554  0.398344\n",
      "13919  0.402573\n",
      "13885  0.402894\n",
      "13829  0.403335\n",
      "13822  0.404758\n",
      "67436  0.408547\n",
      "13891  0.412816\n",
      "13824  0.413016\n",
      "13862  0.413558\n",
      "13826  0.414771\n",
      "67621  0.416505\n",
      "67527  0.417422\n",
      "13878  0.419672\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283631  0.356857\n",
      "621283433  0.362059\n",
      "621283639  0.366465\n",
      "621283430  0.367449\n",
      "621283428  0.367512\n",
      "621283434  0.368636\n",
      "621283473  0.371076\n",
      "621283432  0.376233\n",
      "621283625  0.377260\n",
      "621283549  0.379415\n",
      "621283507  0.383133\n",
      "621283554  0.383652\n",
      "621283429  0.384434\n",
      "621283583  0.384843\n",
      "621283584  0.384909\n",
      "621283566  0.386861\n",
      "621283646  0.388549\n",
      "621283551  0.389582\n",
      "621283605  0.390351\n",
      "621283472  0.390913\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13918  0.389549\n",
      "13826  0.398458\n",
      "13919  0.402952\n",
      "13902  0.405572\n",
      "67616  0.406722\n",
      "13829  0.409499\n",
      "13862  0.410563\n",
      "13887  0.410818\n",
      "67554  0.411375\n",
      "67338  0.412295\n",
      "13891  0.414015\n",
      "13772  0.415498\n",
      "67621  0.417088\n",
      "13901  0.418335\n",
      "13885  0.422635\n",
      "67527  0.424112\n",
      "13824  0.426968\n",
      "13878  0.429544\n",
      "67436  0.436125\n",
      "13822  0.442700\n",
      ">>> Fastest Solution is rocblas 621283631 0.3568573951721191\n",
      "M N K dtype 4096 29696 512 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 4096 29696 512 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283200  0.266665\n",
      "621283198  0.273821\n",
      "621283488  0.274042\n",
      "621283608  0.275184\n",
      "621283197  0.275349\n",
      "621283472  0.276146\n",
      "621283430  0.279073\n",
      "621283609  0.280336\n",
      "621283613  0.281113\n",
      "621283433  0.281563\n",
      "621283157  0.281659\n",
      "621283578  0.282015\n",
      "621283502  0.282681\n",
      "621283196  0.282746\n",
      "621283605  0.284696\n",
      "621283566  0.285828\n",
      "621283284  0.286124\n",
      "621283524  0.287292\n",
      "621283473  0.287793\n",
      "621283563  0.288119\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13874  0.257805\n",
      "13869  0.264560\n",
      "13828  0.280246\n",
      "13888  0.289391\n",
      "13878  0.292002\n",
      "13837  0.292128\n",
      "13838  0.296553\n",
      "13823  0.297474\n",
      "13760  0.300221\n",
      "13792  0.301203\n",
      "13872  0.303173\n",
      "13829  0.305382\n",
      "13903  0.305448\n",
      "13824  0.307477\n",
      "13826  0.308340\n",
      "13825  0.311096\n",
      "13871  0.312569\n",
      "13868  0.313025\n",
      "13861  0.315185\n",
      "13891  0.317560\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283197  0.266629\n",
      "621283473  0.277892\n",
      "621283502  0.278840\n",
      "621283472  0.279724\n",
      "621283196  0.281096\n",
      "621283284  0.281825\n",
      "621283578  0.282134\n",
      "621283613  0.283206\n",
      "621283433  0.283956\n",
      "621283608  0.284992\n",
      "621283609  0.287073\n",
      "621283430  0.287119\n",
      "621283566  0.289318\n",
      "621283563  0.289627\n",
      "621283200  0.290451\n",
      "621283605  0.291716\n",
      "621283198  0.297978\n",
      "621283488  0.298142\n",
      "621283524  0.313334\n",
      "621283157  0.315509\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13837  0.274679\n",
      "13874  0.282679\n",
      "13888  0.286163\n",
      "13869  0.287240\n",
      "13760  0.290846\n",
      "13878  0.292770\n",
      "13823  0.295202\n",
      "13828  0.295496\n",
      "13838  0.297773\n",
      "13792  0.299834\n",
      "13829  0.300692\n",
      "13868  0.301592\n",
      "13861  0.302340\n",
      "13903  0.306367\n",
      "13872  0.308308\n",
      "13825  0.311114\n",
      "13826  0.312850\n",
      "13824  0.313858\n",
      "13891  0.322457\n",
      "13871  0.324749\n",
      ">>> Fastest Solution is rocblas 621283197 0.2666287422180176\n",
      "M N K dtype 3584 29696 4096 torch.bfloat16 >>> Total rocb solutions 390\n",
      "M N K bias dtype 3584 29696 4096 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621286477  1.496634\n",
      "621286461  1.518203\n",
      "621283192  1.526302\n",
      "621283196  1.546508\n",
      "621283197  1.550176\n",
      "621283553  1.550517\n",
      "621283200  1.551659\n",
      "621283444  1.553163\n",
      "621283635  1.556972\n",
      "621283449  1.558876\n",
      "621283447  1.561522\n",
      "621283433  1.563365\n",
      "621283634  1.564849\n",
      "621283428  1.566072\n",
      "621283549  1.566152\n",
      "621283650  1.567134\n",
      "621283627  1.568658\n",
      "621283443  1.571183\n",
      "621283557  1.573488\n",
      "621283446  1.573830\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13908  1.571203\n",
      "13918  1.629014\n",
      "13828  1.634006\n",
      "13900  1.638240\n",
      "13822  1.638857\n",
      "13832  1.649241\n",
      "13792  1.653972\n",
      "13821  1.656036\n",
      "13874  1.658301\n",
      "13869  1.658522\n",
      "13824  1.659183\n",
      "13888  1.660426\n",
      "13840  1.664756\n",
      "13820  1.668765\n",
      "13837  1.669466\n",
      "13758  1.671792\n",
      "13819  1.673195\n",
      "13868  1.677806\n",
      "13903  1.679991\n",
      "13871  1.680591\n",
      ">>> rocblas Solidx 621286477 FAILED reference test\n",
      ">>> rocblas Solidx 621286461 FAILED reference test\n",
      ">>> rocblas Solidx 621283192 FAILED reference test\n",
      ">>> rocblas Solidx 621283196 FAILED reference test\n",
      ">>> rocblas Solidx 621283197 FAILED reference test\n",
      ">>> rocblas Solidx 621283200 FAILED reference test\n",
      ">>> rocblas Solidx 621283449 FAILED reference test\n",
      ">>> rocblas Solidx 621283433 FAILED reference test\n",
      ">>> rocblas Solidx 621283634 FAILED reference test\n",
      ">>> rocblas Solidx 621283428 FAILED reference test\n",
      ">>> rocblas Solidx 621283549 FAILED reference test\n",
      ">>> rocblas Solidx 621283627 FAILED reference test\n",
      ">>> rocblas Solidx 621283557 FAILED reference test\n",
      ">>> hipblaslt Solidx 13908 FAILED reference test\n",
      ">>> hipblaslt Solidx 13918 FAILED reference test\n",
      ">>> hipblaslt Solidx 13828 FAILED reference test\n",
      ">>> hipblaslt Solidx 13900 FAILED reference test\n",
      ">>> hipblaslt Solidx 13822 FAILED reference test\n",
      ">>> hipblaslt Solidx 13792 FAILED reference test\n",
      ">>> hipblaslt Solidx 13874 FAILED reference test\n",
      ">>> hipblaslt Solidx 13869 FAILED reference test\n",
      ">>> hipblaslt Solidx 13824 FAILED reference test\n",
      ">>> hipblaslt Solidx 13840 FAILED reference test\n",
      ">>> hipblaslt Solidx 13820 FAILED reference test\n",
      ">>> hipblaslt Solidx 13758 FAILED reference test\n",
      ">>> hipblaslt Solidx 13819 FAILED reference test\n",
      ">>> hipblaslt Solidx 13868 FAILED reference test\n",
      ">>> hipblaslt Solidx 13903 FAILED reference test\n",
      ">>> hipblaslt Solidx 13871 FAILED reference test\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283553  1.557041\n",
      "621283444  1.558892\n",
      "621283443  1.560467\n",
      "621283446  1.567223\n",
      "621283635  1.574122\n",
      "621283447  1.581619\n",
      "621283650  1.643327\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13821  1.589008\n",
      "13888  1.642319\n",
      "13837  1.671685\n",
      "13832  1.702012\n",
      ">>> Fastest Solution is rocblas 621283553 1.5570414543151856\n",
      "M N K dtype 4096 29696 1792 torch.bfloat16 >>> Total rocb solutions 388\n",
      "M N K bias dtype 4096 29696 1792 False torch.bfloat16 >>> Total hipb solutions 445\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283488  0.734846\n",
      "621283200  0.742684\n",
      "621283284  0.751344\n",
      "621283197  0.756155\n",
      "621283180  0.759683\n",
      "621283196  0.760925\n",
      "621283192  0.763151\n",
      "621283286  0.767380\n",
      "621286458  0.770222\n",
      "621286477  0.771770\n",
      "621283650  0.773334\n",
      "621286461  0.773915\n",
      "621283578  0.778185\n",
      "621283198  0.778385\n",
      "621283613  0.778867\n",
      "621283641  0.779809\n",
      "621283628  0.780791\n",
      "621283177  0.781031\n",
      "621283593  0.784699\n",
      "621283285  0.785501\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13874  0.772632\n",
      "13832  0.784319\n",
      "13869  0.788608\n",
      "13828  0.801659\n",
      "67589  0.805748\n",
      "13758  0.807070\n",
      "13836  0.810398\n",
      "13837  0.816773\n",
      "13918  0.820120\n",
      "13737  0.826194\n",
      "13792  0.830363\n",
      "13726  0.831245\n",
      "13887  0.835696\n",
      "13862  0.839585\n",
      "68900  0.842731\n",
      "13824  0.847402\n",
      "13871  0.848104\n",
      "13888  0.852413\n",
      "67255  0.854599\n",
      "13820  0.863840\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283192  0.762908\n",
      "621283198  0.767016\n",
      "621286477  0.767671\n",
      "621283196  0.771179\n",
      "621283197  0.773472\n",
      "621283200  0.773613\n",
      "621283284  0.777117\n",
      "621283628  0.778628\n",
      "621283593  0.779917\n",
      "621283285  0.781062\n",
      "621283286  0.781438\n",
      "621283650  0.785814\n",
      "621283613  0.786137\n",
      "621283578  0.787759\n",
      "621283488  0.792207\n",
      "621283641  0.793424\n",
      "621283180  0.799796\n",
      "621283177  0.805377\n",
      "621286461  0.809572\n",
      "621286458  0.810352\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13828  0.771231\n",
      "13888  0.782497\n",
      "13837  0.787179\n",
      "13869  0.787209\n",
      "13874  0.787217\n",
      "13836  0.789950\n",
      "13792  0.805996\n",
      "13832  0.814537\n",
      "68900  0.819593\n",
      "67589  0.829022\n",
      "13758  0.832598\n",
      "13820  0.853949\n",
      "13737  0.855957\n",
      "13871  0.861059\n",
      "13862  0.861328\n",
      "13726  0.861338\n",
      "13824  0.865924\n",
      "13887  0.865990\n",
      "13918  0.874517\n",
      "67255  0.893424\n",
      ">>> Fastest Solution is rocblas 621283192 0.7629083633422852\n",
      "M N K dtype 16032 232 4096 torch.bfloat16 >>> Total rocb solutions 431\n",
      "M N K bias dtype 16032 232 4096 False torch.bfloat16 >>> Total hipb solutions 1077\n",
      ">>> Rocblas top solutions, Fast Mode 1\n",
      "            gtimems\n",
      "621283375  0.107604\n",
      "621283374  0.108391\n",
      "621283377  0.109594\n",
      "621283376  0.110070\n",
      "621283540  0.111593\n",
      "621283536  0.112305\n",
      "621283412  0.113017\n",
      "621283537  0.113503\n",
      "621283538  0.115467\n",
      "621283408  0.117887\n",
      "621283364  0.118168\n",
      "621283418  0.118569\n",
      "621283339  0.118769\n",
      "621283406  0.119271\n",
      "621283378  0.119631\n",
      "621283405  0.120027\n",
      "621283363  0.120954\n",
      "621283539  0.120954\n",
      "621283410  0.121050\n",
      "621283415  0.121736\n",
      ">>> HipBlasLt top solutions, Fast Mode 1\n",
      "        gtimems\n",
      "13901  0.097812\n",
      "13902  0.107349\n",
      "13885  0.110350\n",
      "67410  0.111413\n",
      "13928  0.111433\n",
      "13854  0.113177\n",
      "67602  0.117206\n",
      "67544  0.117587\n",
      "67527  0.118830\n",
      "67142  0.119396\n",
      "67594  0.120278\n",
      "13756  0.121977\n",
      "13783  0.122447\n",
      "13891  0.122778\n",
      "13784  0.123250\n",
      "13860  0.123425\n",
      "67877  0.123846\n",
      "13851  0.125299\n",
      "67505  0.125791\n",
      "13772  0.126507\n",
      ">>> Rocblas top solutions, Fast Mode 0\n",
      "            gtimems\n",
      "621283536  0.092710\n",
      "621283375  0.093991\n",
      "621283376  0.095839\n",
      "621283377  0.096250\n",
      "621283537  0.096715\n",
      "621283406  0.097036\n",
      "621283374  0.098828\n",
      "621283408  0.098964\n",
      "621283405  0.102096\n",
      "621283538  0.103368\n",
      "621283540  0.111290\n",
      "621283410  0.114975\n",
      "621283363  0.116068\n",
      "621283364  0.116827\n",
      "621283418  0.119044\n",
      "621283412  0.119936\n",
      "621283339  0.120482\n",
      "621283378  0.123266\n",
      "621283539  0.124388\n",
      "621283415  0.126926\n",
      ">>> HipBlasLt top solutions, Fast Mode 0\n",
      "        gtimems\n",
      "13901  0.092454\n",
      "13885  0.092939\n",
      "13902  0.096415\n",
      "67142  0.101405\n",
      "13784  0.103346\n",
      "67877  0.105369\n",
      "67594  0.111485\n",
      "13783  0.111832\n",
      "13928  0.113353\n",
      "13854  0.116572\n",
      "13772  0.116841\n",
      "67505  0.117251\n",
      "67527  0.119192\n",
      "13756  0.119198\n",
      "13851  0.120388\n",
      "67410  0.120505\n",
      "67544  0.120782\n",
      "67602  0.122257\n",
      "13891  0.127706\n",
      "13860  0.131667\n",
      ">>> Fastest Solution is hipblaslt 13901 0.09245365262031555\n",
      "         M       N     K   bias  ...        outdtype    libtype     solidx  soltimes\n",
      "0      768  131072  4096  False  ...  torch.bfloat16    rocblas  621283627  1.549446\n",
      "1     4096  131072   512  False  ...  torch.bfloat16    rocblas  621283521   1.22748\n",
      "2     3584  131072  4096  False  ...  torch.bfloat16    rocblas  621283445   6.91003\n",
      "3     4096  131072  1792  False  ...  torch.bfloat16    rocblas  621283284  3.532712\n",
      "4    16032     256  4096  False  ...  torch.bfloat16    rocblas  621283537  0.094783\n",
      "..     ...     ...   ...    ...  ...             ...        ...        ...       ...\n",
      "149    768   29696  4096  False  ...  torch.bfloat16    rocblas  621283631  0.356857\n",
      "150   4096   29696   512  False  ...  torch.bfloat16    rocblas  621283197  0.266629\n",
      "151   3584   29696  4096  False  ...  torch.bfloat16    rocblas  621283553  1.557041\n",
      "152   4096   29696  1792  False  ...  torch.bfloat16    rocblas  621283192  0.762908\n",
      "153  16032     232  4096  False  ...  torch.bfloat16  hipblaslt      13901  0.092454\n",
      "\n",
      "[154 rows x 9 columns]\n",
      "\n",
      "real\t6m43.007s\n",
      "user\t4m10.791s\n",
      "sys\t4m26.698s\n"
     ]
    }
   ],
   "source": [
    "# GEMM Tuning\n",
    "!time python ~/vllm-rocm/gradlib/gradlib/gemm_tuner.py --input /tmp/vllm_untuned.csv --tuned_file tuned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaff5056-4f5c-4580-ab45-73b54284e299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 19:09:02 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=15, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 19:09:11 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-30 19:09:11 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-30 19:09:11 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 19:09:11 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=15, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 19:09:11 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 19:09:11 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 19:09:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "WARNING 10-30 19:09:12 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m WARNING 10-30 19:09:16 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m WARNING 10-30 19:09:16 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m WARNING 10-30 19:09:16 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m WARNING 10-30 19:09:16 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m WARNING 10-30 19:09:16 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m WARNING 10-30 19:09:16 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m WARNING 10-30 19:09:16 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 19:09:17 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7b2257edf450>, local_subscribe_port=40827, remote_subscribe_port=None)\n",
      "INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:17 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 19:09:18 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.37it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.88it/s]\n",
      "\n",
      "INFO 10-30 19:09:19 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:19 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:20 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:20 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:20 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:20 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:20 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:20 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 19:09:35 distributed_gpu_executor.py:57] # GPU blocks: 647379, # CPU blocks: 16384\n",
      "INFO 10-30 19:09:35 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 79.03x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 19:09:37 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 19:09:37 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:09:53 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:07<00:00, 141.46it/s, est. speed input:\n",
      "INFO 10-30 19:10:05 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223967)\u001b[0;0m INFO 10-30 19:10:05 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223968)\u001b[0;0m INFO 10-30 19:10:05 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223970)\u001b[0;0m INFO 10-30 19:10:05 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223973)\u001b[0;0m INFO 10-30 19:10:05 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223969)\u001b[0;0m INFO 10-30 19:10:05 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223972)\u001b[0;0m INFO 10-30 19:10:05 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3223971)\u001b[0;0m INFO 10-30 19:10:05 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 138.18 requests/s, 35372.99 tokens/s\n",
      "[rank0]:[W1030 19:10:07.262981316 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Test w/ tuning\n",
    "!VLLM_TUNE_FILE=\"$(pwd)/tuned.csv\" VLLM_TUNE_GEMM=0 VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --num-scheduler-steps 15 --model meta-llama/Llama-3.1-8B-Instruct --input-len 128 --output-len 128 -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f415c05b-ce39-4be2-b43d-fb583935bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 19:10:15 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=15, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 19:10:24 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-30 19:10:24 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 19:10:24 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=15, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-30 19:10:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "WARNING 10-30 19:10:25 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "INFO 10-30 19:10:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 10-30 19:10:25 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 19:10:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.10it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.38s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.77s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.94s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.73s/it]\n",
      "\n",
      "INFO 10-30 19:10:33 model_runner.py:1071] Loading model weights took 14.9888 GB\n",
      "INFO 10-30 19:10:49 gpu_executor.py:122] # GPU blocks: 71538, # CPU blocks: 2048\n",
      "INFO 10-30 19:10:49 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 8.73x\n",
      "INFO 10-30 19:10:50 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 19:10:50 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-30 19:10:57 model_runner.py:1530] Graph capturing finished in 7 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:15<00:00, 64.15it/s, est. speed input: \n",
      "Throughput: 63.48 requests/s, 16251.16 tokens/s\n",
      "[rank0]:[W1030 19:11:14.867587815 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Test w/ tuning - does this work w/ TP1?\n",
    "!VLLM_TUNE_FILE=\"$(pwd)/tuned.csv\" VLLM_TUNE_GEMM=0 VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --num-scheduler-steps 15 --model meta-llama/Llama-3.1-8B-Instruct --input-len 128 --output-len 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a2744d2-e8d7-44d7-925e-813ff48df5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 19:34:19 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=512, output_len=512, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=15, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 19:34:28 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-30 19:34:28 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-30 19:34:28 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 19:34:28 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=15, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 19:34:29 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 19:34:29 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 19:34:29 selector.py:120] Using ROCmFlashAttention backend.\n",
      "WARNING 10-30 19:34:29 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:33 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m WARNING 10-30 19:34:33 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:33 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:33 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:33 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m WARNING 10-30 19:34:33 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:33 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m WARNING 10-30 19:34:33 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:33 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:34 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m WARNING 10-30 19:34:34 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:34 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:34 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m WARNING 10-30 19:34:34 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:34 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m WARNING 10-30 19:34:34 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m WARNING 10-30 19:34:34 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:34 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 19:34:35 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7af930b08f10>, local_subscribe_port=56031, remote_subscribe_port=None)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:35 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 19:34:35 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  4.95it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.96it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.59it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.81it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:37 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 19:34:37 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:37 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:37 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:37 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:38 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:38 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:38 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 19:34:53 distributed_gpu_executor.py:57] # GPU blocks: 647379, # CPU blocks: 16384\n",
      "INFO 10-30 19:34:53 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 79.03x\n",
      "INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:34:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255648)\u001b[0;0m INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255650)\u001b[0;0m INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255644)\u001b[0;0m INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255645)\u001b[0;0m INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255647)\u001b[0;0m INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255646)\u001b[0;0m INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3255649)\u001b[0;0m INFO 10-30 19:35:10 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:28<00:00, 34.86it/s, est. speed input: \n",
      "INFO 10-30 19:35:45 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "Throughput: 34.43 requests/s, 35258.13 tokens/s\n",
      "[rank0]:[W1030 19:35:47.001647854 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Does this work with different in/ou?\n",
    "!VLLM_TUNE_FILE=\"$(pwd)/tuned.csv\" VLLM_TUNE_GEMM=0 VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --num-scheduler-steps 15 --model meta-llama/Llama-3.1-8B-Instruct --input-len 512 --output-len 512 -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6adcc48e-3fb3-4fc2-a334-ef707203a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-30 19:37:10 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=512, output_len=512, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=15, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-30 19:37:19 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-30 19:37:19 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "WARNING 10-30 19:37:19 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 10-30 19:37:19 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=15, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-30 19:37:19 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-30 19:37:19 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-30 19:37:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "WARNING 10-30 19:37:19 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m WARNING 10-30 19:37:24 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m WARNING 10-30 19:37:24 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m WARNING 10-30 19:37:24 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m WARNING 10-30 19:37:24 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m WARNING 10-30 19:37:24 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m WARNING 10-30 19:37:24 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m WARNING 10-30 19:37:24 registry.py:205] `mm_limits` has already been set for model=meta-llama/Llama-3.1-8B-Instruct, and will be overwritten by the new values.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:24 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:24 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-30 19:37:25 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7c342d679e90>, local_subscribe_port=53299, remote_subscribe_port=None)\n",
      "INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:25 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-30 19:37:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:27 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  5.08it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:27 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.97it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.84it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 19:37:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:28 model_runner.py:1071] Loading model weights took 1.9028 GB\n",
      "INFO 10-30 19:37:43 distributed_gpu_executor.py:57] # GPU blocks: 647379, # CPU blocks: 16384\n",
      "INFO 10-30 19:37:43 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 79.03x\n",
      "INFO 10-30 19:37:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-30 19:37:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:37:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:37:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:37:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:37:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260031)\u001b[0;0m INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260030)\u001b[0;0m INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260026)\u001b[0;0m INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260029)\u001b[0;0m INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260025)\u001b[0;0m INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260028)\u001b[0;0m INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3260027)\u001b[0;0m INFO 10-30 19:38:01 model_runner.py:1530] Graph capturing finished in 16 secs.\n",
      "Processed prompts: 100%|█| 1000/1000 [00:28<00:00, 34.79it/s, est. speed input: \n",
      "INFO 10-30 19:38:36 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "Throughput: 34.36 requests/s, 35187.73 tokens/s\n",
      "[rank0]:[W1030 19:38:37.588781225 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Don't use tuned GEMM\n",
    "!VLLM_UNTUNE_FILE=\"/tmp/vllm_untuned-512-512.csv\" VLLM_TUNE_GEMM=1 VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --num-scheduler-steps 15 --model meta-llama/Llama-3.1-8B-Instruct --input-len 512 --output-len 512 -tp 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vllm]",
   "language": "python",
   "name": "conda-env-vllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
