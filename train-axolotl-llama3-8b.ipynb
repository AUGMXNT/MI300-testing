{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620ad71f-089d-49dc-b4e2-78e2208f8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ulimit -n 1031072\n",
    "# sudo mount /dev/nvme8n1p1 /tmp -o noatime\n",
    "#sudo apt install libaio-dev\n",
    "\"\"\"\n",
    "git clone axolotl\n",
    "\n",
    "mkdir /mnt/nvme7n1p1/outputs\n",
    "ln -s /mnt/nvme7n1p1/outputs\n",
    "\n",
    "mamba install ipykernel\n",
    "python -m ipykernel install --user --name axolotl\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e90c914-76a9-4a41-b848-4021ac12f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-22 16:21:04,833] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "--------------------------------------------------\n",
      "DeepSpeed C++/CUDA extension op report\n",
      "--------------------------------------------------\n",
      "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
      "      runtime if needed. Op compatibility means that your system\n",
      "      meet the required dependencies to JIT install the op.\n",
      "--------------------------------------------------\n",
      "JIT compiled ops requires ninja\n",
      "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
      "--------------------------------------------------\n",
      "op name ................ installed .. compatible\n",
      "--------------------------------------------------\n",
      "async_io ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "fused_adam ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "cpu_adam ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "cpu_adagrad ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "cpu_lion ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "evoformer_attn ......... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
      "\u001b[93m [WARNING] \u001b[0m FP Quantizer is using an untested triton version (3.1.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels\n",
      "fp_quantizer ........... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
      "fused_lamb ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "fused_lion ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "\u001b[93m [WARNING] \u001b[0m gds is not compatible with ROCM\n",
      "gds .................... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
      "transformer_inference .. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "inference_core_ops ..... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "cutlass_ops ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "quantizer .............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "ragged_device_ops ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "ragged_ops ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "random_ltd ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn is not compatible with ROCM\n",
      "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
      "spatial_inference ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "transformer ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "stochastic_transformer . \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "--------------------------------------------------\n",
      "DeepSpeed general environment info:\n",
      "torch install path ............... ['/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/torch']\n",
      "torch version .................... 2.5.0+rocm6.2\n",
      "deepspeed install path ........... ['/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/deepspeed']\n",
      "deepspeed info ................... 0.15.3+unknown, unknown, unknown\n",
      "torch cuda version ............... None\n",
      "torch hip version ................ 6.2.41133-dd7f95766\n",
      "nvcc version ..................... None\n",
      "deepspeed wheel compiled w. ...... torch 2.5, hip 6.2\n",
      "shared memory (/dev/shm) size .... 1007.67 GB\n"
     ]
    }
   ],
   "source": [
    "!ds_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f55c7-4be7-4191-9be2-eceb55506e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `8`\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:28: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:39: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "[2024-10-23 01:30:23,806] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:23,836] [INFO] [root.spawn:60] [PID:442426] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmpd4nx4hkz/test.c -o /tmp/tmpd4nx4hkz/test.o\n",
      "[2024-10-23 01:30:23,851] [INFO] [root.spawn:60] [PID:442426] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmpd4nx4hkz/test.o -laio -o /tmp/tmpd4nx4hkz/a.out\n",
      "[2024-10-23 01:30:23,888] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:23,917] [INFO] [root.spawn:60] [PID:442424] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmpa538zpuk/test.c -o /tmp/tmpa538zpuk/test.o\n",
      "[2024-10-23 01:30:23,931] [INFO] [root.spawn:60] [PID:442424] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmpa538zpuk/test.o -laio -o /tmp/tmpa538zpuk/a.out\n",
      "[2024-10-23 01:30:23,976] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:24,007] [INFO] [root.spawn:60] [PID:442429] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmp08li1z7j/test.c -o /tmp/tmp08li1z7j/test.o\n",
      "[2024-10-23 01:30:24,021] [INFO] [root.spawn:60] [PID:442429] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmp08li1z7j/test.o -laio -o /tmp/tmp08li1z7j/a.out\n",
      "[2024-10-23 01:30:24,051] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:24,051] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:24,061] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:24,080] [INFO] [root.spawn:60] [PID:442425] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmpc8e58ubz/test.c -o /tmp/tmpc8e58ubz/test.o\n",
      "[2024-10-23 01:30:24,081] [INFO] [root.spawn:60] [PID:442428] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmpkdtw1hyo/test.c -o /tmp/tmpkdtw1hyo/test.o\n",
      "[2024-10-23 01:30:24,084] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:24,090] [INFO] [root.spawn:60] [PID:442427] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmpd0o96q9o/test.c -o /tmp/tmpd0o96q9o/test.o\n",
      "[2024-10-23 01:30:24,094] [INFO] [root.spawn:60] [PID:442428] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmpkdtw1hyo/test.o -laio -o /tmp/tmpkdtw1hyo/a.out\n",
      "[2024-10-23 01:30:24,096] [INFO] [root.spawn:60] [PID:442425] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmpc8e58ubz/test.o -laio -o /tmp/tmpc8e58ubz/a.out\n",
      "[2024-10-23 01:30:24,106] [INFO] [root.spawn:60] [PID:442427] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmpd0o96q9o/test.o -laio -o /tmp/tmpd0o96q9o/a.out\n",
      "[2024-10-23 01:30:24,114] [INFO] [root.spawn:60] [PID:442423] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmpen49aqbp/test.c -o /tmp/tmpen49aqbp/test.o\n",
      "[2024-10-23 01:30:24,125] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-23 01:30:24,128] [INFO] [root.spawn:60] [PID:442423] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmpen49aqbp/test.o -laio -o /tmp/tmpen49aqbp/a.out\n",
      "[2024-10-23 01:30:24,155] [INFO] [root.spawn:60] [PID:442430] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/axolotl/include -fPIC -c /tmp/tmpgqedpxh_/test.c -o /tmp/tmpgqedpxh_/test.o\n",
      "[2024-10-23 01:30:24,169] [INFO] [root.spawn:60] [PID:442430] gcc -pthread -B /home/hotaisle/miniforge3/envs/axolotl/compiler_compat /tmp/tmpgqedpxh_/test.o -laio -o /tmp/tmpgqedpxh_/a.out\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/mnt/nvme1n1p1/MI300-testing/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "[2024-10-23 01:30:25,013] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442426] [RANK:3] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,015] [DEBUG] [axolotl.normalize_config:83] [PID:442426] [RANK:3] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "[2024-10-23 01:30:25,078] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442424] [RANK:1] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,080] [DEBUG] [axolotl.normalize_config:83] [PID:442424] [RANK:1] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-10-23 01:30:25,209] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442429] [RANK:6] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,212] [DEBUG] [axolotl.normalize_config:83] [PID:442429] [RANK:6] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-10-23 01:30:25,241] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442428] [RANK:5] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,242] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442425] [RANK:2] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,244] [DEBUG] [axolotl.normalize_config:83] [PID:442428] [RANK:5] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-10-23 01:30:25,244] [DEBUG] [axolotl.normalize_config:83] [PID:442425] [RANK:2] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-10-23 01:30:25,255] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442427] [RANK:4] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,257] [DEBUG] [axolotl.normalize_config:83] [PID:442427] [RANK:4] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-10-23 01:30:25,267] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442423] [RANK:0] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,269] [DEBUG] [axolotl.normalize_config:83] [PID:442423] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-10-23 01:30:25,297] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1044] [PID:442430] [RANK:7] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-10-23 01:30:25,300] [DEBUG] [axolotl.normalize_config:83] [PID:442430] [RANK:7] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-10-23 01:30:25,355] [INFO] [axolotl.normalize_config:207] [PID:442426] [RANK:3] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,355] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442426] [RANK:3] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,359] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:25,405] [INFO] [axolotl.normalize_config:207] [PID:442429] [RANK:6] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,406] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442429] [RANK:6] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,407] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:25,431] [INFO] [axolotl.normalize_config:207] [PID:442428] [RANK:5] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,431] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442428] [RANK:5] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,433] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:25,436] [INFO] [axolotl.normalize_config:207] [PID:442424] [RANK:1] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,436] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442424] [RANK:1] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,440] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:25,442] [INFO] [axolotl.normalize_config:207] [PID:442425] [RANK:2] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,442] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442425] [RANK:2] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,443] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:25,448] [INFO] [axolotl.normalize_config:207] [PID:442427] [RANK:4] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,448] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442427] [RANK:4] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,450] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:25,479] [INFO] [axolotl.normalize_config:207] [PID:442423] [RANK:0] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,479] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442423] [RANK:0] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,480] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:25,480] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2024-10-23 01:30:25,499] [INFO] [axolotl.normalize_config:207] [PID:442430] [RANK:7] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
      "[2024-10-23 01:30:25,499] [INFO] [axolotl.normalize_cfg_datasets:219] [PID:442430] [RANK:7] updating dataset augmxnt/ultra-orca-boros-en-ja-v1 with `conversation: ChatTemplate.llama3` to match your chat_template\u001b[39m\n",
      "[2024-10-23 01:30:25,501] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 01:30:26,150] [DEBUG] [axolotl.load_tokenizer:290] [PID:442426] [RANK:3] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,151] [DEBUG] [axolotl.load_tokenizer:291] [PID:442426] [RANK:3] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,151] [DEBUG] [axolotl.load_tokenizer:292] [PID:442426] [RANK:3] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,151] [DEBUG] [axolotl.load_tokenizer:293] [PID:442426] [RANK:3] UNK: None / None\u001b[39m\n",
      "[rank3]:[W1023 01:30:26.287916114 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:26,182] [DEBUG] [axolotl.load_tokenizer:290] [PID:442427] [RANK:4] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,182] [DEBUG] [axolotl.load_tokenizer:291] [PID:442427] [RANK:4] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,182] [DEBUG] [axolotl.load_tokenizer:292] [PID:442427] [RANK:4] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,182] [DEBUG] [axolotl.load_tokenizer:293] [PID:442427] [RANK:4] UNK: None / None\u001b[39m\n",
      "[rank4]:[W1023 01:30:26.319706708 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:26,200] [DEBUG] [axolotl.load_tokenizer:290] [PID:442429] [RANK:6] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,200] [DEBUG] [axolotl.load_tokenizer:291] [PID:442429] [RANK:6] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,200] [DEBUG] [axolotl.load_tokenizer:292] [PID:442429] [RANK:6] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,200] [DEBUG] [axolotl.load_tokenizer:293] [PID:442429] [RANK:6] UNK: None / None\u001b[39m\n",
      "[rank6]:[W1023 01:30:26.337934148 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:26,226] [DEBUG] [axolotl.load_tokenizer:290] [PID:442428] [RANK:5] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,227] [DEBUG] [axolotl.load_tokenizer:291] [PID:442428] [RANK:5] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,227] [DEBUG] [axolotl.load_tokenizer:292] [PID:442428] [RANK:5] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,227] [DEBUG] [axolotl.load_tokenizer:293] [PID:442428] [RANK:5] UNK: None / None\u001b[39m\n",
      "[rank5]:[W1023 01:30:26.363948723 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:26,243] [DEBUG] [axolotl.load_tokenizer:290] [PID:442425] [RANK:2] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,243] [DEBUG] [axolotl.load_tokenizer:291] [PID:442425] [RANK:2] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,243] [DEBUG] [axolotl.load_tokenizer:292] [PID:442425] [RANK:2] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,243] [DEBUG] [axolotl.load_tokenizer:293] [PID:442425] [RANK:2] UNK: None / None\u001b[39m\n",
      "[rank2]:[W1023 01:30:26.380611555 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:26,263] [DEBUG] [axolotl.load_tokenizer:290] [PID:442423] [RANK:0] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,263] [DEBUG] [axolotl.load_tokenizer:291] [PID:442423] [RANK:0] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,263] [DEBUG] [axolotl.load_tokenizer:292] [PID:442423] [RANK:0] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,263] [DEBUG] [axolotl.load_tokenizer:293] [PID:442423] [RANK:0] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:26,264] [INFO] [axolotl.load_tokenized_prepared_datasets:210] [PID:442423] [RANK:0] Unable to find prepared dataset in last_run_prepared/0aa53633c446d584533a032a68222d60\u001b[39m\n",
      "[2024-10-23 01:30:26,264] [INFO] [axolotl.load_tokenized_prepared_datasets:211] [PID:442423] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2024-10-23 01:30:26,264] [WARNING] [axolotl.load_tokenized_prepared_datasets:213] [PID:442423] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2024-10-23 01:30:26,264] [INFO] [axolotl.load_tokenized_prepared_datasets:220] [PID:442423] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
      "[2024-10-23 01:30:26,292] [DEBUG] [axolotl.load_tokenizer:290] [PID:442430] [RANK:7] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,292] [DEBUG] [axolotl.load_tokenizer:291] [PID:442430] [RANK:7] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,292] [DEBUG] [axolotl.load_tokenizer:292] [PID:442430] [RANK:7] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,292] [DEBUG] [axolotl.load_tokenizer:293] [PID:442430] [RANK:7] UNK: None / None\u001b[39m\n",
      "[rank7]:[W1023 01:30:26.429025930 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:26,374] [DEBUG] [axolotl.load_tokenizer:290] [PID:442424] [RANK:1] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:26,374] [DEBUG] [axolotl.load_tokenizer:291] [PID:442424] [RANK:1] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,374] [DEBUG] [axolotl.load_tokenizer:292] [PID:442424] [RANK:1] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:26,375] [DEBUG] [axolotl.load_tokenizer:293] [PID:442424] [RANK:1] UNK: None / None\u001b[39m\n",
      "[rank1]:[W1023 01:30:26.512024865 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:28,325] [INFO] [axolotl.get_dataset_wrapper:588] [PID:442423] [RANK:0] Loading dataset with base_type: sharegpt and prompt_style: None\u001b[39m\n",
      "\u001b[33m[2024-10-23 01:30:28,326] [WARNING] [axolotl._load:64] [PID:442423] [RANK:0] sharegpt type support will be deprecated in the next release of Axolotl. Please use chat_template instead.\u001b[39m\n",
      "[2024-10-23 01:30:31,615] [INFO] [axolotl.load_tokenized_prepared_datasets:467] [PID:442423] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/0aa53633c446d584533a032a68222d60\u001b[39m\n",
      "Saving the dataset (5/5 shards): 100%|â–ˆ| 187937/187937 [00:01<00:00, 120026.79 e\n",
      "[rank0]:[W1023 01:30:33.374519241 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[2024-10-23 01:30:34,407] [INFO] [axolotl.load_tokenized_prepared_datasets:199] [PID:442430] [RANK:7] Loading prepared dataset from disk at last_run_prepared/0aa53633c446d584533a032a68222d60...\u001b[39m\n",
      "[2024-10-23 01:30:34,407] [INFO] [axolotl.load_tokenized_prepared_datasets:199] [PID:442425] [RANK:2] Loading prepared dataset from disk at last_run_prepared/0aa53633c446d584533a032a68222d60...\u001b[39m\n",
      "[2024-10-23 01:30:34,407] [INFO] [axolotl.load_tokenized_prepared_datasets:199] [PID:442424] [RANK:1] Loading prepared dataset from disk at last_run_prepared/0aa53633c446d584533a032a68222d60...\u001b[39m\n",
      "[2024-10-23 01:30:34,407] [INFO] [axolotl.load_tokenized_prepared_datasets:199] [PID:442429] [RANK:6] Loading prepared dataset from disk at last_run_prepared/0aa53633c446d584533a032a68222d60...\u001b[39m\n",
      "[2024-10-23 01:30:34,408] [INFO] [axolotl.load_tokenized_prepared_datasets:199] [PID:442428] [RANK:5] Loading prepared dataset from disk at last_run_prepared/0aa53633c446d584533a032a68222d60...\u001b[39m\n",
      "[2024-10-23 01:30:34,408] [INFO] [axolotl.load_tokenized_prepared_datasets:199] [PID:442426] [RANK:3] Loading prepared dataset from disk at last_run_prepared/0aa53633c446d584533a032a68222d60...\u001b[39m\n",
      "[2024-10-23 01:30:34,408] [INFO] [axolotl.load_tokenized_prepared_datasets:199] [PID:442427] [RANK:4] Loading prepared dataset from disk at last_run_prepared/0aa53633c446d584533a032a68222d60...\u001b[39m\n",
      "[2024-10-23 01:30:34,414] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:442430] [RANK:7] Prepared dataset loaded from disk...\u001b[39m\n",
      "[2024-10-23 01:30:34,415] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:442425] [RANK:2] Prepared dataset loaded from disk...\u001b[39m\n",
      "[2024-10-23 01:30:34,416] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:442424] [RANK:1] Prepared dataset loaded from disk...\u001b[39m\n",
      "[2024-10-23 01:30:34,417] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:442429] [RANK:6] Prepared dataset loaded from disk...\u001b[39m\n",
      "[2024-10-23 01:30:34,426] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:442426] [RANK:3] Prepared dataset loaded from disk...\u001b[39m\n",
      "[2024-10-23 01:30:34,426] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:442428] [RANK:5] Prepared dataset loaded from disk...\u001b[39m\n",
      "[2024-10-23 01:30:34,427] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:442427] [RANK:4] Prepared dataset loaded from disk...\u001b[39m\n",
      "[2024-10-23 01:30:34,455] [DEBUG] [axolotl.calculate_total_num_steps:320] [PID:442423] [RANK:0] total_num_tokens: 15_537_156\u001b[39m\n",
      "[2024-10-23 01:30:34,617] [DEBUG] [axolotl.calculate_total_num_steps:338] [PID:442423] [RANK:0] `total_supervised_tokens: 7_583_970`\u001b[39m\n",
      "[2024-10-23 01:30:38,499] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:442423] [RANK:0] gather_len_batches: [241, 241, 241, 241, 242, 241, 241, 241]\u001b[39m\n",
      "[2024-10-23 01:30:38,499] [DEBUG] [axolotl.calculate_total_num_steps:390] [PID:442423] [RANK:0] data_loader_len: 30\u001b[39m\n",
      "[2024-10-23 01:30:38,516] [INFO] [axolotl.calc_sample_packing_eff_est:396] [PID:442423] [RANK:0] sample_packing_eff_est across ranks: [0.9837269186973572, 0.9837269186973572, 0.9837269186973572, 0.9837269186973572, 0.9796619415283203, 0.9837269186973572, 0.9837269186973572, 0.9837269186973572]\u001b[39m\n",
      "[2024-10-23 01:30:38,516] [DEBUG] [axolotl.calculate_total_num_steps:408] [PID:442423] [RANK:0] sample_packing_eff_est: None\u001b[39m\n",
      "[2024-10-23 01:30:38,516] [DEBUG] [axolotl.calculate_total_num_steps:416] [PID:442423] [RANK:0] total_num_steps: 90\u001b[39m\n",
      "[2024-10-23 01:30:38,796] [DEBUG] [axolotl.calculate_total_num_steps:320] [PID:442423] [RANK:0] total_num_tokens: 79_721_158\u001b[39m\n",
      "[2024-10-23 01:30:40,183] [DEBUG] [axolotl.calculate_total_num_steps:338] [PID:442423] [RANK:0] `total_supervised_tokens: 39_365_516`\u001b[39m\n",
      "[2024-10-23 01:30:40,408] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:442423] [RANK:0] gather_len_batches: [1226, 1226, 1226, 1226, 1226, 1226, 1226, 1226]\u001b[39m\n",
      "[2024-10-23 01:30:40,408] [DEBUG] [axolotl.calculate_total_num_steps:390] [PID:442423] [RANK:0] data_loader_len: 152\u001b[39m\n",
      "[2024-10-23 01:30:40,409] [INFO] [axolotl.calc_sample_packing_eff_est:396] [PID:442423] [RANK:0] sample_packing_eff_est across ranks: [0.9922090768814087, 0.9922090768814087, 0.9922090768814087, 0.9922090768814087, 0.9922090768814087, 0.9922090768814087, 0.9922090768814087, 0.9922090768814087]\u001b[39m\n",
      "[2024-10-23 01:30:40,410] [DEBUG] [axolotl.calculate_total_num_steps:408] [PID:442423] [RANK:0] sample_packing_eff_est: 1.0\u001b[39m\n",
      "[2024-10-23 01:30:40,410] [DEBUG] [axolotl.calculate_total_num_steps:416] [PID:442423] [RANK:0] total_num_steps: 456\u001b[39m\n",
      "[2024-10-23 01:30:40,435] [DEBUG] [axolotl.train.train:66] [PID:442423] [RANK:0] loading tokenizer... meta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\n",
      "[2024-10-23 01:30:41,090] [DEBUG] [axolotl.load_tokenizer:290] [PID:442423] [RANK:0] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,090] [DEBUG] [axolotl.load_tokenizer:291] [PID:442423] [RANK:0] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,090] [DEBUG] [axolotl.load_tokenizer:292] [PID:442423] [RANK:0] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,090] [DEBUG] [axolotl.load_tokenizer:293] [PID:442423] [RANK:0] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:41,090] [DEBUG] [axolotl.train.train:98] [PID:442423] [RANK:0] loading model\u001b[39m\n",
      "[2024-10-23 01:30:41,099] [DEBUG] [axolotl.load_tokenizer:290] [PID:442429] [RANK:6] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,099] [DEBUG] [axolotl.load_tokenizer:291] [PID:442429] [RANK:6] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,099] [DEBUG] [axolotl.load_tokenizer:292] [PID:442429] [RANK:6] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,099] [DEBUG] [axolotl.load_tokenizer:293] [PID:442429] [RANK:6] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:41,107] [DEBUG] [axolotl.load_tokenizer:290] [PID:442425] [RANK:2] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,107] [DEBUG] [axolotl.load_tokenizer:291] [PID:442425] [RANK:2] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,107] [DEBUG] [axolotl.load_tokenizer:292] [PID:442425] [RANK:2] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,107] [DEBUG] [axolotl.load_tokenizer:293] [PID:442425] [RANK:2] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:41,119] [DEBUG] [axolotl.load_tokenizer:290] [PID:442428] [RANK:5] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,119] [DEBUG] [axolotl.load_tokenizer:291] [PID:442428] [RANK:5] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,119] [DEBUG] [axolotl.load_tokenizer:292] [PID:442428] [RANK:5] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,119] [DEBUG] [axolotl.load_tokenizer:293] [PID:442428] [RANK:5] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:41,120] [DEBUG] [axolotl.load_tokenizer:290] [PID:442427] [RANK:4] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,120] [DEBUG] [axolotl.load_tokenizer:291] [PID:442427] [RANK:4] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,120] [DEBUG] [axolotl.load_tokenizer:292] [PID:442427] [RANK:4] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,120] [DEBUG] [axolotl.load_tokenizer:293] [PID:442427] [RANK:4] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:41,122] [DEBUG] [axolotl.load_tokenizer:290] [PID:442430] [RANK:7] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,122] [DEBUG] [axolotl.load_tokenizer:291] [PID:442430] [RANK:7] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,122] [DEBUG] [axolotl.load_tokenizer:292] [PID:442430] [RANK:7] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,122] [DEBUG] [axolotl.load_tokenizer:293] [PID:442430] [RANK:7] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:41,127] [DEBUG] [axolotl.load_tokenizer:290] [PID:442424] [RANK:1] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,127] [DEBUG] [axolotl.load_tokenizer:291] [PID:442424] [RANK:1] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,127] [DEBUG] [axolotl.load_tokenizer:292] [PID:442424] [RANK:1] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,127] [DEBUG] [axolotl.load_tokenizer:293] [PID:442424] [RANK:1] UNK: None / None\u001b[39m\n",
      "[2024-10-23 01:30:41,272] [DEBUG] [axolotl.load_tokenizer:290] [PID:442426] [RANK:3] EOS: 128009 / <|eot_id|>\u001b[39m\n",
      "[2024-10-23 01:30:41,272] [DEBUG] [axolotl.load_tokenizer:291] [PID:442426] [RANK:3] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,272] [DEBUG] [axolotl.load_tokenizer:292] [PID:442426] [RANK:3] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-10-23 01:30:41,272] [DEBUG] [axolotl.load_tokenizer:293] [PID:442426] [RANK:3] UNK: None / None\u001b[39m\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.45s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.45s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.46s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.47s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.46s/it]\n",
      "[2024-10-23 01:30:51,695] [INFO] [axolotl.load_model:855] [PID:442423] [RANK:0] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:51,699] [INFO] [axolotl.load_model:922] [PID:442423] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-10-23 01:30:51,717] [INFO] [axolotl.load_model:855] [PID:442430] [RANK:7] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:51,721] [INFO] [axolotl.load_model:922] [PID:442430] [RANK:7] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-10-23 01:30:51,729] [INFO] [axolotl.load_model:855] [PID:442425] [RANK:2] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:51,735] [INFO] [axolotl.load_model:922] [PID:442425] [RANK:2] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-10-23 01:30:51,750] [INFO] [axolotl.load_model:855] [PID:442428] [RANK:5] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:51,755] [INFO] [axolotl.load_model:922] [PID:442428] [RANK:5] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.52s/it]\n",
      "[2024-10-23 01:30:51,865] [INFO] [axolotl.load_model:855] [PID:442427] [RANK:4] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:51,869] [INFO] [axolotl.load_model:922] [PID:442427] [RANK:4] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.50s/it]\n",
      "[2024-10-23 01:30:51,976] [INFO] [axolotl.load_model:855] [PID:442429] [RANK:6] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:51,982] [INFO] [axolotl.load_model:922] [PID:442429] [RANK:6] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.51s/it]\n",
      "[2024-10-23 01:30:52,058] [INFO] [axolotl.load_model:855] [PID:442424] [RANK:1] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:52,063] [INFO] [axolotl.load_model:922] [PID:442424] [RANK:1] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-10-23 01:30:52,166] [INFO] [axolotl.load_model:855] [PID:442426] [RANK:3] GPU memory usage after model load: 14.958GB (+0.126GB cache)\u001b[39m\n",
      "[2024-10-23 01:30:52,171] [INFO] [axolotl.load_model:922] [PID:442426] [RANK:3] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[2024-10-23 01:30:52,731] [INFO] [axolotl.train.train:178] [PID:442423] [RANK:0] Starting trainer...\u001b[39m\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/hotaisle/miniforge3/envs/axolotl/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[2024-10-23 01:30:53,496] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:442423] [RANK:0] gather_len_batches: [1226, 1226, 1226, 1226, 1226, 1226, 1226, 1226]\u001b[39m\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "Emitting ninja build file /home/hotaisle/.cache/torch_extensions/py312_cpu/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.6414060592651367 seconds\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "Emitting ninja build file /home/hotaisle/.cache/torch_extensions/py312_cpu/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 3.167980909347534 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 3.213873863220215 seconds\n",
      "Time to load cpu_adam op: 3.2124733924865723 seconds\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "Emitting ninja build file /home/hotaisle/.cache/torch_extensions/py312_cpu/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 3.391488790512085 seconds\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "Using /home/hotaisle/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "Emitting ninja build file /home/hotaisle/.cache/torch_extensions/py312_cpu/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 3.4703497886657715 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 3.4853782653808594 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 3.5162506103515625 seconds\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrandomfoo\u001b[0m (\u001b[33maugmxnt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/nvme1n1p1/MI300-testing/wandb/run-20241023_013130-1eb49pxg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmi300x-shisa-llama3.1-8b-v1-dsz2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/augmxnt/shisa-v2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/augmxnt/shisa-v2/runs/1eb49pxg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "[2024-10-23 01:31:31,046] [INFO] [axolotl.callbacks.on_train_begin:794] [PID:442423] [RANK:0] The Axolotl config has been saved to the WandB run under files.\u001b[39m\n",
      "  0%|                                                   | 0/456 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.1231, 'grad_norm': 3.81037974357605, 'learning_rate': 8e-08, 'epoch': 0.01}\n",
      "  0%|                                         | 1/456 [00:31<3:57:29, 31.32s/it][2024-10-23 01:32:02,378] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:442423] [RANK:0] gather_len_batches: [241, 241, 241, 241, 241, 241, 241, 241]\u001b[39m\n",
      "\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–‰                                         | 2/30 [00:03<00:42,  1.52s/it]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 3/30 [00:06<00:58,  2.16s/it]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/30 [00:09<01:04,  2.49s/it]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 5/30 [00:12<01:06,  2.67s/it]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 6/30 [00:15<01:06,  2.76s/it]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                 | 7/30 [00:18<01:04,  2.82s/it]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 8/30 [00:20<01:02,  2.85s/it]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 9/30 [00:23<01:00,  2.88s/it]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 10/30 [00:27<00:59,  2.96s/it]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 11/30 [00:30<00:56,  2.98s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 12/30 [00:33<00:53,  3.00s/it]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 13/30 [00:36<00:51,  3.01s/it]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 14/30 [00:39<00:48,  3.02s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 15/30 [00:42<00:45,  3.02s/it]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 16/30 [00:45<00:42,  3.02s/it]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 17/30 [00:48<00:39,  3.02s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 18/30 [00:51<00:36,  3.03s/it]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 19/30 [00:54<00:33,  3.01s/it]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 20/30 [00:57<00:30,  3.02s/it]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 21/30 [01:00<00:27,  3.01s/it]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 22/30 [01:03<00:24,  3.03s/it]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 23/30 [01:06<00:21,  3.04s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 24/30 [01:09<00:18,  3.05s/it]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 25/30 [01:12<00:15,  3.04s/it]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 26/30 [01:15<00:12,  3.07s/it]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/30 [01:18<00:09,  3.06s/it]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/30 [01:21<00:06,  3.06s/it]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29/30 [01:24<00:03,  3.04s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6680425405502319, 'eval_runtime': 90.9107, 'eval_samples_per_second': 103.365, 'eval_steps_per_second': 1.617, 'epoch': 0.01}\n",
      "  0%|                                         | 1/456 [02:02<3:57:29, 31.32s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:28<00:00,  3.02s/it]\u001b[A\n",
      "                                                                                \u001b[A[2024-10-23 01:33:48,186] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442423] [RANK:0] GPU memory usage while training: 15.938GB (+144.562GB cache)\u001b[39m\n",
      "[2024-10-23 01:33:48,186] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442425] [RANK:2] GPU memory usage while training: 15.938GB (+138.339GB cache)\u001b[39m\n",
      "[2024-10-23 01:33:48,186] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442427] [RANK:4] GPU memory usage while training: 15.938GB (+138.191GB cache)\u001b[39m\n",
      "[2024-10-23 01:33:48,186] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442424] [RANK:1] GPU memory usage while training: 15.938GB (+138.072GB cache)\u001b[39m\n",
      "[2024-10-23 01:33:48,186] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442428] [RANK:5] GPU memory usage while training: 15.938GB (+138.201GB cache)\u001b[39m\n",
      "[2024-10-23 01:33:48,187] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442426] [RANK:3] GPU memory usage while training: 15.938GB (+138.072GB cache)\u001b[39m\n",
      "  0%|â–                                        | 2/456 [02:17<9:28:35, 75.14s/it][2024-10-23 01:33:48,187] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442429] [RANK:6] GPU memory usage while training: 15.938GB (+138.632GB cache)\u001b[39m\n",
      "[2024-10-23 01:33:48,199] [INFO] [axolotl.callbacks.on_step_end:128] [PID:442430] [RANK:7] GPU memory usage while training: 15.938GB (+142.363GB cache)\u001b[39m\n",
      "{'loss': 1.1024, 'grad_norm': 3.8261754512786865, 'learning_rate': 1.6e-07, 'epoch': 0.01}\n",
      "{'loss': 1.0876, 'grad_norm': 3.7819855213165283, 'learning_rate': 2.4e-07, 'epoch': 0.02}\n",
      "{'loss': 1.0526, 'grad_norm': 3.7339770793914795, 'learning_rate': 3.2e-07, 'epoch': 0.03}\n",
      "{'loss': 1.0441, 'grad_norm': 3.6772046089172363, 'learning_rate': 4e-07, 'epoch': 0.03}\n",
      "{'loss': 1.056, 'grad_norm': 3.954829216003418, 'learning_rate': 4.8e-07, 'epoch': 0.04}\n",
      "{'loss': 1.0924, 'grad_norm': 3.7208900451660156, 'learning_rate': 5.6e-07, 'epoch': 0.05}\n",
      "{'loss': 1.1012, 'grad_norm': 3.4644031524658203, 'learning_rate': 6.4e-07, 'epoch': 0.05}\n",
      "{'loss': 1.052, 'grad_norm': 3.160262107849121, 'learning_rate': 7.2e-07, 'epoch': 0.06}\n",
      "{'loss': 1.086, 'grad_norm': 2.9125308990478516, 'learning_rate': 8e-07, 'epoch': 0.07}\n",
      "{'loss': 1.0807, 'grad_norm': 2.9186336994171143, 'learning_rate': 8.799999999999999e-07, 'epoch': 0.07}\n",
      "{'loss': 1.0921, 'grad_norm': 1.837082862854004, 'learning_rate': 9.6e-07, 'epoch': 0.08}\n",
      "{'loss': 1.0194, 'grad_norm': 1.8400083780288696, 'learning_rate': 1.04e-06, 'epoch': 0.09}\n",
      "{'loss': 1.0893, 'grad_norm': 1.7839946746826172, 'learning_rate': 1.12e-06, 'epoch': 0.09}\n",
      "{'loss': 1.058, 'grad_norm': 1.7156178951263428, 'learning_rate': 1.2e-06, 'epoch': 0.1}\n",
      "{'loss': 1.0723, 'grad_norm': 1.8021931648254395, 'learning_rate': 1.28e-06, 'epoch': 0.11}\n",
      "{'loss': 1.0018, 'grad_norm': 1.8203176259994507, 'learning_rate': 1.3600000000000001e-06, 'epoch': 0.11}\n",
      "{'loss': 0.9846, 'grad_norm': 1.9103730916976929, 'learning_rate': 1.44e-06, 'epoch': 0.12}\n",
      "{'loss': 1.0309, 'grad_norm': 1.6475203037261963, 'learning_rate': 1.5199999999999998e-06, 'epoch': 0.12}\n",
      "{'loss': 1.0146, 'grad_norm': 1.4023973941802979, 'learning_rate': 1.6e-06, 'epoch': 0.13}\n",
      "{'loss': 1.0373, 'grad_norm': 1.0949639081954956, 'learning_rate': 1.6799999999999998e-06, 'epoch': 0.14}\n",
      "{'loss': 0.986, 'grad_norm': 1.0371222496032715, 'learning_rate': 1.7599999999999999e-06, 'epoch': 0.14}\n",
      "{'loss': 1.0355, 'grad_norm': 1.4787925481796265, 'learning_rate': 1.84e-06, 'epoch': 0.15}\n",
      "{'loss': 1.1059, 'grad_norm': 1.5859098434448242, 'learning_rate': 1.92e-06, 'epoch': 0.16}\n",
      "{'loss': 1.0054, 'grad_norm': 1.20163893699646, 'learning_rate': 2e-06, 'epoch': 0.16}\n",
      "{'loss': 0.9956, 'grad_norm': 0.9749578237533569, 'learning_rate': 2.08e-06, 'epoch': 0.17}\n",
      "{'loss': 0.9929, 'grad_norm': 0.9380613565444946, 'learning_rate': 2.16e-06, 'epoch': 0.18}\n",
      "{'loss': 1.019, 'grad_norm': 0.986482560634613, 'learning_rate': 2.24e-06, 'epoch': 0.18}\n",
      "{'loss': 0.9521, 'grad_norm': 0.9602142572402954, 'learning_rate': 2.32e-06, 'epoch': 0.19}\n",
      "{'loss': 0.9543, 'grad_norm': 1.1316558122634888, 'learning_rate': 2.4e-06, 'epoch': 0.2}\n",
      "{'loss': 1.0243, 'grad_norm': 0.9063349962234497, 'learning_rate': 2.48e-06, 'epoch': 0.2}\n",
      "{'loss': 0.9949, 'grad_norm': 0.8671271204948425, 'learning_rate': 2.56e-06, 'epoch': 0.21}\n",
      "{'loss': 0.9887, 'grad_norm': 0.8445854187011719, 'learning_rate': 2.64e-06, 'epoch': 0.22}\n",
      "{'loss': 0.9684, 'grad_norm': 0.9383912086486816, 'learning_rate': 2.7200000000000002e-06, 'epoch': 0.22}\n",
      "{'loss': 0.9461, 'grad_norm': 0.9557889103889465, 'learning_rate': 2.8e-06, 'epoch': 0.23}\n",
      "{'loss': 0.9552, 'grad_norm': 0.8385952711105347, 'learning_rate': 2.88e-06, 'epoch': 0.24}\n",
      "{'loss': 0.9916, 'grad_norm': 0.8377764225006104, 'learning_rate': 2.96e-06, 'epoch': 0.24}\n",
      "{'loss': 0.9134, 'grad_norm': 0.8878225088119507, 'learning_rate': 3.0399999999999997e-06, 'epoch': 0.25}\n",
      "{'loss': 0.9012, 'grad_norm': 0.9103479385375977, 'learning_rate': 3.1199999999999998e-06, 'epoch': 0.26}\n",
      "{'loss': 0.9807, 'grad_norm': 0.8359912037849426, 'learning_rate': 3.2e-06, 'epoch': 0.26}\n",
      "{'loss': 0.9505, 'grad_norm': 0.7564811110496521, 'learning_rate': 3.2799999999999995e-06, 'epoch': 0.27}\n",
      "{'loss': 0.9398, 'grad_norm': 0.842942476272583, 'learning_rate': 3.3599999999999996e-06, 'epoch': 0.28}\n",
      "{'loss': 0.9626, 'grad_norm': 0.8205924034118652, 'learning_rate': 3.4399999999999997e-06, 'epoch': 0.28}\n",
      "{'loss': 0.963, 'grad_norm': 0.7521784901618958, 'learning_rate': 3.5199999999999998e-06, 'epoch': 0.29}\n",
      "{'loss': 0.9991, 'grad_norm': 0.7797653675079346, 'learning_rate': 3.6e-06, 'epoch': 0.3}\n",
      "{'loss': 0.926, 'grad_norm': 0.7841501235961914, 'learning_rate': 3.68e-06, 'epoch': 0.3}\n",
      "{'loss': 0.9423, 'grad_norm': 0.7647865414619446, 'learning_rate': 3.7599999999999996e-06, 'epoch': 0.31}\n",
      "{'loss': 0.9605, 'grad_norm': 0.7867656946182251, 'learning_rate': 3.84e-06, 'epoch': 0.32}\n",
      "{'loss': 0.9721, 'grad_norm': 0.7965825200080872, 'learning_rate': 3.92e-06, 'epoch': 0.32}\n",
      "{'loss': 0.936, 'grad_norm': 0.8621001839637756, 'learning_rate': 4e-06, 'epoch': 0.33}\n",
      "{'loss': 0.9544, 'grad_norm': 0.8260868787765503, 'learning_rate': 4.08e-06, 'epoch': 0.34}\n",
      "{'loss': 0.9307, 'grad_norm': 0.8443419337272644, 'learning_rate': 4.16e-06, 'epoch': 0.34}\n",
      "{'loss': 0.931, 'grad_norm': 0.7845333218574524, 'learning_rate': 4.24e-06, 'epoch': 0.35}\n",
      "{'loss': 0.8688, 'grad_norm': 0.7558678388595581, 'learning_rate': 4.32e-06, 'epoch': 0.36}\n",
      "{'loss': 0.9473, 'grad_norm': 0.7372950315475464, 'learning_rate': 4.4e-06, 'epoch': 0.36}\n",
      "{'loss': 0.9139, 'grad_norm': 0.7302007675170898, 'learning_rate': 4.48e-06, 'epoch': 0.37}\n",
      "{'loss': 0.8891, 'grad_norm': 0.7551398873329163, 'learning_rate': 4.5599999999999995e-06, 'epoch': 0.38}\n",
      "{'loss': 0.9683, 'grad_norm': 0.739309549331665, 'learning_rate': 4.64e-06, 'epoch': 0.38}\n",
      "{'loss': 0.9367, 'grad_norm': 0.749015212059021, 'learning_rate': 4.72e-06, 'epoch': 0.39}\n",
      "{'loss': 0.8288, 'grad_norm': 0.7172075510025024, 'learning_rate': 4.8e-06, 'epoch': 0.39}\n",
      "{'loss': 0.9609, 'grad_norm': 0.7491523027420044, 'learning_rate': 4.88e-06, 'epoch': 0.4}\n",
      "{'loss': 0.953, 'grad_norm': 0.7591322064399719, 'learning_rate': 4.96e-06, 'epoch': 0.41}\n",
      "{'loss': 0.9121, 'grad_norm': 0.7522286176681519, 'learning_rate': 5.04e-06, 'epoch': 0.41}\n",
      "{'loss': 0.8591, 'grad_norm': 0.7370781302452087, 'learning_rate': 5.12e-06, 'epoch': 0.42}\n",
      "{'loss': 0.9269, 'grad_norm': 0.7647408246994019, 'learning_rate': 5.2e-06, 'epoch': 0.43}\n",
      "{'loss': 0.8881, 'grad_norm': 0.7233222723007202, 'learning_rate': 5.28e-06, 'epoch': 0.43}\n",
      "{'loss': 0.8801, 'grad_norm': 0.731185793876648, 'learning_rate': 5.36e-06, 'epoch': 0.44}\n",
      "{'loss': 0.9384, 'grad_norm': 0.7539277076721191, 'learning_rate': 5.4400000000000004e-06, 'epoch': 0.45}\n",
      "{'loss': 0.9127, 'grad_norm': 0.7376229166984558, 'learning_rate': 5.52e-06, 'epoch': 0.45}\n",
      "{'loss': 0.8884, 'grad_norm': 0.7619006037712097, 'learning_rate': 5.6e-06, 'epoch': 0.46}\n",
      "{'loss': 0.9769, 'grad_norm': 0.7685237526893616, 'learning_rate': 5.68e-06, 'epoch': 0.47}\n",
      "{'loss': 0.9195, 'grad_norm': 0.7442647218704224, 'learning_rate': 5.76e-06, 'epoch': 0.47}\n",
      "{'loss': 0.8883, 'grad_norm': 0.7686992287635803, 'learning_rate': 5.84e-06, 'epoch': 0.48}\n",
      "{'loss': 0.8777, 'grad_norm': 0.7303164005279541, 'learning_rate': 5.92e-06, 'epoch': 0.49}\n",
      "{'loss': 0.8308, 'grad_norm': 0.7201308012008667, 'learning_rate': 6e-06, 'epoch': 0.49}\n",
      "{'loss': 0.9516, 'grad_norm': 0.7926754951477051, 'learning_rate': 6.079999999999999e-06, 'epoch': 0.5}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 76/456 [19:27<1:27:00, 13.74s/it][2024-10-23 01:50:58,308] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:442423] [RANK:0] gather_len_batches: [241, 241, 241, 241, 241, 241, 241, 241]\u001b[39m\n",
      "\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–‰                                         | 2/30 [00:03<00:42,  1.53s/it]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 3/30 [00:05<00:56,  2.11s/it]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/30 [00:08<01:02,  2.41s/it]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 5/30 [00:11<01:04,  2.59s/it]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 6/30 [00:14<01:04,  2.70s/it]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                 | 7/30 [00:17<01:03,  2.77s/it]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 8/30 [00:20<01:01,  2.81s/it]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 9/30 [00:23<00:59,  2.86s/it]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 10/30 [00:26<00:57,  2.88s/it]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 11/30 [00:29<00:55,  2.91s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 12/30 [00:32<00:52,  2.91s/it]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 13/30 [00:35<00:49,  2.92s/it]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 14/30 [00:38<00:46,  2.93s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 15/30 [00:41<00:44,  2.95s/it]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 16/30 [00:44<00:41,  2.95s/it]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 17/30 [00:47<00:38,  2.96s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 18/30 [00:50<00:35,  2.96s/it]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 19/30 [00:53<00:32,  2.97s/it]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 20/30 [00:56<00:29,  2.96s/it]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 21/30 [00:59<00:26,  3.00s/it]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 22/30 [01:02<00:24,  3.03s/it]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 23/30 [01:05<00:21,  3.03s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 24/30 [01:08<00:18,  3.03s/it]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 25/30 [01:11<00:15,  3.03s/it]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 26/30 [01:14<00:12,  3.02s/it]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/30 [01:17<00:09,  3.02s/it]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/30 [01:20<00:06,  3.03s/it]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29/30 [01:23<00:03,  3.02s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5476625561714172, 'eval_runtime': 89.4039, 'eval_samples_per_second': 105.107, 'eval_steps_per_second': 1.644, 'epoch': 0.5}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 76/456 [20:56<1:27:00, 13.74s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:26<00:00,  2.99s/it]\u001b[A\n",
      "{'loss': 0.8508, 'grad_norm': 0.739250123500824, 'learning_rate': 6.1599999999999995e-06, 'epoch': 0.51}\n",
      "{'loss': 0.9303, 'grad_norm': 0.8037915825843811, 'learning_rate': 6.2399999999999995e-06, 'epoch': 0.51}\n",
      "{'loss': 0.9249, 'grad_norm': 0.7442043423652649, 'learning_rate': 6.32e-06, 'epoch': 0.52}\n",
      "{'loss': 0.8219, 'grad_norm': 0.711682915687561, 'learning_rate': 6.4e-06, 'epoch': 0.53}\n",
      "{'loss': 0.8984, 'grad_norm': 0.7492638230323792, 'learning_rate': 6.48e-06, 'epoch': 0.53}\n",
      "{'loss': 0.8891, 'grad_norm': 0.7120915651321411, 'learning_rate': 6.559999999999999e-06, 'epoch': 0.54}\n",
      "{'loss': 0.8555, 'grad_norm': 0.7281190752983093, 'learning_rate': 6.639999999999999e-06, 'epoch': 0.55}\n",
      "{'loss': 0.8545, 'grad_norm': 0.6940129995346069, 'learning_rate': 6.719999999999999e-06, 'epoch': 0.55}\n",
      "{'loss': 0.9467, 'grad_norm': 0.821199893951416, 'learning_rate': 6.799999999999999e-06, 'epoch': 0.56}\n",
      "{'loss': 0.846, 'grad_norm': 0.6922428607940674, 'learning_rate': 6.879999999999999e-06, 'epoch': 0.57}\n",
      "{'loss': 0.8804, 'grad_norm': 0.7701594829559326, 'learning_rate': 6.9599999999999994e-06, 'epoch': 0.57}\n",
      "{'loss': 0.8684, 'grad_norm': 0.7456250786781311, 'learning_rate': 7.0399999999999995e-06, 'epoch': 0.58}\n",
      "{'loss': 0.8991, 'grad_norm': 1.3318194150924683, 'learning_rate': 7.12e-06, 'epoch': 0.59}\n",
      "{'loss': 0.8844, 'grad_norm': 0.7559447884559631, 'learning_rate': 7.2e-06, 'epoch': 0.59}\n",
      "{'loss': 0.8767, 'grad_norm': 0.7887424230575562, 'learning_rate': 7.28e-06, 'epoch': 0.6}\n",
      "{'loss': 0.8531, 'grad_norm': 0.7184342741966248, 'learning_rate': 7.36e-06, 'epoch': 0.61}\n",
      "{'loss': 0.8529, 'grad_norm': 0.7244106531143188, 'learning_rate': 7.44e-06, 'epoch': 0.61}\n",
      "{'loss': 0.8931, 'grad_norm': 0.7772510647773743, 'learning_rate': 7.519999999999999e-06, 'epoch': 0.62}\n",
      "{'loss': 0.858, 'grad_norm': 0.7741959095001221, 'learning_rate': 7.599999999999999e-06, 'epoch': 0.62}\n",
      "{'loss': 0.8712, 'grad_norm': 0.7301244139671326, 'learning_rate': 7.68e-06, 'epoch': 0.63}\n",
      "{'loss': 0.9051, 'grad_norm': 0.7431594729423523, 'learning_rate': 7.76e-06, 'epoch': 0.64}\n",
      "{'loss': 0.8561, 'grad_norm': 0.7673982381820679, 'learning_rate': 7.84e-06, 'epoch': 0.64}\n",
      "{'loss': 0.8448, 'grad_norm': 0.7228937745094299, 'learning_rate': 7.92e-06, 'epoch': 0.65}\n",
      "{'loss': 0.8816, 'grad_norm': 0.7602968811988831, 'learning_rate': 8e-06, 'epoch': 0.66}\n",
      "{'loss': 0.8399, 'grad_norm': 0.7660859823226929, 'learning_rate': 7.97752808988764e-06, 'epoch': 0.66}\n",
      "{'loss': 0.8839, 'grad_norm': 0.8004366755485535, 'learning_rate': 7.95505617977528e-06, 'epoch': 0.67}\n",
      "{'loss': 0.8425, 'grad_norm': 0.743326723575592, 'learning_rate': 7.93258426966292e-06, 'epoch': 0.68}\n",
      "{'loss': 0.9009, 'grad_norm': 0.8006546497344971, 'learning_rate': 7.910112359550562e-06, 'epoch': 0.68}\n",
      "{'loss': 0.8893, 'grad_norm': 0.7760390639305115, 'learning_rate': 7.887640449438203e-06, 'epoch': 0.69}\n",
      "{'loss': 0.9019, 'grad_norm': 0.7478974461555481, 'learning_rate': 7.865168539325842e-06, 'epoch': 0.7}\n",
      "{'loss': 0.8817, 'grad_norm': 0.7727059125900269, 'learning_rate': 7.842696629213483e-06, 'epoch': 0.7}\n",
      "{'loss': 0.8468, 'grad_norm': 0.7285112738609314, 'learning_rate': 7.820224719101124e-06, 'epoch': 0.71}\n",
      "{'loss': 0.8714, 'grad_norm': 0.8152254819869995, 'learning_rate': 7.797752808988764e-06, 'epoch': 0.72}\n",
      "{'loss': 0.8474, 'grad_norm': 0.7310388088226318, 'learning_rate': 7.775280898876404e-06, 'epoch': 0.72}\n",
      "{'loss': 0.8865, 'grad_norm': 0.7936209440231323, 'learning_rate': 7.752808988764045e-06, 'epoch': 0.73}\n",
      "{'loss': 0.8424, 'grad_norm': 0.7045730352401733, 'learning_rate': 7.730337078651686e-06, 'epoch': 0.74}\n",
      "{'loss': 0.9066, 'grad_norm': 0.8514375686645508, 'learning_rate': 7.707865168539325e-06, 'epoch': 0.74}\n",
      "{'loss': 0.8703, 'grad_norm': 0.926544189453125, 'learning_rate': 7.685393258426966e-06, 'epoch': 0.75}\n",
      "{'loss': 0.8383, 'grad_norm': 0.7559124827384949, 'learning_rate': 7.662921348314607e-06, 'epoch': 0.76}\n",
      "{'loss': 0.8045, 'grad_norm': 0.7645379900932312, 'learning_rate': 7.640449438202247e-06, 'epoch': 0.76}\n",
      "{'loss': 0.8823, 'grad_norm': 0.7284439206123352, 'learning_rate': 7.6179775280898875e-06, 'epoch': 0.77}\n",
      "{'loss': 0.8107, 'grad_norm': 0.7670190930366516, 'learning_rate': 7.595505617977528e-06, 'epoch': 0.78}\n",
      "{'loss': 0.8215, 'grad_norm': 0.7397695183753967, 'learning_rate': 7.5730337078651685e-06, 'epoch': 0.78}\n",
      "{'loss': 0.8595, 'grad_norm': 0.8233634829521179, 'learning_rate': 7.5505617977528086e-06, 'epoch': 0.79}\n",
      "{'loss': 0.8951, 'grad_norm': 0.7571905851364136, 'learning_rate': 7.5280898876404495e-06, 'epoch': 0.8}\n",
      "{'loss': 0.8533, 'grad_norm': 0.7672440409660339, 'learning_rate': 7.5056179775280895e-06, 'epoch': 0.8}\n",
      "{'loss': 0.8277, 'grad_norm': 0.7708967328071594, 'learning_rate': 7.4831460674157305e-06, 'epoch': 0.81}\n",
      "{'loss': 0.8998, 'grad_norm': 0.7816552519798279, 'learning_rate': 7.46067415730337e-06, 'epoch': 0.82}\n",
      "{'loss': 0.7932, 'grad_norm': 0.7311809062957764, 'learning_rate': 7.438202247191011e-06, 'epoch': 0.82}\n",
      "{'loss': 0.8821, 'grad_norm': 0.8017817139625549, 'learning_rate': 7.4157303370786515e-06, 'epoch': 0.83}\n",
      "{'loss': 0.8675, 'grad_norm': 0.7862182259559631, 'learning_rate': 7.3932584269662916e-06, 'epoch': 0.84}\n",
      "{'loss': 0.8312, 'grad_norm': 0.7237002849578857, 'learning_rate': 7.3707865168539325e-06, 'epoch': 0.84}\n",
      "{'loss': 0.8362, 'grad_norm': 0.7229022979736328, 'learning_rate': 7.3483146067415725e-06, 'epoch': 0.85}\n",
      "{'loss': 0.8761, 'grad_norm': 0.7399474382400513, 'learning_rate': 7.3258426966292134e-06, 'epoch': 0.86}\n",
      "{'loss': 0.8288, 'grad_norm': 0.700545072555542, 'learning_rate': 7.3033707865168535e-06, 'epoch': 0.86}\n",
      "{'loss': 0.8235, 'grad_norm': 0.723656177520752, 'learning_rate': 7.2808988764044944e-06, 'epoch': 0.87}\n",
      "{'loss': 0.9101, 'grad_norm': 0.7364098429679871, 'learning_rate': 7.2584269662921345e-06, 'epoch': 0.88}\n",
      "{'loss': 0.8525, 'grad_norm': 0.7468666434288025, 'learning_rate': 7.2359550561797746e-06, 'epoch': 0.88}\n",
      "{'loss': 0.8487, 'grad_norm': 0.7427132725715637, 'learning_rate': 7.2134831460674155e-06, 'epoch': 0.89}\n",
      "{'loss': 0.8587, 'grad_norm': 0.7525140643119812, 'learning_rate': 7.1910112359550555e-06, 'epoch': 0.89}\n",
      "{'loss': 0.8771, 'grad_norm': 0.7914164662361145, 'learning_rate': 7.1685393258426964e-06, 'epoch': 0.9}\n",
      "{'loss': 0.8568, 'grad_norm': 0.741431713104248, 'learning_rate': 7.1460674157303365e-06, 'epoch': 0.91}\n",
      "{'loss': 0.8689, 'grad_norm': 0.8071787357330322, 'learning_rate': 7.123595505617977e-06, 'epoch': 0.91}\n",
      "{'loss': 0.847, 'grad_norm': 0.731109619140625, 'learning_rate': 7.1011235955056175e-06, 'epoch': 0.92}\n",
      "{'loss': 0.819, 'grad_norm': 0.7453051209449768, 'learning_rate': 7.078651685393258e-06, 'epoch': 0.93}\n",
      "{'loss': 0.8134, 'grad_norm': 0.6930183172225952, 'learning_rate': 7.056179775280899e-06, 'epoch': 0.93}\n",
      "{'loss': 0.8387, 'grad_norm': 0.7323773503303528, 'learning_rate': 7.0337078651685385e-06, 'epoch': 0.94}\n",
      "{'loss': 0.8227, 'grad_norm': 0.719099760055542, 'learning_rate': 7.0112359550561794e-06, 'epoch': 0.95}\n",
      "{'loss': 0.8268, 'grad_norm': 0.7303609848022461, 'learning_rate': 6.9887640449438195e-06, 'epoch': 0.95}\n",
      "{'loss': 0.8677, 'grad_norm': 0.7478364706039429, 'learning_rate': 6.96629213483146e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8667, 'grad_norm': 0.7587575316429138, 'learning_rate': 6.9438202247191005e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7942, 'grad_norm': 0.7226529121398926, 'learning_rate': 6.921348314606741e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8355, 'grad_norm': 0.7552550435066223, 'learning_rate': 6.898876404494382e-06, 'epoch': 0.98}\n",
      "{'loss': 0.8242, 'grad_norm': 0.7158328890800476, 'learning_rate': 6.876404494382022e-06, 'epoch': 0.99}\n",
      "{'loss': 0.8502, 'grad_norm': 0.748361349105835, 'learning_rate': 6.853932584269663e-06, 'epoch': 0.99}\n",
      "{'loss': 0.8215, 'grad_norm': 0.6786748170852661, 'learning_rate': 6.8314606741573025e-06, 'epoch': 1.0}\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 152/456 [38:36<1:10:14, 13.86s/it][2024-10-23 02:10:07,273] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:442423] [RANK:0] gather_len_batches: [241, 241, 241, 241, 241, 241, 241, 241]\u001b[39m\n",
      "\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–‰                                         | 2/30 [00:02<00:40,  1.46s/it]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 3/30 [00:05<00:56,  2.09s/it]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/30 [00:08<01:02,  2.39s/it]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 5/30 [00:11<01:04,  2.56s/it]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 6/30 [00:14<01:04,  2.68s/it]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                 | 7/30 [00:17<01:04,  2.79s/it]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 8/30 [00:20<01:02,  2.82s/it]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 9/30 [00:23<00:59,  2.85s/it]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 10/30 [00:26<00:57,  2.88s/it]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 11/30 [00:29<00:55,  2.92s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 12/30 [00:32<00:52,  2.93s/it]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 13/30 [00:35<00:50,  2.95s/it]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 14/30 [00:38<00:47,  2.94s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 15/30 [00:41<00:44,  2.96s/it]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 16/30 [00:44<00:41,  2.95s/it]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 17/30 [00:47<00:38,  2.96s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 18/30 [00:50<00:35,  2.95s/it]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 19/30 [00:53<00:32,  2.97s/it]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 20/30 [00:56<00:29,  2.96s/it]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 21/30 [00:59<00:26,  2.97s/it]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 22/30 [01:02<00:23,  2.97s/it]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 23/30 [01:04<00:20,  2.97s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 24/30 [01:07<00:17,  2.97s/it]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 25/30 [01:10<00:14,  2.97s/it]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 26/30 [01:13<00:11,  2.98s/it]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/30 [01:17<00:09,  3.06s/it]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/30 [01:20<00:06,  3.14s/it]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29/30 [01:23<00:03,  3.13s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5100435614585876, 'eval_runtime': 89.5988, 'eval_samples_per_second': 104.879, 'eval_steps_per_second': 1.641, 'epoch': 1.0}\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 152/456 [40:05<1:10:14, 13.86s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:26<00:00,  3.07s/it]\u001b[A\n",
      "{'loss': 0.845, 'grad_norm': 0.7694020867347717, 'learning_rate': 6.808988764044943e-06, 'epoch': 1.01}\n",
      "{'loss': 0.7571, 'grad_norm': 0.9739260673522949, 'learning_rate': 6.7865168539325835e-06, 'epoch': 1.01}\n",
      "{'loss': 0.773, 'grad_norm': 0.7655594944953918, 'learning_rate': 6.764044943820224e-06, 'epoch': 1.01}\n",
      "{'loss': 0.7798, 'grad_norm': 0.8208426237106323, 'learning_rate': 6.7415730337078645e-06, 'epoch': 1.02}\n",
      "{'loss': 0.7944, 'grad_norm': 0.9579406976699829, 'learning_rate': 6.719101123595505e-06, 'epoch': 1.03}\n",
      "{'loss': 0.7478, 'grad_norm': 0.826525092124939, 'learning_rate': 6.696629213483146e-06, 'epoch': 1.03}\n",
      "{'loss': 0.7604, 'grad_norm': 0.785991907119751, 'learning_rate': 6.674157303370786e-06, 'epoch': 1.04}\n",
      "{'loss': 0.7152, 'grad_norm': 0.8087158799171448, 'learning_rate': 6.651685393258427e-06, 'epoch': 1.05}\n",
      "{'loss': 0.7727, 'grad_norm': 0.8449831008911133, 'learning_rate': 6.6292134831460665e-06, 'epoch': 1.05}\n",
      "{'loss': 0.7613, 'grad_norm': 0.8393862247467041, 'learning_rate': 6.606741573033707e-06, 'epoch': 1.06}\n",
      "{'loss': 0.7717, 'grad_norm': 0.860580325126648, 'learning_rate': 6.5842696629213475e-06, 'epoch': 1.07}\n",
      "{'loss': 0.7419, 'grad_norm': 0.8222426176071167, 'learning_rate': 6.561797752808988e-06, 'epoch': 1.07}\n",
      "{'loss': 0.7428, 'grad_norm': 0.8830024003982544, 'learning_rate': 6.539325842696629e-06, 'epoch': 1.08}\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 165/456 [44:39<1:12:42, 14.99s/it]"
     ]
    }
   ],
   "source": [
    "!accelerate launch -m axolotl.cli.train mi300x-llama3.1-8b-fft.dsz2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c14ab-0deb-4a7e-9715-c63cd3337958",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch -m axolotl.cli.train mi300x-llama3.1-8b-fft.dsz3.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:axolotl]",
   "language": "python",
   "name": "conda-env-axolotl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
