{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0f8a78-6669-4822-b6d7-4648c45530e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv setup, select in the top right - newer cmake usually required vs what's selected by Ubuntu LTS\n",
    "# mamba create -n vllm python=3.11 ipykernel cmake ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8feefc6e-ec0c-406c-81b3-5b1c86691d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/rocm6.2\n",
      "Requirement already satisfied: torch in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (2.6.0.dev20241015+rocm6.2)\n",
      "Requirement already satisfied: torchvision in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (0.20.0.dev20241015+rocm6.2)\n",
      "Requirement already satisfied: torchaudio in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (2.5.0.dev20241015+rocm6.2)\n",
      "Requirement already satisfied: filelock in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.1.0+cf34004b8a in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch) (3.1.0+cf34004b8a)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# PyTorch Nightly\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.2 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dee35fb-64fd-4814-b460-898911fd3189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: triton in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from triton) (3.13.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Triton\n",
    "!pip install triton -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f1f4ab-00ea-4003-ba37-6daab3607f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme1n1p1/MI300-testing/amd_smi\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /mnt/nvme1n1p1/MI300-testing/amd_smi\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from amdsmi==24.6.3+52b3947) (6.0.2)\n",
      "Requirement already satisfied: clang>=14.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from amdsmi==24.6.3+52b3947) (17.0.6)\n",
      "Building wheels for collected packages: amdsmi\n",
      "  Building wheel for amdsmi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for amdsmi: filename=amdsmi-24.6.3+52b3947-py3-none-any.whl size=642971 sha256=a40bb629e4586caa00e52ad2282a05feaeb8a9022fa81d4fb077f3ec9ac9a688\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-10gvryf5/wheels/3e/ac/8a/3d86efb89ef5c48b5b58bcce504bb804ee69c8dc7a86c9606d\n",
      "Successfully built amdsmi\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: amdsmi\n",
      "  Attempting uninstall: amdsmi\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: amdsmi 24.6.3+52b3947\n",
      "    Uninstalling amdsmi-24.6.3+52b3947:\n",
      "      Successfully uninstalled amdsmi-24.6.3+52b3947\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed amdsmi-24.6.3+52b3947\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m/mnt/nvme1n1p1/MI300-testing\n"
     ]
    }
   ],
   "source": [
    "# AMD SMI - permission issues if you don't copy the folder\n",
    "!cp -r /opt/rocm/share/amd_smi ./\n",
    "%cd amd_smi\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ac4d5b0-7d83-42c8-a042-672673a75448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vllm' already exists and is not an empty directory.\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "# vLLM time\n",
    "!git clone https://github.com/vllm-project/vllm\n",
    "%cd vllm\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2339afc3-9549-4c21-bcdd-e952a9f4f03a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numba in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (0.60.0)\n",
      "Requirement already satisfied: scipy in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (0.25.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from numba) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests->huggingface-hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests->huggingface-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests->huggingface-hub) (2024.8.30)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy<2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mIgnoring fastapi: markers 'python_version < \"3.9\"' don't match your environment\n",
      "Ignoring six: markers 'python_version > \"3.11\"' don't match your environment\n",
      "Ignoring setuptools: markers 'python_version > \"3.11\"' don't match your environment\n",
      "Requirement already satisfied: psutil in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: sentencepiece in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 5)) (4.66.5)\n",
      "Requirement already satisfied: py-cpuinfo in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 6)) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.45.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 7)) (4.45.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: protobuf in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 9)) (5.28.2)\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 11)) (0.115.0)\n",
      "Requirement already satisfied: aiohttp in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 12)) (3.10.9)\n",
      "Requirement already satisfied: openai>=1.40.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 13)) (1.51.2)\n",
      "Requirement already satisfied: pydantic>=2.9 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 15)) (2.9.2)\n",
      "Requirement already satisfied: pillow in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 16)) (10.4.0)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 17)) (0.21.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 18)) (7.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 19)) (0.7.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.10.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 20)) (0.10.6)\n",
      "Requirement already satisfied: outlines<0.1,>=0.0.43 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 21)) (0.0.46)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 22)) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.10.4 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 23)) (3.13.1)\n",
      "Requirement already satisfied: partial-json-parser in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 24)) (0.2.1.1.post4)\n",
      "Requirement already satisfied: pyzmq in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 25)) (26.2.0)\n",
      "Requirement already satisfied: msgspec in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 26)) (0.18.6)\n",
      "Requirement already satisfied: gguf==0.10.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 27)) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 28)) (8.5.0)\n",
      "Requirement already satisfied: mistral_common>=1.4.4 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from mistral_common[opencv]>=1.4.4->-r requirements-common.txt (line 29)) (1.4.4)\n",
      "Requirement already satisfied: pyyaml in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 30)) (6.0.2)\n",
      "Requirement already satisfied: einops in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 33)) (0.8.0)\n",
      "Collecting compressed-tensors==0.6.0 (from -r requirements-common.txt (line 34))\n",
      "  Downloading compressed_tensors-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: awscli in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-rocm.txt (line 5)) (1.35.3)\n",
      "Requirement already satisfied: boto3 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-rocm.txt (line 6)) (1.35.37)\n",
      "Requirement already satisfied: botocore in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-rocm.txt (line 7)) (1.35.37)\n",
      "Requirement already satisfied: ray>=2.10.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-rocm.txt (line 8)) (2.37.0)\n",
      "Requirement already satisfied: peft in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-rocm.txt (line 9)) (0.13.1)\n",
      "Requirement already satisfied: pytest-asyncio in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-rocm.txt (line 10)) (0.24.0)\n",
      "Requirement already satisfied: tensorizer>=2.9.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-rocm.txt (line 11)) (2.9.0)\n",
      "Requirement already satisfied: uvicorn[standard] in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from -r requirements-common.txt (line 14)) (0.31.1)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from lm-format-enforcer==0.10.6->-r requirements-common.txt (line 20)) (0.3.3)\n",
      "Requirement already satisfied: packaging in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from lm-format-enforcer==0.10.6->-r requirements-common.txt (line 20)) (24.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from compressed-tensors==0.6.0->-r requirements-common.txt (line 34)) (2.6.0.dev20241015+rocm6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests>=2.26.0->-r requirements-common.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests>=2.26.0->-r requirements-common.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests>=2.26.0->-r requirements-common.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from requests>=2.26.0->-r requirements-common.txt (line 4)) (2024.8.30)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from transformers>=4.45.0->-r requirements-common.txt (line 7)) (0.25.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from transformers>=4.45.0->-r requirements-common.txt (line 7)) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from transformers>=4.45.0->-r requirements-common.txt (line 7)) (0.4.5)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->-r requirements-common.txt (line 11)) (0.38.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from aiohttp->-r requirements-common.txt (line 12)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from aiohttp->-r requirements-common.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from aiohttp->-r requirements-common.txt (line 12)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from aiohttp->-r requirements-common.txt (line 12)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from aiohttp->-r requirements-common.txt (line 12)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from aiohttp->-r requirements-common.txt (line 12)) (1.14.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from openai>=1.40.0->-r requirements-common.txt (line 13)) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from openai>=1.40.0->-r requirements-common.txt (line 13)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from openai>=1.40.0->-r requirements-common.txt (line 13)) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from openai>=1.40.0->-r requirements-common.txt (line 13)) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from openai>=1.40.0->-r requirements-common.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->-r requirements-common.txt (line 14)) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->-r requirements-common.txt (line 14)) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->-r requirements-common.txt (line 14)) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->-r requirements-common.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->-r requirements-common.txt (line 14)) (0.20.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->-r requirements-common.txt (line 14)) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->-r requirements-common.txt (line 14)) (13.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from pydantic>=2.9->-r requirements-common.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from pydantic>=2.9->-r requirements-common.txt (line 15)) (2.23.4)\n",
      "Requirement already satisfied: jinja2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (3.1.4)\n",
      "Requirement already satisfied: lark in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (1.2.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (3.0.0)\n",
      "Requirement already satisfied: diskcache in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (5.6.3)\n",
      "Requirement already satisfied: numba in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (0.60.0)\n",
      "Requirement already satisfied: referencing in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (4.23.0)\n",
      "Requirement already satisfied: datasets in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (3.0.1)\n",
      "Requirement already satisfied: pycountry in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (24.6.1)\n",
      "Requirement already satisfied: pyairports in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (2.1.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from importlib_metadata->-r requirements-common.txt (line 28)) (3.20.2)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from awscli->-r requirements-rocm.txt (line 5)) (0.16)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from awscli->-r requirements-rocm.txt (line 5)) (0.10.3)\n",
      "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from awscli->-r requirements-rocm.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from awscli->-r requirements-rocm.txt (line 5)) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from botocore->-r requirements-rocm.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from botocore->-r requirements-rocm.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from ray>=2.10.0->-r requirements-rocm.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from peft->-r requirements-rocm.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: pytest<9,>=8.2 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from pytest-asyncio->-r requirements-rocm.txt (line 10)) (8.3.3)\n",
      "Requirement already satisfied: redis>=4.5.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from tensorizer>=2.9.0->-r requirements-rocm.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: hiredis>=2.2.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from tensorizer>=2.9.0->-r requirements-rocm.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: libnacl>=2.1.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from tensorizer>=2.9.0->-r requirements-rocm.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from mistral_common[opencv]>=1.4.4->-r requirements-common.txt (line 29)) (4.10.0.84)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.40.0->-r requirements-common.txt (line 13)) (1.0.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.45.0->-r requirements-common.txt (line 7)) (2024.6.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (0.20.0)\n",
      "Requirement already satisfied: iniconfig in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from pytest<9,>=8.2->pytest-asyncio->-r requirements-rocm.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from pytest<9,>=8.2->pytest-asyncio->-r requirements-rocm.txt (line 10)) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->-r requirements-rocm.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from rsa<4.8,>=3.1.2->awscli->-r requirements-rocm.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: networkx in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch>=1.7.0->compressed-tensors==0.6.0->-r requirements-common.txt (line 34)) (3.3)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.1.0+cf34004b8a in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch>=1.7.0->compressed-tensors==0.6.0->-r requirements-common.txt (line 34)) (3.1.0+cf34004b8a)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from torch>=1.7.0->compressed-tensors==0.6.0->-r requirements-common.txt (line 34)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.7.0->compressed-tensors==0.6.0->-r requirements-common.txt (line 34)) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->-r requirements-common.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from datasets->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from datasets->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from datasets->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from datasets->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from datasets->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (0.70.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jinja2->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (2.1.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from numba->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->-r requirements-common.txt (line 21)) (2024.2)\n",
      "Downloading compressed_tensors-0.6.0-py3-none-any.whl (92 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: compressed-tensors\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed compressed-tensors-0.6.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "!pip install numba scipy huggingface-hub -U\n",
    "!pip install \"numpy<2\" -U\n",
    "!pip install -r requirements-rocm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b60e8bc-a646-4cc2-b4db-8d71b560a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: setuptools_scm in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (8.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from setuptools_scm) (24.1)\n",
      "Requirement already satisfied: setuptools in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from setuptools_scm) (75.1.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Undocumented dependencies\n",
    "!pip install setuptools_scm -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fb58d19-e28a-46d3-8737-0ca6238585d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running develop\n",
      "/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  easy_install.initialize_options(self)\n",
      "/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "running egg_info\n",
      "writing vllm.egg-info/PKG-INFO\n",
      "writing dependency_links to vllm.egg-info/dependency_links.txt\n",
      "writing entry points to vllm.egg-info/entry_points.txt\n",
      "writing requirements to vllm.egg-info/requires.txt\n",
      "writing top-level names to vllm.egg-info/top_level.txt\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'vllm.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "-- Build type: RelWithDebInfo\n",
      "-- Target device: cuda\n",
      "-- Found python matching: /home/hotaisle/miniforge3/envs/vllm/bin/python.\n",
      "\u001b[0mBuilding PyTorch for GPU arch: gfx942\u001b[0m\n",
      "\u001b[0mHIP VERSION: 6.2.41134-65d174c3e\u001b[0m\n",
      "\u001b[0m\n",
      "***** ROCm version from rocm_version.h ****\n",
      "\u001b[0m\n",
      "\u001b[0mROCM_VERSION_DEV: 6.2.2\u001b[0m\n",
      "\u001b[0mROCM_VERSION_DEV_MAJOR: 6\u001b[0m\n",
      "\u001b[0mROCM_VERSION_DEV_MINOR: 2\u001b[0m\n",
      "\u001b[0mROCM_VERSION_DEV_PATCH: 2\u001b[0m\n",
      "\u001b[0mROCM_VERSION_DEV_INT:   60202\u001b[0m\n",
      "\u001b[0mHIP_VERSION_MAJOR: 6\u001b[0m\n",
      "\u001b[0mHIP_VERSION_MINOR: 2\u001b[0m\n",
      "\u001b[0mTORCH_HIP_VERSION: 602\u001b[0m\n",
      "\u001b[0m\n",
      "***** Library versions from cmake find_package *****\n",
      "\u001b[0m\n",
      "\u001b[0mhip VERSION: 6.2.41134\u001b[0m\n",
      "\u001b[0mhsa-runtime64 VERSION: 1.14.60202\u001b[0m\n",
      "\u001b[0mamd_comgr VERSION: 2.8.0\u001b[0m\n",
      "\u001b[0mrocrand VERSION: 3.1.0\u001b[0m\n",
      "\u001b[0mhiprand VERSION: 2.11.0\u001b[0m\n",
      "\u001b[0mrocblas VERSION: 4.2.1\u001b[0m\n",
      "\u001b[0mhipblas VERSION: 2.2.0\u001b[0m\n",
      "\u001b[0mhipblaslt VERSION: 0.8.0\u001b[0m\n",
      "\u001b[0mmiopen VERSION: 3.2.0\u001b[0m\n",
      "\u001b[0mhipfft VERSION: 1.0.15\u001b[0m\n",
      "\u001b[0mhipsparse VERSION: 3.1.1\u001b[0m\n",
      "\u001b[0mrccl VERSION: 2.20.5\u001b[0m\n",
      "\u001b[0mrocprim VERSION: 3.2.0\u001b[0m\n",
      "\u001b[0mhipcub VERSION: 3.2.0\u001b[0m\n",
      "\u001b[0mrocthrust VERSION: 3.1.0\u001b[0m\n",
      "\u001b[0mhipsolver VERSION: 2.2.0\u001b[0m\n",
      "\u001b[0mCMake Deprecation Warning at /opt/rocm/lib/cmake/hiprtc/hiprtc-config.cmake:21 (cmake_minimum_required):\n",
      "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "  CMake.\n",
      "\n",
      "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  CMake that the project does not need compatibility with older versions.\n",
      "Call Stack (most recent call first):\n",
      "  /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/torch/share/cmake/Caffe2/public/LoadHIP.cmake:56 (find_package)\n",
      "  /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/torch/share/cmake/Caffe2/public/LoadHIP.cmake:131 (find_package_and_print_version)\n",
      "  /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:74 (include)\n",
      "  /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
      "  CMakeLists.txt:84 (find_package)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[0mhiprtc VERSION: 6.2.41134\u001b[0m\n",
      "\u001b[0mHIP is using new type enums\u001b[0m\n",
      "\u001b[33mCMake Warning at /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
      "  static library kineto_LIBRARY-NOTFOUND not found.\n",
      "Call Stack (most recent call first):\n",
      "  /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\n",
      "  CMakeLists.txt:84 (find_package)\n",
      "\n",
      "\u001b[0m\n",
      "-- Enabling core extension.\n",
      "\u001b[33mCMake Warning at CMakeLists.txt:139 (message):\n",
      "  Pytorch version >= 2.5.0 expected for ROCm build, saw 2.6.0 instead.\n",
      "\n",
      "\u001b[0m\n",
      "-- HIP supported arches: gfx906;gfx908;gfx90a;gfx940;gfx941;gfx942;gfx1030;gfx1100\n",
      "-- FetchContent base directory: /mnt/nvme1n1p1/MI300-testing/vllm/.deps\n",
      "-- Enabling C extension.\n",
      "-- Enabling moe extension.\n",
      "-- Build type: RelWithDebInfo\n",
      "-- Target device: cuda\n",
      "-- Building vllm-flash-attn inside vLLM. Skipping flag detection and relying on parent build.\n",
      "-- vllm-flash-attn is available at /mnt/nvme1n1p1/MI300-testing/vllm/.deps/vllm-flash-attn-src\n",
      "-- Configuring done (6.7s)\n",
      "-- Generating done (0.1s)\n",
      "-- Build files have been written to: /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311\n",
      "[1/19] Running hipify on _rocm_C extension source files.\u001b[K\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_generic.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_generic.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_fp8.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_fp8.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8_impl.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8_impl.h [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8.h [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_float32.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_float32.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_bfloat16.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_bfloat16_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/quant_utils.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/quant_utils_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/rocm/attention.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/rocm/attention.hip [skipped, already hipified]\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n",
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 2\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/rocm/attention.hip\n",
      "[2/19] Running hipify on _moe_C extension source files.\u001b[Km/attention.hip.o\u001b[K\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cuda_compat.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_compat.h [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/moe/topk_softmax_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/moe/topk_softmax_kernels.hip [skipped, already hipified]\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n",
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 3\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/moe/topk_softmax_kernels.hip\n",
      "[5/19] Running hipify on _C extension source files.\u001b[Kopk_softmax_kernels.hip.o\u001b[K\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cuda_compat.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_compat.h [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/dispatch_utils.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/dispatch_utils.h [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8_impl.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8_impl.h [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/hip_float8.h [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_generic.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_generic.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_fp8.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_fp8.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_float32.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_float32.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_bfloat16.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_bfloat16_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/quant_utils.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/amd/quant_utils_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_float16.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/dtype_float16.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_dtypes.h -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_dtypes_hip.h [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/nvidia/quant_utils.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/nvidia/quant_utils_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_utils.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_utils_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/pos_encoding_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/pos_encoding_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/activation_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/activation_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/layernorm_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/layernorm_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/compat.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/compat.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_util.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_util.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/matrix_view.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/matrix_view_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_2.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_2.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_3.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_3.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_4.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_4.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_8.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/qdq_8.cuh [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/compressed_tensors/int8_quant_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/compressed_tensors/int8_quant_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/common.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/common.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cuda_utils_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_utils_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/moe_align_block_size_kernels.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/moe_align_block_size_kernels.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.cuh -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step_hip.cuh [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.cu -> /mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip [skipped, already hipified]\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n",
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 34\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/pos_encoding_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/activation_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/layernorm_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/compressed_tensors/int8_quant_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/fp8/common.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_utils_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/moe_align_block_size_kernels.hip\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip\n",
      "[6/19] Building HIP object CMakeFiles/_C.dir/csrc/hip_utils_kernels.hip.o\u001b[Kp.o\u001b[KK\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_utils_kernels.hip:9:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "    9 |     hipGetDevice(&device);\n",
      "      |     ^~~~~~~~~~~~ ~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_utils_kernels.hip:13:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "   13 |   hipDeviceGetAttribute(&value, static_cast<hipDeviceAttribute_t>(attribute),\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   14 |                          device);\n",
      "      |                          ~~~~~~\n",
      "2 warnings generated when compiling for gfx942.\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_utils_kernels.hip:9:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "    9 |     hipGetDevice(&device);\n",
      "      |     ^~~~~~~~~~~~ ~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/hip_utils_kernels.hip:13:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "   13 |   hipDeviceGetAttribute(&value, static_cast<hipDeviceAttribute_t>(attribute),\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   14 |                          device);\n",
      "      |                          ~~~~~~\n",
      "2 warnings generated when compiling for host.\n",
      "[8/19] Building HIP object CMakeFiles/_C.dir/csrc/cache_kernels.hip.o\u001b[K.o\u001b[K\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:62:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "   62 |     hipMemcpyAsync(dst_ptr + dst_offset, src_ptr + src_offset,\n",
      "      |     ^~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   63 |                     block_size_in_bytes, memcpy_type, stream);\n",
      "      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:116:26: warning: variable length arrays in C++ are a Clang extension [-Wvla-cxx-extension]\n",
      "  116 |   int64_t key_cache_ptrs[num_layers];\n",
      "      |                          ^~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:116:26: note: read of non-const variable 'num_layers' is not allowed in a constant expression\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:106:7: note: declared here\n",
      "  106 |   int num_layers = key_caches.size();\n",
      "      |       ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:117:28: warning: variable length arrays in C++ are a Clang extension [-Wvla-cxx-extension]\n",
      "  117 |   int64_t value_cache_ptrs[num_layers];\n",
      "      |                            ^~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:117:28: note: read of non-const variable 'num_layers' is not allowed in a constant expression\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:106:7: note: declared here\n",
      "  106 |   int num_layers = key_caches.size();\n",
      "      |       ^\n",
      "3 warnings generated when compiling for gfx942.\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:62:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "   62 |     hipMemcpyAsync(dst_ptr + dst_offset, src_ptr + src_offset,\n",
      "      |     ^~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   63 |                     block_size_in_bytes, memcpy_type, stream);\n",
      "      |                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:116:26: warning: variable length arrays in C++ are a Clang extension [-Wvla-cxx-extension]\n",
      "  116 |   int64_t key_cache_ptrs[num_layers];\n",
      "      |                          ^~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:116:26: note: read of non-const variable 'num_layers' is not allowed in a constant expression\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:106:7: note: declared here\n",
      "  106 |   int num_layers = key_caches.size();\n",
      "      |       ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:117:28: warning: variable length arrays in C++ are a Clang extension [-Wvla-cxx-extension]\n",
      "  117 |   int64_t value_cache_ptrs[num_layers];\n",
      "      |                            ^~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:117:28: note: read of non-const variable 'num_layers' is not allowed in a constant expression\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/cache_kernels.hip:106:7: note: declared here\n",
      "  106 |   int num_layers = key_caches.size();\n",
      "      |       ^\n",
      "3 warnings generated when compiling for host.\n",
      "[11/19] Building HIP object CMakeFiles...ir/csrc/quantization/gptq/q_gemm.hip.o\u001b[K\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1776:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1776 |     hipMalloc(&new_qweight, height / 32 * bit * width * sizeof(uint32_t));\n",
      "      |     ^~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1797:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1797 |     hipMemcpyAsync(q_weight, new_qweight,\n",
      "      |     ^~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1801:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1801 |     hipDeviceSynchronize();\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1802:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1802 |     hipFree(new_qweight);\n",
      "      |     ^~~~~~~ ~~~~~~~~~~~\n",
      "4 warnings generated when compiling for gfx942.\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1776:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1776 |     hipMalloc(&new_qweight, height / 32 * bit * width * sizeof(uint32_t));\n",
      "      |     ^~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1797:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1797 |     hipMemcpyAsync(q_weight, new_qweight,\n",
      "      |     ^~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1801:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1801 |     hipDeviceSynchronize();\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/quantization/gptq/q_gemm.hip:1802:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      " 1802 |     hipFree(new_qweight);\n",
      "      |     ^~~~~~~ ~~~~~~~~~~~\n",
      "4 warnings generated when compiling for host.\n",
      "[15/19] Building HIP object CMakeFiles...csrc/prepare_inputs/advance_step.hip.o\u001b[KK\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip:195:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  195 |   hipDeviceGetAttribute(&blocks, hipDeviceAttributeMultiprocessorCount, dev);\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip:250:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  250 |   hipDeviceGetAttribute(&blocks, hipDeviceAttributeMultiprocessorCount, dev);\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip:251:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  251 |   hipDeviceGetAttribute(&threads, hipDeviceAttributeMaxThreadsPerBlock, dev);\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "3 warnings generated when compiling for gfx942.\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip:195:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  195 |   hipDeviceGetAttribute(&blocks, hipDeviceAttributeMultiprocessorCount, dev);\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip:250:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  250 |   hipDeviceGetAttribute(&blocks, hipDeviceAttributeMultiprocessorCount, dev);\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/prepare_inputs/advance_step.hip:251:3: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  251 |   hipDeviceGetAttribute(&threads, hipDeviceAttributeMaxThreadsPerBlock, dev);\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "3 warnings generated when compiling for host.\n",
      "[18/19] Building HIP object CMakeFiles...csrc/attention/attention_kernels.hip.o\u001b[K\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  184 |   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:663:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  663 |   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  184 |   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:663:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  663 |   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  184 |   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:663:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  663 |   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  184 |   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:663:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  663 |   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  184 |   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:663:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  663 |   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  184 |   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:663:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  663 |   for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "  184 |   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n",
      "      |   ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:184:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n",
      "402 warnings generated when compiling for gfx942.\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:797:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  797 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:800:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  800 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  829 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:803:7: note: expanded from macro 'CALL_V1_LAUNCHER_BLOCK_SIZE'\n",
      "  803 |       CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:783:3: note: expanded from macro 'CALL_V1_LAUNCHER_SPARSITY'\n",
      "  783 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:551:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  551 |         FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:553:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  553 |         FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:555:9: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  555 |         FN(__hip_bfloat16, __hip_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n",
      "      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:562:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  562 |           FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:564:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  564 |           FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:962:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  962 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:965:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  965 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:998:30: warning: switch condition has boolean value [-Wswitch-bool]\n",
      "  997 |   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n",
      "      |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  998 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../quantization/fp8/amd/quant_utils_hip.cuh:566:11: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'\n",
      "  566 |           FN(__hip_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n",
      "      |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:968:7: note: expanded from macro 'CALL_V2_LAUNCHER_BLOCK_SIZE'\n",
      "  968 |       CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:948:3: note: expanded from macro 'CALL_V2_LAUNCHER_SPARSITY'\n",
      "  948 |   switch (is_block_sparse) {                                               \\\n",
      "      |   ^       ~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:745:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  745 |       LAUNCH_PAGED_ATTENTION_V1(64);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:830:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here\n",
      "  830 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)\n",
      "      |                              ^\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:748:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  748 |       LAUNCH_PAGED_ATTENTION_V1(80);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:751:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  751 |       LAUNCH_PAGED_ATTENTION_V1(96);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:754:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  754 |       LAUNCH_PAGED_ATTENTION_V1(112);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:757:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  757 |       LAUNCH_PAGED_ATTENTION_V1(120);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:760:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  760 |       LAUNCH_PAGED_ATTENTION_V1(128);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:763:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  763 |       LAUNCH_PAGED_ATTENTION_V1(192);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:766:7: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  766 |       LAUNCH_PAGED_ATTENTION_V1(256);\n",
      "      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/attention_kernels.hip:676:3: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'\n",
      "  676 |   VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\\n",
      "      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  677 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  678 |                                               BLOCK_SIZE, NUM_THREADS,      \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  679 |                                               KV_DTYPE, IS_BLOCK_SPARSE>),  \\\n",
      "      |                                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  680 |       shared_mem_size);                                                     \\\n",
      "      |       ~~~~~~~~~~~~~~~~\n",
      "/mnt/nvme1n1p1/MI300-testing/vllm/build/temp.linux-x86_64-cpython-311/csrc/attention/../hip_compat.h:49:5: note: expanded from macro 'VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize'\n",
      "   49 |     hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "332 warnings generated when compiling for host.\n",
      "[19/19] Linking HIP shared module _C.abi3.so\u001b[K\n",
      "-- Install configuration: \"RelWithDebInfo\"\n",
      "-- Up-to-date: /mnt/nvme1n1p1/MI300-testing/vllm/build/lib.linux-x86_64-cpython-311/vllm/_core_C.abi3.so\n",
      "-- Install configuration: \"RelWithDebInfo\"\n",
      "-- Installing: /mnt/nvme1n1p1/MI300-testing/vllm/build/lib.linux-x86_64-cpython-311/vllm/_moe_C.abi3.so\n",
      "-- Set non-toolchain portion of runtime path of \"/mnt/nvme1n1p1/MI300-testing/vllm/build/lib.linux-x86_64-cpython-311/vllm/_moe_C.abi3.so\" to \"\"\n",
      "-- Install configuration: \"RelWithDebInfo\"\n",
      "-- Installing: /mnt/nvme1n1p1/MI300-testing/vllm/build/lib.linux-x86_64-cpython-311/vllm/_rocm_C.abi3.so\n",
      "-- Set non-toolchain portion of runtime path of \"/mnt/nvme1n1p1/MI300-testing/vllm/build/lib.linux-x86_64-cpython-311/vllm/_rocm_C.abi3.so\" to \"\"\n",
      "-- Install configuration: \"RelWithDebInfo\"\n",
      "-- Installing: /mnt/nvme1n1p1/MI300-testing/vllm/build/lib.linux-x86_64-cpython-311/vllm/_C.abi3.so\n",
      "-- Set non-toolchain portion of runtime path of \"/mnt/nvme1n1p1/MI300-testing/vllm/build/lib.linux-x86_64-cpython-311/vllm/_C.abi3.so\" to \"\"\n",
      "copying build/lib.linux-x86_64-cpython-311/vllm/_moe_C.abi3.so -> vllm\n",
      "copying build/lib.linux-x86_64-cpython-311/vllm/_rocm_C.abi3.so -> vllm\n",
      "copying build/lib.linux-x86_64-cpython-311/vllm/_C.abi3.so -> vllm\n",
      "Creating /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm.egg-link (link to .)\n",
      "Removing vllm 0.6.4.dev8+ge9d517f2.rocm624 from easy-install.pth file\n",
      "vllm 0.6.4.dev11+gba309422.rocm624 is already the active version in easy-install.pth\n",
      "detected new path '/home/hotaisle/vllm'\n",
      "Installing vllm script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Installed /mnt/nvme1n1p1/MI300-testing/vllm\n",
      "Processing dependencies for vllm==0.6.4.dev11+gba309422.rocm624\n",
      "Searching for fastapi==0.115.0\n",
      "Best match: fastapi 0.115.0\n",
      "Adding fastapi 0.115.0 to easy-install.pth file\n",
      "Installing fastapi script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for tensorizer==2.9.0\n",
      "Best match: tensorizer 2.9.0\n",
      "Adding tensorizer 2.9.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pytest-asyncio==0.24.0\n",
      "Best match: pytest-asyncio 0.24.0\n",
      "Adding pytest-asyncio 0.24.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for peft==0.13.1\n",
      "Best match: peft 0.13.1\n",
      "Adding peft 0.13.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for ray==2.37.0\n",
      "Best match: ray 2.37.0\n",
      "Adding ray 2.37.0 to easy-install.pth file\n",
      "Installing ray script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing rllib script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing serve script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing tune script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for botocore==1.35.37\n",
      "Best match: botocore 1.35.37\n",
      "Adding botocore 1.35.37 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for boto3==1.35.37\n",
      "Best match: boto3 1.35.37\n",
      "Adding boto3 1.35.37 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for awscli==1.35.3\n",
      "Best match: awscli 1.35.3\n",
      "Adding awscli 1.35.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for compressed-tensors==0.6.0\n",
      "Best match: compressed-tensors 0.6.0\n",
      "Adding compressed-tensors 0.6.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for einops==0.8.0\n",
      "Best match: einops 0.8.0\n",
      "Adding einops 0.8.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for PyYAML==6.0.2\n",
      "Best match: PyYAML 6.0.2\n",
      "Adding PyYAML 6.0.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for mistral-common==1.4.4\n",
      "Best match: mistral-common 1.4.4\n",
      "Adding mistral-common 1.4.4 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for importlib-metadata==8.5.0\n",
      "Best match: importlib-metadata 8.5.0\n",
      "Adding importlib-metadata 8.5.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for gguf==0.10.0\n",
      "Best match: gguf 0.10.0\n",
      "Adding gguf 0.10.0 to easy-install.pth file\n",
      "Installing gguf-convert-endian script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing gguf-dump script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing gguf-new-metadata script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing gguf-set-metadata script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for msgspec==0.18.6\n",
      "Best match: msgspec 0.18.6\n",
      "Adding msgspec 0.18.6 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pyzmq==26.2.0\n",
      "Best match: pyzmq 26.2.0\n",
      "Adding pyzmq 26.2.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for partial-json-parser==0.2.1.1.post4\n",
      "Best match: partial-json-parser 0.2.1.1.post4\n",
      "Adding partial-json-parser 0.2.1.1.post4 to easy-install.pth file\n",
      "Installing json-playground script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for filelock==3.13.1\n",
      "Best match: filelock 3.13.1\n",
      "Adding filelock 3.13.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for typing-extensions==4.12.2\n",
      "Best match: typing-extensions 4.12.2\n",
      "Adding typing-extensions 4.12.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/_vendor\n",
      "Searching for outlines==0.0.46\n",
      "Best match: outlines 0.0.46\n",
      "Adding outlines 0.0.46 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for lm-format-enforcer==0.10.6\n",
      "Best match: lm-format-enforcer 0.10.6\n",
      "Adding lm-format-enforcer 0.10.6 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for tiktoken==0.7.0\n",
      "Best match: tiktoken 0.7.0\n",
      "Adding tiktoken 0.7.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for prometheus-fastapi-instrumentator==7.0.0\n",
      "Best match: prometheus-fastapi-instrumentator 7.0.0\n",
      "Adding prometheus-fastapi-instrumentator 7.0.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for prometheus-client==0.21.0\n",
      "Best match: prometheus-client 0.21.0\n",
      "Adding prometheus-client 0.21.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pillow==10.4.0\n",
      "Best match: pillow 10.4.0\n",
      "Adding pillow 10.4.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pydantic==2.9.2\n",
      "Best match: pydantic 2.9.2\n",
      "Adding pydantic 2.9.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for uvicorn==0.31.1\n",
      "Best match: uvicorn 0.31.1\n",
      "Adding uvicorn 0.31.1 to easy-install.pth file\n",
      "Installing uvicorn script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for openai==1.51.2\n",
      "Best match: openai 1.51.2\n",
      "Adding openai 1.51.2 to easy-install.pth file\n",
      "Installing openai script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for aiohttp==3.10.9\n",
      "Best match: aiohttp 3.10.9\n",
      "Adding aiohttp 3.10.9 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for protobuf==5.28.2\n",
      "Best match: protobuf 5.28.2\n",
      "Adding protobuf 5.28.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for tokenizers==0.20.0\n",
      "Best match: tokenizers 0.20.0\n",
      "Adding tokenizers 0.20.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for transformers==4.45.2\n",
      "Best match: transformers 4.45.2\n",
      "Adding transformers 4.45.2 to easy-install.pth file\n",
      "Installing transformers-cli script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for py-cpuinfo==9.0.0\n",
      "Best match: py-cpuinfo 9.0.0\n",
      "Adding py-cpuinfo 9.0.0 to easy-install.pth file\n",
      "Installing cpuinfo script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for tqdm==4.66.5\n",
      "Best match: tqdm 4.66.5\n",
      "Adding tqdm 4.66.5 to easy-install.pth file\n",
      "Installing tqdm script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for requests==2.32.3\n",
      "Best match: requests 2.32.3\n",
      "Adding requests 2.32.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for numpy==1.26.4\n",
      "Best match: numpy 1.26.4\n",
      "Adding numpy 1.26.4 to easy-install.pth file\n",
      "Installing f2py script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for sentencepiece==0.2.0\n",
      "Best match: sentencepiece 0.2.0\n",
      "Adding sentencepiece 0.2.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for psutil==6.0.0\n",
      "Best match: psutil 6.0.0\n",
      "Adding psutil 6.0.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for starlette==0.38.6\n",
      "Best match: starlette 0.38.6\n",
      "Adding starlette 0.38.6 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for libnacl==2.1.0\n",
      "Best match: libnacl 2.1.0\n",
      "Adding libnacl 2.1.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for hiredis==3.0.0\n",
      "Best match: hiredis 3.0.0\n",
      "Adding hiredis 3.0.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for redis==5.1.1\n",
      "Best match: redis 5.1.1\n",
      "Adding redis 5.1.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for torch==2.6.0.dev20241015+rocm6.2\n",
      "Best match: torch 2.6.0.dev20241015+rocm6.2\n",
      "Adding torch 2.6.0.dev20241015+rocm6.2 to easy-install.pth file\n",
      "Installing convert-caffe2-to-onnx script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing convert-onnx-to-caffe2 script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing torchfrtrace script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing torchrun script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pytest==8.3.3\n",
      "Best match: pytest 8.3.3\n",
      "Adding pytest 8.3.3 to easy-install.pth file\n",
      "Installing py.test script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing pytest script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for huggingface-hub==0.25.2\n",
      "Best match: huggingface-hub 0.25.2\n",
      "Adding huggingface-hub 0.25.2 to easy-install.pth file\n",
      "Installing huggingface-cli script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for safetensors==0.4.5\n",
      "Best match: safetensors 0.4.5\n",
      "Adding safetensors 0.4.5 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for accelerate==1.0.0\n",
      "Best match: accelerate 1.0.0\n",
      "Adding accelerate 1.0.0 to easy-install.pth file\n",
      "Installing accelerate script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing accelerate-config script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing accelerate-estimate-memory script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing accelerate-launch script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing accelerate-merge-weights script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for packaging==24.1\n",
      "Best match: packaging 24.1\n",
      "packaging 24.1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/_vendor\n",
      "Searching for frozenlist==1.4.1\n",
      "Best match: frozenlist 1.4.1\n",
      "Adding frozenlist 1.4.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for aiosignal==1.3.1\n",
      "Best match: aiosignal 1.3.1\n",
      "Adding aiosignal 1.3.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for msgpack==1.1.0\n",
      "Best match: msgpack 1.1.0\n",
      "Adding msgpack 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for jsonschema==4.23.0\n",
      "Best match: jsonschema 4.23.0\n",
      "Adding jsonschema 4.23.0 to easy-install.pth file\n",
      "Installing jsonschema script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for click==8.1.7\n",
      "Best match: click 8.1.7\n",
      "Adding click 8.1.7 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for urllib3==2.2.3\n",
      "Best match: urllib3 2.2.3\n",
      "Adding urllib3 2.2.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for python-dateutil==2.9.0.post0\n",
      "Best match: python-dateutil 2.9.0.post0\n",
      "Adding python-dateutil 2.9.0.post0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for jmespath==1.0.1\n",
      "Best match: jmespath 1.0.1\n",
      "Adding jmespath 1.0.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for s3transfer==0.10.3\n",
      "Best match: s3transfer 0.10.3\n",
      "Adding s3transfer 0.10.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for rsa==4.7.2\n",
      "Best match: rsa 4.7.2\n",
      "Adding rsa 4.7.2 to easy-install.pth file\n",
      "Installing pyrsa-decrypt script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing pyrsa-encrypt script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing pyrsa-keygen script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing pyrsa-priv2pub script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing pyrsa-sign script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing pyrsa-verify script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for colorama==0.4.6\n",
      "Best match: colorama 0.4.6\n",
      "Adding colorama 0.4.6 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for docutils==0.16\n",
      "Best match: docutils 0.16\n",
      "Adding docutils 0.16 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for opencv-python-headless==4.10.0.84\n",
      "Best match: opencv-python-headless 4.10.0.84\n",
      "Adding opencv-python-headless 4.10.0.84 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for zipp==3.20.2\n",
      "Best match: zipp 3.20.2\n",
      "Adding zipp 3.20.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pyairports==2.1.1\n",
      "Best match: pyairports 2.1.1\n",
      "Adding pyairports 2.1.1 to easy-install.pth file\n",
      "Installing pyairports script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pycountry==24.6.1\n",
      "Best match: pycountry 24.6.1\n",
      "Adding pycountry 24.6.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for datasets==3.0.1\n",
      "Best match: datasets 3.0.1\n",
      "Adding datasets 3.0.1 to easy-install.pth file\n",
      "Installing datasets-cli script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for referencing==0.35.1\n",
      "Best match: referencing 0.35.1\n",
      "Adding referencing 0.35.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for numba==0.60.0\n",
      "Best match: numba 0.60.0\n",
      "Adding numba 0.60.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for diskcache==5.6.3\n",
      "Best match: diskcache 5.6.3\n",
      "Adding diskcache 5.6.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for cloudpickle==3.0.0\n",
      "Best match: cloudpickle 3.0.0\n",
      "Adding cloudpickle 3.0.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for nest-asyncio==1.6.0\n",
      "Best match: nest-asyncio 1.6.0\n",
      "Adding nest-asyncio 1.6.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for lark==1.2.2\n",
      "Best match: lark 1.2.2\n",
      "Adding lark 1.2.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for jinja2==3.1.4\n",
      "Best match: jinja2 3.1.4\n",
      "Adding jinja2 3.1.4 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for interegular==0.3.3\n",
      "Best match: interegular 0.3.3\n",
      "Adding interegular 0.3.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for regex==2024.9.11\n",
      "Best match: regex 2024.9.11\n",
      "Adding regex 2024.9.11 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pydantic-core==2.23.4\n",
      "Best match: pydantic-core 2.23.4\n",
      "Adding pydantic-core 2.23.4 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for annotated-types==0.7.0\n",
      "Best match: annotated-types 0.7.0\n",
      "Adding annotated-types 0.7.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for websockets==13.1\n",
      "Best match: websockets 13.1\n",
      "Adding websockets 13.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for watchfiles==0.24.0\n",
      "Best match: watchfiles 0.24.0\n",
      "Adding watchfiles 0.24.0 to easy-install.pth file\n",
      "Installing watchfiles script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for uvloop==0.20.0\n",
      "Best match: uvloop 0.20.0\n",
      "Adding uvloop 0.20.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for python-dotenv==1.0.1\n",
      "Best match: python-dotenv 1.0.1\n",
      "Adding python-dotenv 1.0.1 to easy-install.pth file\n",
      "Installing dotenv script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for httptools==0.6.1\n",
      "Best match: httptools 0.6.1\n",
      "Adding httptools 0.6.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for h11==0.14.0\n",
      "Best match: h11 0.14.0\n",
      "Adding h11 0.14.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for sniffio==1.3.1\n",
      "Best match: sniffio 1.3.1\n",
      "Adding sniffio 1.3.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for jiter==0.6.1\n",
      "Best match: jiter 0.6.1\n",
      "Adding jiter 0.6.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for httpx==0.27.2\n",
      "Best match: httpx 0.27.2\n",
      "Adding httpx 0.27.2 to easy-install.pth file\n",
      "Installing httpx script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for distro==1.9.0\n",
      "Best match: distro 1.9.0\n",
      "Adding distro 1.9.0 to easy-install.pth file\n",
      "Installing distro script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for anyio==4.6.0\n",
      "Best match: anyio 4.6.0\n",
      "Adding anyio 4.6.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for yarl==1.14.0\n",
      "Best match: yarl 1.14.0\n",
      "Adding yarl 1.14.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for multidict==6.1.0\n",
      "Best match: multidict 6.1.0\n",
      "Adding multidict 6.1.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for attrs==24.2.0\n",
      "Best match: attrs 24.2.0\n",
      "Adding attrs 24.2.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for aiohappyeyeballs==2.4.3\n",
      "Best match: aiohappyeyeballs 2.4.3\n",
      "Adding aiohappyeyeballs 2.4.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for certifi==2024.8.30\n",
      "Best match: certifi 2024.8.30\n",
      "Adding certifi 2024.8.30 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for idna==3.10\n",
      "Best match: idna 3.10\n",
      "Adding idna 3.10 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for charset-normalizer==3.4.0\n",
      "Best match: charset-normalizer 3.4.0\n",
      "Adding charset-normalizer 3.4.0 to easy-install.pth file\n",
      "Installing normalizer script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for sympy==1.13.1\n",
      "Best match: sympy 1.13.1\n",
      "Adding sympy 1.13.1 to easy-install.pth file\n",
      "Installing isympy script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "Best match: pytorch-triton-rocm 3.1.0+cf34004b8a\n",
      "Adding pytorch-triton-rocm 3.1.0+cf34004b8a to easy-install.pth file\n",
      "Installing proton script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing proton-viewer script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for fsspec==2024.6.1\n",
      "Best match: fsspec 2024.6.1\n",
      "Adding fsspec 2024.6.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for networkx==3.3\n",
      "Best match: networkx 3.3\n",
      "Adding networkx 3.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pluggy==1.5.0\n",
      "Best match: pluggy 1.5.0\n",
      "Adding pluggy 1.5.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for iniconfig==2.0.0\n",
      "Best match: iniconfig 2.0.0\n",
      "Adding iniconfig 2.0.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for rpds-py==0.20.0\n",
      "Best match: rpds-py 0.20.0\n",
      "Adding rpds-py 0.20.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for jsonschema-specifications==2024.10.1\n",
      "Best match: jsonschema-specifications 2024.10.1\n",
      "Adding jsonschema-specifications 2024.10.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for six==1.16.0\n",
      "Best match: six 1.16.0\n",
      "Adding six 1.16.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pyasn1==0.6.1\n",
      "Best match: pyasn1 0.6.1\n",
      "Adding pyasn1 0.6.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for multiprocess==0.70.16\n",
      "Best match: multiprocess 0.70.16\n",
      "Adding multiprocess 0.70.16 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for xxhash==3.5.0\n",
      "Best match: xxhash 3.5.0\n",
      "Adding xxhash 3.5.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pandas==2.2.3\n",
      "Best match: pandas 2.2.3\n",
      "Adding pandas 2.2.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for dill==0.3.8\n",
      "Best match: dill 0.3.8\n",
      "Adding dill 0.3.8 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pyarrow==17.0.0\n",
      "Best match: pyarrow 17.0.0\n",
      "Adding pyarrow 17.0.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for llvmlite==0.43.0\n",
      "Best match: llvmlite 0.43.0\n",
      "Adding llvmlite 0.43.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for MarkupSafe==2.1.5\n",
      "Best match: MarkupSafe 2.1.5\n",
      "Adding MarkupSafe 2.1.5 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for httpcore==1.0.6\n",
      "Best match: httpcore 1.0.6\n",
      "Adding httpcore 1.0.6 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for propcache==0.2.0\n",
      "Best match: propcache 0.2.0\n",
      "Adding propcache 0.2.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for mpmath==1.3.0\n",
      "Best match: mpmath 1.3.0\n",
      "Adding mpmath 1.3.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for tzdata==2024.2\n",
      "Best match: tzdata 2024.2\n",
      "Adding tzdata 2024.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pytz==2024.2\n",
      "Best match: pytz 2024.2\n",
      "Adding pytz 2024.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Finished processing dependencies for vllm==0.6.4.dev11+gba309422.rocm624\n"
     ]
    }
   ],
   "source": [
    "# Build vLLM\n",
    "!PYTORCH_ROCM_ARCH=\"gfx942\" python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b7d71db-e101-4a11-9f4f-6e33be834de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.4.dev9+g5d264f4a\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import vllm\n",
    "print(vllm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "991d7e85-47a6-4272-a5e8-6c39ac7a734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'flash-attention' already exists and is not an empty directory.\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention\n",
      "Already up to date.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ninja in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (1.11.1.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\n",
      "torch.__version__  = 2.6.0.dev20241015+rocm6.2\n",
      "\n",
      "\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/flash_common.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/flash_common_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/flash_api.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/flash_api.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/flash_common.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/flash_common.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/config.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/config.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/integer.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/integer.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/integral_constant.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/integral_constant.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/type_traits.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/type_traits.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/bit_cast.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/bit_cast.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/math.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/math.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/to_sequence.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/to_sequence_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/sequence.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/sequence_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/functional.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/functional_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/array.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/array_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/tuple.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/tuple_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/multi_index.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/multi_index_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/map.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/map_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/container_helper.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/container_helper_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/magic_div.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/magic_div_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/algorithm/coordinate_transform_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/numeric.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/numeric.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_adaptor.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_adaptor_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/algorithm/cluster_descriptor.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/algorithm/cluster_descriptor_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/statically_indexed_array.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/statically_indexed_array_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/algorithm/space_filling_curve.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/algorithm/space_filling_curve_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/half.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/half.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/random.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/random.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/float8.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/float8.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/bfloat16.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/bfloat16.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/vector_type.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/vector_type_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/thread_buffer.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/thread_buffer_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/amd_buffer_addressing.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/amd_buffer_addressing_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/arch.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/arch_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/type_convert.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/type_convert.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/generic_memory_space_atomic.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/generic_memory_space_atomic_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/utility.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/arch/utility.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/meta_data_buffer.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/meta_data_buffer_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/span.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/container/span.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/null_type.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/numeric/null_type.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/buffer_view.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/buffer_view_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_adaptor_coordinate.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_adaptor_coordinate_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_distribution_encoding.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_distribution_encoding_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_distribution.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_distribution_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/static_distributed_tensor.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/static_distributed_tensor_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_window.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_window_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_descriptor.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_descriptor_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_view.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_view_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/null_tile_window.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/null_tile_window_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/null_tensor.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/null_tensor.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/load_tile.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/load_tile_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_elementwise.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tile_elementwise_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/transpose_vectors.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/transpose_vectors_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/shuffle_tile.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/shuffle_tile_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/slice_tile.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/slice_tile_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/store_tile.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/store_tile_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/sweep_tile.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/sweep_tile_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_coordinate.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/tensor_coordinate_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/update_tile.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/tensor/update_tile_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/ignore.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/ignore.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/philox_rand.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/philox_rand.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/unary_element_function.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core/utility/unary_element_function_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/core_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/stream_config.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/stream_config.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/hip_check_error.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/hip_check_error.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/timer.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/timer.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/kernel_launch.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/kernel_launch_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_attention_bias_enum.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_attention_bias_enum.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_impl.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_impl_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_impl.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_impl_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_attribute_mfma_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_dropout.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_dropout_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_masking.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_masking_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_position_encoding.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_position_encoding_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_rotary_embedding.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/block_rotary_embedding.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/page_block_navigator.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/block/page_block_navigator_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/common/tensor_layout.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/common/tensor_layout.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/common.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/common.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_bwd_kernel_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_kernel.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_kernel_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_tile_partitioner.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_appendkv_tile_partitioner_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_kernel.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_kernel_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_kernel.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_kernel_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_tile_partitioner.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_combine_tile_partitioner_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_kernel.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_kernel_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_tile_partitioner.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_splitkv_tile_partitioner_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_tile_partitioner.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/kernel/fmha_fwd_tile_partitioner_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/pipeline/block_gemm_pipeline_problem.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/pipeline/block_gemm_pipeline_problem_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/pipeline/tile_gemm_shape.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/pipeline/tile_gemm_shape_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_dispatcher.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/warp/warp_gemm_dispatcher_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_custom_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_custom_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v1_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_custom_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_custom_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_breg_creg_v1_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_custom_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_custom_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_breg_creg_v1_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_custom_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_custom_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_asmem_bsmem_creg_v1_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_convert_dq_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dot_do_o.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dot_do_o_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/reduce/block/block_reduce.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/reduce/block/block_reduce_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_dq_dk_dv_pipeline_kr_ktr_vr_iglp_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_enum.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_enum.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_problem.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_bwd_pipeline_problem_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_appendkv_pipeline_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_custom_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_custom_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/gemm/block/block_gemm_areg_bsmem_creg_v2_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qx_ks_vs_custom_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qx_ks_vs_custom_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_combine_pipeline_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_fwd_splitkv_pipeline_qr_ks_vs_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_enum.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_enum.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_problem.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_problem_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_async_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_fp8.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qr_ks_vs_fp8_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_default_policy.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_default_policy_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/block_fmha_pipeline_qs_ks_vs_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/tile_fmha_shape.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/tile_fmha_shape_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/tile_fmha_traits.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha/pipeline/tile_fmha_traits_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/fmha_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/epilogue/default_2d_epilogue.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/epilogue/default_2d_epilogue_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/epilogue.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/ops/epilogue_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/mask.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/mask_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/bias.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/bias_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/fmha_bwd.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/fmha_bwd_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_bwd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_bwd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/ranges.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/ranges.hpp [skipped, no changes]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/host_tensor.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/include/ck_tile/host/host_tensor_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/rotary.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/rotary_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/fmha_fwd.hpp -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/composable_kernel/example/ck_tile/01_fmha/fmha_fwd_hip.hpp [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_fwd_kvcache.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_fwd_kvcache.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_fwd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_fwd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_varlen_bwd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_varlen_bwd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_varlen_fwd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/csrc/flash_attn_ck/mha_varlen_fwd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp8_batch_b64x256_unused_squant.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp8_batch_b64x256_unused_squant.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp8_batch_b64x64_unused_squant.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp8_batch_b64x64_unused_squant.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp8_batch_b64x128_unused_squant.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp8_batch_b64x128_unused_squant.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_api.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_api.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_api.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_api.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_api.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_api.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_pdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_pdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_group_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_fp16_group_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_half.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_half.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psk_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psk_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d256_bf16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_pdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_pdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d128_fp16_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_ps_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_ps_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_half_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_half_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_inter_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_inter_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_psdv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_psdv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_psdv_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_psdv_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_inter.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_inter.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_api.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_api.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_pagedkv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_pagedkv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.hip [skipped, already hipified]\n",
      "/mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.cu -> /mnt/nvme1n1p1/MI300-testing/flash-attention/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip [skipped, already hipified]\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n",
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 1\n",
      "/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Requirements should be satisfied by a PEP 517 installer.\n",
      "        If you are using pip, you can try `pip install --use-pep517`.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  dist.fetch_build_eggs(dist.setup_requires)\n",
      "running install\n",
      "/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing flash_attn.egg-info/PKG-INFO\n",
      "writing dependency_links to flash_attn.egg-info/dependency_links.txt\n",
      "writing requirements to flash_attn.egg-info/requires.txt\n",
      "writing top-level names to flash_attn.egg-info/top_level.txt\n",
      "reading manifest template 'MANIFEST.in'\n",
      "warning: no files found matching '*.cu' under directory 'flash_attn'\n",
      "warning: no files found matching '*.h' under directory 'flash_attn'\n",
      "warning: no files found matching '*.cuh' under directory 'flash_attn'\n",
      "warning: no files found matching '*.cpp' under directory 'flash_attn'\n",
      "warning: no files found matching '*.hpp' under directory 'flash_attn'\n",
      "adding license file 'LICENSE'\n",
      "adding license file 'AUTHORS'\n",
      "writing manifest file 'flash_attn.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-311/flash_attn\n",
      "copying hopper/benchmark_flash_attention_fp8.py -> build/lib.linux-x86_64-cpython-311/hopper\n",
      "copying hopper/__init__.py -> build/lib.linux-x86_64-cpython-311/hopper\n",
      "copying hopper/benchmark_attn.py -> build/lib.linux-x86_64-cpython-311/hopper\n",
      "copying hopper/test_flash_attn.py -> build/lib.linux-x86_64-cpython-311/hopper\n",
      "copying hopper/setup.py -> build/lib.linux-x86_64-cpython-311/hopper\n",
      "copying hopper/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-311/hopper\n",
      "copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "copying flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-311/flash_attn/modules\n",
      "copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/losses\n",
      "copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-311/flash_attn/losses\n",
      "copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/layers\n",
      "copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-311/flash_attn/layers\n",
      "copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-311/flash_attn/layers\n",
      "copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/btlm.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-cpython-311/flash_attn/models\n",
      "copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-311/flash_attn/utils\n",
      "copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops\n",
      "copying flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton\n",
      "copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton\n",
      "copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton\n",
      "copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton\n",
      "copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton\n",
      "copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton\n",
      "copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton\n",
      "running build_ext\n",
      "building 'flash_attn_2_cuda' extension\n",
      "Emitting ninja build file /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
      "Compiling objects...\n",
      "Using envvar MAX_JOBS (104) as the number of workers...\n",
      "ninja: no work to do.\n",
      "g++ -pthread -B /home/hotaisle/miniforge3/envs/vllm/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/vllm/include -fPIC -O2 -isystem /home/hotaisle/miniforge3/envs/vllm/include -pthread -B /home/hotaisle/miniforge3/envs/vllm/compiler_compat -shared /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_api.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_bf16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_bf16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_bf16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_fp16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_fp16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d128_fp16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_bf16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_bf16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_bf16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_fp16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_fp16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d256_fp16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_bf16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_bf16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_bf16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_fp16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_fp16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d32_fp16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_bf16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_bf16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_bf16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_fp16_batch_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_fp16_group_o2_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_bwd_dot_do_o_d64_fp16_group_o2_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_api.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_api.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_bf16_b64x64x128x128_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d128_fp16_b64x64x128x128_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_bf16_b64x64x256x256_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d256_fp16_b64x64x256x256_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_bf16_b64x64x32x32_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d32_fp16_b64x64x32x32_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_bf16_b64x64x64x64_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psk.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psk_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_pskd_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_half.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_half_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_inter.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_inter_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_appendkv_d64_fp16_b64x64x64x64_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_api.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_batch_b64x128_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_bf16_group_b64x128_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_batch_b64x128_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp16_group_b64x128_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d128_fp8_batch_b64x128_unused_squant.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_batch_b64x256_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_bf16_group_b64x256_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_batch_b64x256_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp16_group_b64x256_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d256_fp8_batch_b64x256_unused_squant.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_batch_b64x32_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_bf16_group_b64x32_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_batch_b64x32_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d32_fp16_group_b64x32_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_batch_b64x64_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_bf16_group_b64x64_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_pdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_pdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_batch_b64x64_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_ps.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_ps_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_psdv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp16_group_b64x64_unused_psdv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_combine_d64_fp8_batch_b64x64_unused_squant.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_bf16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_batch_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d128_fp16_group_b128x128x32x128x32x128_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_bf16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_batch_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d256_fp16_group_b128x128x32x256x32x256_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_bf16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_batch_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d32_fp16_group_b128x64x16x32x32x32_r2x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_bf16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_batch_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_pagedkv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/build/fmha_fwd_splitkv_d64_fp16_group_b128x64x32x64x32x64_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/csrc/flash_attn_ck/flash_api.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/csrc/flash_attn_ck/flash_common.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/csrc/flash_attn_ck/mha_bwd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/csrc/flash_attn_ck/mha_fwd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/csrc/flash_attn_ck/mha_fwd_kvcache.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/csrc/flash_attn_ck/mha_varlen_bwd.o /mnt/nvme1n1p1/MI300-testing/flash-attention/build/temp.linux-x86_64-cpython-311/csrc/flash_attn_ck/mha_varlen_fwd.o -L/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/torch/lib -L/opt/rocm-6.2.2/lib -L/opt/rocm-6.2.2/hip/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -lamdhip64 -lc10_hip -ltorch_hip -o build/lib.linux-x86_64-cpython-311/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/flash_attn_triton.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/bert_padding.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn/modules\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/modules/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/modules\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/modules/mlp.py -> build/bdist.linux-x86_64/egg/flash_attn/modules\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/modules/embedding.py -> build/bdist.linux-x86_64/egg/flash_attn/modules\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/modules/mha.py -> build/bdist.linux-x86_64/egg/flash_attn/modules\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/modules/block.py -> build/bdist.linux-x86_64/egg/flash_attn/modules\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/fused_softmax.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/flash_blocksparse_attention.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn/losses\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/losses/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/losses\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/losses/cross_entropy.py -> build/bdist.linux-x86_64/egg/flash_attn/losses\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn/layers\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/layers/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/layers\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/layers/rotary.py -> build/bdist.linux-x86_64/egg/flash_attn/layers\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/layers/patch_embed.py -> build/bdist.linux-x86_64/egg/flash_attn/layers\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/flash_blocksparse_attn_interface.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/bigcode.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/opt.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/falcon.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/gptj.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/gpt.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/vit.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/bert.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/btlm.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/gpt_neox.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/llama.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/models/baichuan.py -> build/bdist.linux-x86_64/egg/flash_attn/models\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn/utils\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/utils/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/utils\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/utils/benchmark.py -> build/bdist.linux-x86_64/egg/flash_attn/utils\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/utils/generation.py -> build/bdist.linux-x86_64/egg/flash_attn/utils\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/utils/pretrained.py -> build/bdist.linux-x86_64/egg/flash_attn/utils\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/utils/distributed.py -> build/bdist.linux-x86_64/egg/flash_attn/utils\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn/ops\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/layer_norm.py -> build/bdist.linux-x86_64/egg/flash_attn/ops\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/ops\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/fused_dense.py -> build/bdist.linux-x86_64/egg/flash_attn/ops\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/activations.py -> build/bdist.linux-x86_64/egg/flash_attn/ops\n",
      "creating build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton/layer_norm.py -> build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton/__init__.py -> build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton/k_activations.py -> build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton/rotary.py -> build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton/mlp.py -> build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton/cross_entropy.py -> build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/triton/linear.py -> build/bdist.linux-x86_64/egg/flash_attn/ops/triton\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/ops/rms_norm.py -> build/bdist.linux-x86_64/egg/flash_attn/ops\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/flash_attn_triton_og.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn/flash_attn_interface.py -> build/bdist.linux-x86_64/egg/flash_attn\n",
      "copying build/lib.linux-x86_64-cpython-311/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/hopper\n",
      "copying build/lib.linux-x86_64-cpython-311/hopper/benchmark_flash_attention_fp8.py -> build/bdist.linux-x86_64/egg/hopper\n",
      "copying build/lib.linux-x86_64-cpython-311/hopper/__init__.py -> build/bdist.linux-x86_64/egg/hopper\n",
      "copying build/lib.linux-x86_64-cpython-311/hopper/benchmark_attn.py -> build/bdist.linux-x86_64/egg/hopper\n",
      "copying build/lib.linux-x86_64-cpython-311/hopper/test_flash_attn.py -> build/bdist.linux-x86_64/egg/hopper\n",
      "copying build/lib.linux-x86_64-cpython-311/hopper/setup.py -> build/bdist.linux-x86_64/egg/hopper\n",
      "copying build/lib.linux-x86_64-cpython-311/hopper/flash_attn_interface.py -> build/bdist.linux-x86_64/egg/hopper\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_attn_triton.py to flash_attn_triton.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/bert_padding.py to bert_padding.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/mlp.py to mlp.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/embedding.py to embedding.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/mha.py to mha.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/modules/block.py to block.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/fused_softmax.py to fused_softmax.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_blocksparse_attention.py to flash_blocksparse_attention.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/losses/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/losses/cross_entropy.py to cross_entropy.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/layers/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/layers/rotary.py to rotary.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/layers/patch_embed.py to patch_embed.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_blocksparse_attn_interface.py to flash_blocksparse_attn_interface.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/bigcode.py to bigcode.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/opt.py to opt.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/falcon.py to falcon.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/gptj.py to gptj.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/gpt.py to gpt.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/vit.py to vit.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/bert.py to bert.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/btlm.py to btlm.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/gpt_neox.py to gpt_neox.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/llama.py to llama.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/models/baichuan.py to baichuan.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/benchmark.py to benchmark.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/generation.py to generation.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/pretrained.py to pretrained.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/utils/distributed.py to distributed.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/layer_norm.py to layer_norm.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/fused_dense.py to fused_dense.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/activations.py to activations.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/triton/layer_norm.py to layer_norm.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/triton/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/triton/k_activations.py to k_activations.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/triton/rotary.py to rotary.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/triton/mlp.py to mlp.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/triton/cross_entropy.py to cross_entropy.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/triton/linear.py to linear.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/ops/rms_norm.py to rms_norm.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_attn_triton_og.py to flash_attn_triton_og.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn/flash_attn_interface.py to flash_attn_interface.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hopper/benchmark_flash_attention_fp8.py to benchmark_flash_attention_fp8.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hopper/__init__.py to __init__.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hopper/benchmark_attn.py to benchmark_attn.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hopper/test_flash_attn.py to test_flash_attn.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hopper/setup.py to setup.cpython-311.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hopper/flash_attn_interface.py to flash_attn_interface.cpython-311.pyc\n",
      "creating stub loader for flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/flash_attn_2_cuda.py to flash_attn_2_cuda.cpython-311.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying flash_attn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying flash_attn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying flash_attn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying flash_attn.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying flash_attn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "__pycache__.flash_attn_2_cuda.cpython-311: module references __file__\n",
      "hopper.__pycache__.setup.cpython-311: module references __file__\n",
      "creating 'dist/flash_attn-2.6.3-py3.11-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing flash_attn-2.6.3-py3.11-linux-x86_64.egg\n",
      "removing '/mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg' (and everything under it)\n",
      "creating /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg\n",
      "Extracting flash_attn-2.6.3-py3.11-linux-x86_64.egg to /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Adding flash-attn 2.6.3 to easy-install.pth file\n",
      "\n",
      "Installed /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg\n",
      "Processing dependencies for flash-attn==2.6.3\n",
      "Searching for einops==0.8.0\n",
      "Best match: einops 0.8.0\n",
      "Adding einops 0.8.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for torch==2.6.0.dev20241015+rocm6.2\n",
      "Best match: torch 2.6.0.dev20241015+rocm6.2\n",
      "Adding torch 2.6.0.dev20241015+rocm6.2 to easy-install.pth file\n",
      "Installing convert-caffe2-to-onnx script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing convert-onnx-to-caffe2 script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing torchfrtrace script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing torchrun script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for sympy==1.13.1\n",
      "Best match: sympy 1.13.1\n",
      "Adding sympy 1.13.1 to easy-install.pth file\n",
      "Installing isympy script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "Best match: pytorch-triton-rocm 3.1.0+cf34004b8a\n",
      "Adding pytorch-triton-rocm 3.1.0+cf34004b8a to easy-install.pth file\n",
      "Installing proton script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "Installing proton-viewer script to /home/hotaisle/miniforge3/envs/vllm/bin\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for fsspec==2024.6.1\n",
      "Best match: fsspec 2024.6.1\n",
      "Adding fsspec 2024.6.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for jinja2==3.1.4\n",
      "Best match: jinja2 3.1.4\n",
      "Adding jinja2 3.1.4 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for networkx==3.3\n",
      "Best match: networkx 3.3\n",
      "Adding networkx 3.3 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for typing-extensions==4.12.2\n",
      "Best match: typing-extensions 4.12.2\n",
      "Adding typing-extensions 4.12.2 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/setuptools/_vendor\n",
      "Searching for filelock==3.13.1\n",
      "Best match: filelock 3.13.1\n",
      "Adding filelock 3.13.1 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for mpmath==1.3.0\n",
      "Best match: mpmath 1.3.0\n",
      "Adding mpmath 1.3.0 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Searching for MarkupSafe==2.1.5\n",
      "Best match: MarkupSafe 2.1.5\n",
      "Adding MarkupSafe 2.1.5 to easy-install.pth file\n",
      "\n",
      "Using /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages\n",
      "Finished processing dependencies for flash-attn==2.6.3\n",
      "19.37user 8.73system 0:19.68elapsed 142%CPU (0avgtext+0avgdata 1732440maxresident)k\n",
      "0inputs+1210424outputs (0major+454837minor)pagefaults 0swaps\n",
      "/mnt/nvme1n1p1/MI300-testing\n"
     ]
    }
   ],
   "source": [
    "# Let's try CK FA2 https://embeddedllm.com/blog/how-to-build-vllm-on-mi300x-from-source\n",
    "%cd ..\n",
    "!git clone https://github.com/ROCm/flash-attention.git\n",
    "%cd flash-attention\n",
    "!git pull\n",
    "!git submodule update --init\n",
    "# speed up compile\n",
    "!pip install ninja\n",
    "!GPU_ARCHS=\"gfx942\" time python setup.py install\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdd9f15-0d5e-4a41-af82-5e86b89b4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "WARNING 10-15 23:07:07 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "PyTorch version: 2.6.0.dev20241015+rocm6.2\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: N/A\n",
      "ROCM used to build PyTorch: 6.2.41133-dd7f95766\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\n",
      "Nvidia driver version: Could not collect\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: 6.2.41133\n",
      "MIOpen runtime version: 3.2.0\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 57 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               208\n",
      "On-line CPU(s) list:                  0-207\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Platinum 8470\n",
      "CPU family:                           6\n",
      "Model:                                143\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   52\n",
      "Socket(s):                            2\n",
      "Stepping:                             8\n",
      "CPU max MHz:                          3800.0000\n",
      "CPU min MHz:                          800.0000\n",
      "BogoMIPS:                             4000.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            4.9 MiB (104 instances)\n",
      "L1i cache:                            3.3 MiB (104 instances)\n",
      "L2 cache:                             208 MiB (104 instances)\n",
      "L3 cache:                             210 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206\n",
      "NUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Not affected\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] lion-pytorch==0.2.2\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "[pip3] pyzmq==26.2.0\n",
      "[pip3] torch==2.6.0.dev20241015+rocm6.2\n",
      "[pip3] torchao==0.5.0\n",
      "[pip3] torchaudio==2.5.0.dev20241015+rocm6.2\n",
      "[pip3] torchtune==0.3.1\n",
      "[pip3] torchvision==0.20.0.dev20241015+rocm6.2\n",
      "[pip3] transformers==4.45.2\n",
      "[pip3] triton==3.1.0\n",
      "[conda] lion-pytorch              0.2.2                    pypi_0    pypi\n",
      "[conda] numpy                     1.26.4                   pypi_0    pypi\n",
      "[conda] pytorch-triton-rocm       3.1.0+cf34004b8a          pypi_0    pypi\n",
      "[conda] pyzmq                     26.2.0          py311h7deb3e3_3    conda-forge\n",
      "[conda] torch                     2.6.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchao                   0.5.0                    pypi_0    pypi\n",
      "[conda] torchaudio                2.5.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchtune                 0.3.1                    pypi_0    pypi\n",
      "[conda] torchvision               0.20.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] transformers              4.45.2                   pypi_0    pypi\n",
      "[conda] triton                    3.1.0                    pypi_0    pypi\n",
      "ROCM Version: 6.2.41134-65d174c3e\n",
      "Neuron SDK Version: N/A\n",
      "vLLM Version: 0.6.4.dev11+gba309422\n",
      "vLLM Build Flags:\n",
      "CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n",
      "GPU Topology:\n",
      "============================ ROCm System Management Interface ============================\n",
      "================================ Weight between two GPUs =================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            15           15           15           15           15           15           15           \n",
      "GPU1   15           0            15           15           15           15           15           15           \n",
      "GPU2   15           15           0            15           15           15           15           15           \n",
      "GPU3   15           15           15           0            15           15           15           15           \n",
      "GPU4   15           15           15           15           0            15           15           15           \n",
      "GPU5   15           15           15           15           15           0            15           15           \n",
      "GPU6   15           15           15           15           15           15           0            15           \n",
      "GPU7   15           15           15           15           15           15           15           0            \n",
      "\n",
      "================================= Hops between two GPUs ==================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            1            1            1            1            1            1            1            \n",
      "GPU1   1            0            1            1            1            1            1            1            \n",
      "GPU2   1            1            0            1            1            1            1            1            \n",
      "GPU3   1            1            1            0            1            1            1            1            \n",
      "GPU4   1            1            1            1            0            1            1            1            \n",
      "GPU5   1            1            1            1            1            0            1            1            \n",
      "GPU6   1            1            1            1            1            1            0            1            \n",
      "GPU7   1            1            1            1            1            1            1            0            \n",
      "\n",
      "=============================== Link Type between two GPUs ===============================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \n",
      "GPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \n",
      "GPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \n",
      "GPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \n",
      "GPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n",
      "\n",
      "======================================= Numa Nodes =======================================\n",
      "GPU[0]\t\t: (Topology) Numa Node: 0\n",
      "GPU[0]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[1]\t\t: (Topology) Numa Node: 0\n",
      "GPU[1]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[2]\t\t: (Topology) Numa Node: 0\n",
      "GPU[2]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[3]\t\t: (Topology) Numa Node: 0\n",
      "GPU[3]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[4]\t\t: (Topology) Numa Node: 1\n",
      "GPU[4]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[5]\t\t: (Topology) Numa Node: 1\n",
      "GPU[5]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[6]\t\t: (Topology) Numa Node: 1\n",
      "GPU[6]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[7]\t\t: (Topology) Numa Node: 1\n",
      "GPU[7]\t\t: (Topology) Numa Affinity: 1\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!python vllm/collect_env.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vllm]",
   "language": "python",
   "name": "conda-env-vllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
