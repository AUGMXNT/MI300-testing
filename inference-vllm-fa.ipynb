{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecde8ac0-d402-495f-acdd-f02d7e6b2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/137695#issuecomment-2412944849\n",
    "!ulimit -n 131072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a183eaf6-8590-492b-acfd-91f4ba6bc137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "WARNING 10-16 00:02:24 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "PyTorch version: 2.6.0.dev20241015+rocm6.2\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: N/A\n",
      "ROCM used to build PyTorch: 6.2.41133-dd7f95766\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\n",
      "Nvidia driver version: Could not collect\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: 6.2.41133\n",
      "MIOpen runtime version: 3.2.0\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 57 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               208\n",
      "On-line CPU(s) list:                  0-207\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) Platinum 8470\n",
      "CPU family:                           6\n",
      "Model:                                143\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   52\n",
      "Socket(s):                            2\n",
      "Stepping:                             8\n",
      "CPU max MHz:                          3800.0000\n",
      "CPU min MHz:                          800.0000\n",
      "BogoMIPS:                             4000.00\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n",
      "Virtualization:                       VT-x\n",
      "L1d cache:                            4.9 MiB (104 instances)\n",
      "L1i cache:                            3.3 MiB (104 instances)\n",
      "L2 cache:                             208 MiB (104 instances)\n",
      "L3 cache:                             210 MiB (2 instances)\n",
      "NUMA node(s):                         2\n",
      "NUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206\n",
      "NUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Not affected\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] lion-pytorch==0.2.2\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\n",
      "[pip3] pyzmq==26.2.0\n",
      "[pip3] torch==2.6.0.dev20241015+rocm6.2\n",
      "[pip3] torchao==0.5.0\n",
      "[pip3] torchaudio==2.5.0.dev20241015+rocm6.2\n",
      "[pip3] torchtune==0.3.1\n",
      "[pip3] torchvision==0.20.0.dev20241015+rocm6.2\n",
      "[pip3] transformers==4.45.2\n",
      "[pip3] triton==3.1.0\n",
      "[conda] lion-pytorch              0.2.2                    pypi_0    pypi\n",
      "[conda] numpy                     1.26.4                   pypi_0    pypi\n",
      "[conda] pytorch-triton-rocm       3.1.0+cf34004b8a          pypi_0    pypi\n",
      "[conda] pyzmq                     26.2.0          py311h7deb3e3_3    conda-forge\n",
      "[conda] torch                     2.6.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchao                   0.5.0                    pypi_0    pypi\n",
      "[conda] torchaudio                2.5.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] torchtune                 0.3.1                    pypi_0    pypi\n",
      "[conda] torchvision               0.20.0.dev20241015+rocm6.2          pypi_0    pypi\n",
      "[conda] transformers              4.45.2                   pypi_0    pypi\n",
      "[conda] triton                    3.1.0                    pypi_0    pypi\n",
      "ROCM Version: 6.2.41134-65d174c3e\n",
      "Neuron SDK Version: N/A\n",
      "vLLM Version: 0.6.4.dev11+gba309422\n",
      "vLLM Build Flags:\n",
      "CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n",
      "GPU Topology:\n",
      "============================ ROCm System Management Interface ============================\n",
      "================================ Weight between two GPUs =================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            15           15           15           15           15           15           15           \n",
      "GPU1   15           0            15           15           15           15           15           15           \n",
      "GPU2   15           15           0            15           15           15           15           15           \n",
      "GPU3   15           15           15           0            15           15           15           15           \n",
      "GPU4   15           15           15           15           0            15           15           15           \n",
      "GPU5   15           15           15           15           15           0            15           15           \n",
      "GPU6   15           15           15           15           15           15           0            15           \n",
      "GPU7   15           15           15           15           15           15           15           0            \n",
      "\n",
      "================================= Hops between two GPUs ==================================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            1            1            1            1            1            1            1            \n",
      "GPU1   1            0            1            1            1            1            1            1            \n",
      "GPU2   1            1            0            1            1            1            1            1            \n",
      "GPU3   1            1            1            0            1            1            1            1            \n",
      "GPU4   1            1            1            1            0            1            1            1            \n",
      "GPU5   1            1            1            1            1            0            1            1            \n",
      "GPU6   1            1            1            1            1            1            0            1            \n",
      "GPU7   1            1            1            1            1            1            1            0            \n",
      "\n",
      "=============================== Link Type between two GPUs ===============================\n",
      "       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \n",
      "GPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \n",
      "GPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \n",
      "GPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \n",
      "GPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \n",
      "GPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \n",
      "GPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n",
      "\n",
      "======================================= Numa Nodes =======================================\n",
      "GPU[0]\t\t: (Topology) Numa Node: 0\n",
      "GPU[0]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[1]\t\t: (Topology) Numa Node: 0\n",
      "GPU[1]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[2]\t\t: (Topology) Numa Node: 0\n",
      "GPU[2]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[3]\t\t: (Topology) Numa Node: 0\n",
      "GPU[3]\t\t: (Topology) Numa Affinity: 0\n",
      "GPU[4]\t\t: (Topology) Numa Node: 1\n",
      "GPU[4]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[5]\t\t: (Topology) Numa Node: 1\n",
      "GPU[5]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[6]\t\t: (Topology) Numa Node: 1\n",
      "GPU[6]\t\t: (Topology) Numa Affinity: 1\n",
      "GPU[7]\t\t: (Topology) Numa Node: 1\n",
      "GPU[7]\t\t: (Topology) Numa Affinity: 1\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!python vllm/collect_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9266be-0311-45c9-9398-d4e259c0e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-16 00:02:29 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=512, output_len=128, model='meta-llama/Llama-2-7b-chat-hf', tokenizer='meta-llama/Llama-2-7b-chat-hf', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-16 00:02:39 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-16 00:02:39 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-16 00:02:39 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-16 00:02:39 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-16 00:02:39 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-16 00:02:39 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:44 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:44 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:44 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:44 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:44 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:44 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:44 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:44 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:44 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-16 00:02:46 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x74ebe607be50>, local_subscribe_port=60503, remote_subscribe_port=None)\n",
      "INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:46 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:46 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.75it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.88it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:02:47 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "INFO 10-16 00:02:47 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:02:48 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:02:48 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:02:48 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:02:48 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:02:48 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:02:49 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "INFO 10-16 00:03:19 distributed_gpu_executor.py:57] # GPU blocks: 167700, # CPU blocks: 4096\n",
      "INFO 10-16 00:03:19 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 655.08x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:03:20 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:03:20 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:03:20 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:03:20 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:03:20 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:03:20 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-16 00:03:21 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-16 00:03:21 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:03:21 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "INFO 10-16 00:03:34 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "Processed prompts: 100%|â–ˆ| 1000/1000 [00:18<00:00, 53.59it/s, est. speed input: \n",
      "INFO 10-16 00:03:54 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230348)\u001b[0;0m INFO 10-16 00:03:54 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230349)\u001b[0;0m INFO 10-16 00:03:54 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230351)\u001b[0;0m INFO 10-16 00:03:54 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230347)\u001b[0;0m INFO 10-16 00:03:54 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230352)\u001b[0;0m INFO 10-16 00:03:54 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230350)\u001b[0;0m INFO 10-16 00:03:54 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1230353)\u001b[0;0m INFO 10-16 00:03:54 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 52.66 requests/s, 33702.25 tokens/s\n",
      "[rank0]:[W1016 00:03:56.597580017 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Triton FA\n",
    "!VLLM_USE_TRITON_FLASH_ATTN=1 python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 512 --output-len 128 --model meta-llama/Llama-2-7b-chat-hf -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9287b3-a69f-4670-8107-50342baca6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-16 00:04:01 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=512, output_len=128, model='meta-llama/Llama-2-7b-chat-hf', tokenizer='meta-llama/Llama-2-7b-chat-hf', quantization=None, tensor_parallel_size=8, n=1, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=True, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-16 00:04:13 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-16 00:04:13 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-16 00:04:13 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-16 00:04:13 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-16 00:04:13 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-16 00:04:14 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:19 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:19 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:19 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:19 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-16 00:04:20 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7bcd70151450>, local_subscribe_port=39759, remote_subscribe_port=None)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:20 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:20 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:21 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  4.06it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.98it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.14it/s]\n",
      "\n",
      "INFO 10-16 00:04:22 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:22 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:23 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:23 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:23 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:23 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:23 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:24 model_runner.py:1071] Loading model weights took 1.5874 GB\n",
      "INFO 10-16 00:04:37 distributed_gpu_executor.py:57] # GPU blocks: 167738, # CPU blocks: 4096\n",
      "INFO 10-16 00:04:37 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 655.23x\n",
      "INFO 10-16 00:04:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-16 00:04:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:39 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:39 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:39 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:39 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:39 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:39 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:04:52 model_runner.py:1530] Graph capturing finished in 14 secs.\n",
      "Processed prompts: 100%|â–ˆ| 1000/1000 [00:17<00:00, 56.48it/s, est. speed input: \n",
      "INFO 10-16 00:05:12 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236347)\u001b[0;0m INFO 10-16 00:05:12 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236351)\u001b[0;0m INFO 10-16 00:05:12 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236350)\u001b[0;0m INFO 10-16 00:05:12 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236352)\u001b[0;0m INFO 10-16 00:05:12 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236348)\u001b[0;0m INFO 10-16 00:05:12 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236349)\u001b[0;0m INFO 10-16 00:05:12 multiproc_worker_utils.py:242] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1236353)\u001b[0;0m INFO 10-16 00:05:12 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 55.45 requests/s, 35485.78 tokens/s\n",
      "[rank0]:[W1016 00:05:13.586457688 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "# CK FA\n",
    "!VLLM_USE_TRITON_FLASH_ATTN=0 python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len 512 --output-len 128 --model meta-llama/Llama-2-7b-chat-hf -tp 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2369d7d-2a2b-4c6a-980f-e883d268a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/flash_attn-2.6.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nbformat in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.20.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97778c2-c7d7-483e-aed2-21ad7f8ea66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       FA  Requests per Second  Tokens per Second\n",
      "0  triton                52.43           33556.50\n",
      "1      ck                55.22           35340.21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nbformat\n",
    "\n",
    "triton_fa = 'Throughput: 52.43 requests/s, 33556.50 tokens/s'\n",
    "ck_fa = 'Throughput: 55.22 requests/s, 35340.21 tokens/s'\n",
    "\n",
    "# Use regular expressions to extract numbers\n",
    "matches = re.findall(r\"Throughput: ([\\d.]+) requests/s, ([\\d.]+) tokens/s\", triton_fa)\n",
    "if matches:\n",
    "    requests_per_sec, tokens_per_sec = matches[0]\n",
    "    data = {\n",
    "        'FA': 'triton',\n",
    "        'Requests per Second': float(requests_per_sec),\n",
    "        'Tokens per Second': float(tokens_per_sec)\n",
    "    }\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "matches = re.findall(r\"Throughput: ([\\d.]+) requests/s, ([\\d.]+) tokens/s\", ck_fa)\n",
    "if matches:\n",
    "    requests_per_sec, tokens_per_sec = matches[0]\n",
    "    data = {\n",
    "        'FA': 'ck', \n",
    "        'Requests per Second': float(requests_per_sec),\n",
    "        'Tokens per Second': float(tokens_per_sec)\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139fc17c-fb2f-458f-9957-12848f8be8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.321380888804119 Requests per Second faster\n",
      "5.3155424433418235 Tokens per Second faster\n"
     ]
    }
   ],
   "source": [
    "# Extract values for 'ck'\n",
    "ck_rps = df.loc[df['FA'] == 'ck', 'Requests per Second'].values[0]\n",
    "ck_tps = df.loc[df['FA'] == 'ck', 'Tokens per Second'].values[0]\n",
    "\n",
    "# Extract values for 'triton'\n",
    "triton_rps = df.loc[df['FA'] == 'triton', 'Requests per Second'].values[0]\n",
    "triton_tps = df.loc[df['FA'] == 'triton', 'Tokens per Second'].values[0]\n",
    "\n",
    "# Calculate percentage difference for Requests per Second (RPS)\n",
    "percent_diff_rps = ((ck_rps - triton_rps) / triton_rps) * 100\n",
    "print(percent_diff_rps, 'Requests per Second faster')\n",
    "\n",
    "# Calculate percentage difference for Tokens per Second (TKS)\n",
    "percent_diff_tps = ((ck_tps - triton_tps) / triton_tps) * 100\n",
    "print(percent_diff_tps, 'Tokens per Second faster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bebeec3f-9838-45af-9cda-747e4ac8eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, input_len, output_len, tp):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    # Initialize the DataFrame\n",
    "    df = pd.DataFrame(columns=['FA', 'Requests per Second', 'Tokens per Second'])\n",
    "    \n",
    "    # Function to run the benchmark command and capture output\n",
    "    def run_benchmark(use_triton):\n",
    "        # Set the environment variable\n",
    "        VLLM_USE_TRITON_FLASH_ATTN = '1' if use_triton else '0'\n",
    "        # Construct the command\n",
    "        command = f\"VLLM_USE_TRITON_FLASH_ATTN={VLLM_USE_TRITON_FLASH_ATTN} python vllm/benchmarks/benchmark_throughput.py --backend vllm --input-len {input_len} --output-len {output_len} --model {model} -tp {tp}\"\n",
    "        # Run the command and capture the output\n",
    "        output = get_ipython().getoutput(command)\n",
    "        output_str = ' '.join(output)\n",
    "        # Use regular expressions to extract the throughput values\n",
    "        matches = re.findall(r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\", output_str)\n",
    "        if matches:\n",
    "            requests_per_sec, tokens_per_sec = map(float, matches[0])\n",
    "            return requests_per_sec, tokens_per_sec\n",
    "        else:\n",
    "            print(f\"No throughput data found for {'Triton' if use_triton else 'CK'} FA.\")\n",
    "            return None, None\n",
    "\n",
    "    # Run benchmarks for CK FA (use_triton=False)\n",
    "    ck_rps, ck_tps = run_benchmark(use_triton=False)\n",
    "    if ck_rps is None or ck_tps is None:\n",
    "        print(\"Benchmark failed for CK FA.\")\n",
    "        return None\n",
    "\n",
    "    # Append CK FA results to the DataFrame\n",
    "    df.loc[len(df)] = {'FA': 'ck', 'Requests per Second': ck_rps, 'Tokens per Second': ck_tps}\n",
    "\n",
    "    # Run benchmarks for Triton FA (use_triton=True)\n",
    "    triton_rps, triton_tps = run_benchmark(use_triton=True)\n",
    "    if triton_rps is None or triton_tps is None:\n",
    "        print(\"Benchmark failed for Triton FA.\")\n",
    "        return None\n",
    "\n",
    "    # Append Triton FA results to the DataFrame\n",
    "    df.loc[len(df)] = {'FA': 'triton', 'Requests per Second': triton_rps, 'Tokens per Second': triton_tps}\n",
    "\n",
    "    # Calculate percentage differences (Triton is baseline)\n",
    "    percent_diff_rps = ((ck_rps - triton_rps) / triton_rps) * 100\n",
    "    percent_diff_tps = ((ck_tps - triton_tps) / triton_tps) * 100\n",
    "    avg_percent_diff = (percent_diff_rps + percent_diff_tps) / 2\n",
    "\n",
    "    # Add percentage differences to the DataFrame\n",
    "    df['% Difference RPS'] = [percent_diff_rps, 0]\n",
    "    df['% Difference TPS'] = [percent_diff_tps, 0]\n",
    "    df['% Difference Avg'] = [avg_percent_diff, 0]\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "311201e8-bc19-4b6c-bf6d-d23000c0ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                83.72           21432.81          7.984006   \n",
      "1  triton                77.53           19847.90          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          7.985278          7.984642  \n",
      "1          0.000000          0.000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FA</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ck</td>\n",
       "      <td>83.72</td>\n",
       "      <td>21432.81</td>\n",
       "      <td>7.984006</td>\n",
       "      <td>7.985278</td>\n",
       "      <td>7.984642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triton</td>\n",
       "      <td>77.53</td>\n",
       "      <td>19847.90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0      ck                83.72           21432.81          7.984006   \n",
       "1  triton                77.53           19847.90          0.000000   \n",
       "\n",
       "   % Difference TPS  % Difference Avg  \n",
       "0          7.985278          7.984642  \n",
       "1          0.000000          0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function with your parameters\n",
    "df_results = benchmark_model(\n",
    "    model='meta-llama/Llama-2-7b-chat-hf',\n",
    "    input_len=128,\n",
    "    output_len=128,\n",
    "    tp=8\n",
    ")\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739a116e-095b-460f-b0cd-8ba37ba9f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-2-7b-chat-hf', 'input_len': 0, 'output_len': 128, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                93.64           11985.92          7.867757   \n",
      "1  triton                86.81           11111.56          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          7.868922           7.86834  \n",
      "1          0.000000           0.00000  \n",
      "{'model': 'meta-llama/Llama-2-7b-chat-hf', 'input_len': 0, 'output_len': 256, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                48.52           12422.23         10.978957   \n",
      "1  triton                43.72           11191.04          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0         11.001569         10.990263  \n",
      "1          0.000000          0.000000  \n",
      "{'model': 'meta-llama/Llama-2-7b-chat-hf', 'input_len': 0, 'output_len': 512, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                22.56           11548.59         -1.009215   \n",
      "1  triton                22.79           11670.64          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0         -1.045787         -1.027501  \n",
      "1          0.000000          0.000000  \n",
      "{'model': 'meta-llama/Llama-2-7b-chat-hf', 'input_len': 0, 'output_len': 1024, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                10.82           11077.85          1.787394   \n",
      "1  triton                10.63           10888.37          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          1.740205            1.7638  \n",
      "1          0.000000            0.0000  \n",
      "{'model': 'meta-llama/Llama-2-7b-chat-hf', 'input_len': 0, 'output_len': 2048, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                 4.80            9828.08           -0.2079   \n",
      "1  triton                 4.81            9844.90            0.0000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          -0.17085         -0.189375  \n",
      "1           0.00000          0.000000  \n",
      "{'model': 'meta-llama/Llama-2-7b-chat-hf', 'input_len': 0, 'output_len': 4096, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                 1.96            8025.77          0.512821   \n",
      "1  triton                 1.95            7995.48          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          0.378839           0.44583  \n",
      "1          0.000000           0.00000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FA</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ck</td>\n",
       "      <td>93.64</td>\n",
       "      <td>11985.92</td>\n",
       "      <td>7.867757</td>\n",
       "      <td>7.868922</td>\n",
       "      <td>7.868340</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triton</td>\n",
       "      <td>86.81</td>\n",
       "      <td>11111.56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ck</td>\n",
       "      <td>48.52</td>\n",
       "      <td>12422.23</td>\n",
       "      <td>10.978957</td>\n",
       "      <td>11.001569</td>\n",
       "      <td>10.990263</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>triton</td>\n",
       "      <td>43.72</td>\n",
       "      <td>11191.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=256, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ck</td>\n",
       "      <td>22.56</td>\n",
       "      <td>11548.59</td>\n",
       "      <td>-1.009215</td>\n",
       "      <td>-1.045787</td>\n",
       "      <td>-1.027501</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>triton</td>\n",
       "      <td>22.79</td>\n",
       "      <td>11670.64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=512, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ck</td>\n",
       "      <td>10.82</td>\n",
       "      <td>11077.85</td>\n",
       "      <td>1.787394</td>\n",
       "      <td>1.740205</td>\n",
       "      <td>1.763800</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>triton</td>\n",
       "      <td>10.63</td>\n",
       "      <td>10888.37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=1024, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ck</td>\n",
       "      <td>4.80</td>\n",
       "      <td>9828.08</td>\n",
       "      <td>-0.207900</td>\n",
       "      <td>-0.170850</td>\n",
       "      <td>-0.189375</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>triton</td>\n",
       "      <td>4.81</td>\n",
       "      <td>9844.90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=2048, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ck</td>\n",
       "      <td>1.96</td>\n",
       "      <td>8025.77</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.378839</td>\n",
       "      <td>0.445830</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>triton</td>\n",
       "      <td>1.95</td>\n",
       "      <td>7995.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=0, output_len=4096, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0       ck                93.64           11985.92          7.867757   \n",
       "1   triton                86.81           11111.56          0.000000   \n",
       "2       ck                48.52           12422.23         10.978957   \n",
       "3   triton                43.72           11191.04          0.000000   \n",
       "4       ck                22.56           11548.59         -1.009215   \n",
       "5   triton                22.79           11670.64          0.000000   \n",
       "6       ck                10.82           11077.85          1.787394   \n",
       "7   triton                10.63           10888.37          0.000000   \n",
       "8       ck                 4.80            9828.08         -0.207900   \n",
       "9   triton                 4.81            9844.90          0.000000   \n",
       "10      ck                 1.96            8025.77          0.512821   \n",
       "11  triton                 1.95            7995.48          0.000000   \n",
       "\n",
       "    % Difference TPS  % Difference Avg                              Config  \n",
       "0           7.868922          7.868340   input_len=0, output_len=128, tp=8  \n",
       "1           0.000000          0.000000   input_len=0, output_len=128, tp=8  \n",
       "2          11.001569         10.990263   input_len=0, output_len=256, tp=8  \n",
       "3           0.000000          0.000000   input_len=0, output_len=256, tp=8  \n",
       "4          -1.045787         -1.027501   input_len=0, output_len=512, tp=8  \n",
       "5           0.000000          0.000000   input_len=0, output_len=512, tp=8  \n",
       "6           1.740205          1.763800  input_len=0, output_len=1024, tp=8  \n",
       "7           0.000000          0.000000  input_len=0, output_len=1024, tp=8  \n",
       "8          -0.170850         -0.189375  input_len=0, output_len=2048, tp=8  \n",
       "9           0.000000          0.000000  input_len=0, output_len=2048, tp=8  \n",
       "10          0.378839          0.445830  input_len=0, output_len=4096, tp=8  \n",
       "11          0.000000          0.000000  input_len=0, output_len=4096, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 256, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 512, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 1024, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 2048, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 0, 'output_len': 4096, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be3b999-10bd-47a3-bb44-d1ffe7bc6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                84.56           21647.27         24.536082   \n",
      "1  triton                67.90           17381.82          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          24.53972         24.537901  \n",
      "1           0.00000          0.000000  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                75.69           29065.78         18.934632   \n",
      "1  triton                63.64           24437.42          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0         18.939643         18.937137  \n",
      "1          0.000000          0.000000  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                63.53           40659.06          3.116377   \n",
      "1  triton                61.61           39432.03          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0           3.11176          3.114068  \n",
      "1           0.00000          0.000000  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                46.86           53986.25         10.414703   \n",
      "1  triton                42.44           48892.89          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0         10.417384         10.416043  \n",
      "1          0.000000          0.000000  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                31.33           68172.61           8.48338   \n",
      "1  triton                28.88           62844.81           0.00000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          8.477709          8.480544  \n",
      "1          0.000000          0.000000  \n",
      "{'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8}\n",
      "       FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
      "0      ck                18.50           78139.18          9.208973   \n",
      "1  triton                16.94           71548.16          0.000000   \n",
      "\n",
      "   % Difference TPS  % Difference Avg  \n",
      "0          9.212005          9.210489  \n",
      "1          0.000000          0.000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FA</th>\n",
       "      <th>Requests per Second</th>\n",
       "      <th>Tokens per Second</th>\n",
       "      <th>% Difference RPS</th>\n",
       "      <th>% Difference TPS</th>\n",
       "      <th>% Difference Avg</th>\n",
       "      <th>Config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ck</td>\n",
       "      <td>84.56</td>\n",
       "      <td>21647.27</td>\n",
       "      <td>24.536082</td>\n",
       "      <td>24.539720</td>\n",
       "      <td>24.537901</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triton</td>\n",
       "      <td>67.90</td>\n",
       "      <td>17381.82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=128, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ck</td>\n",
       "      <td>75.69</td>\n",
       "      <td>29065.78</td>\n",
       "      <td>18.934632</td>\n",
       "      <td>18.939643</td>\n",
       "      <td>18.937137</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>triton</td>\n",
       "      <td>63.64</td>\n",
       "      <td>24437.42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=256, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ck</td>\n",
       "      <td>63.53</td>\n",
       "      <td>40659.06</td>\n",
       "      <td>3.116377</td>\n",
       "      <td>3.111760</td>\n",
       "      <td>3.114068</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>triton</td>\n",
       "      <td>61.61</td>\n",
       "      <td>39432.03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=512, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ck</td>\n",
       "      <td>46.86</td>\n",
       "      <td>53986.25</td>\n",
       "      <td>10.414703</td>\n",
       "      <td>10.417384</td>\n",
       "      <td>10.416043</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>triton</td>\n",
       "      <td>42.44</td>\n",
       "      <td>48892.89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=1024, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ck</td>\n",
       "      <td>31.33</td>\n",
       "      <td>68172.61</td>\n",
       "      <td>8.483380</td>\n",
       "      <td>8.477709</td>\n",
       "      <td>8.480544</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>triton</td>\n",
       "      <td>28.88</td>\n",
       "      <td>62844.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=2048, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ck</td>\n",
       "      <td>18.50</td>\n",
       "      <td>78139.18</td>\n",
       "      <td>9.208973</td>\n",
       "      <td>9.212005</td>\n",
       "      <td>9.210489</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>triton</td>\n",
       "      <td>16.94</td>\n",
       "      <td>71548.16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>input_len=4096, output_len=128, tp=8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FA  Requests per Second  Tokens per Second  % Difference RPS  \\\n",
       "0       ck                84.56           21647.27         24.536082   \n",
       "1   triton                67.90           17381.82          0.000000   \n",
       "2       ck                75.69           29065.78         18.934632   \n",
       "3   triton                63.64           24437.42          0.000000   \n",
       "4       ck                63.53           40659.06          3.116377   \n",
       "5   triton                61.61           39432.03          0.000000   \n",
       "6       ck                46.86           53986.25         10.414703   \n",
       "7   triton                42.44           48892.89          0.000000   \n",
       "8       ck                31.33           68172.61          8.483380   \n",
       "9   triton                28.88           62844.81          0.000000   \n",
       "10      ck                18.50           78139.18          9.208973   \n",
       "11  triton                16.94           71548.16          0.000000   \n",
       "\n",
       "    % Difference TPS  % Difference Avg                                Config  \n",
       "0          24.539720         24.537901   input_len=128, output_len=128, tp=8  \n",
       "1           0.000000          0.000000   input_len=128, output_len=128, tp=8  \n",
       "2          18.939643         18.937137   input_len=256, output_len=128, tp=8  \n",
       "3           0.000000          0.000000   input_len=256, output_len=128, tp=8  \n",
       "4           3.111760          3.114068   input_len=512, output_len=128, tp=8  \n",
       "5           0.000000          0.000000   input_len=512, output_len=128, tp=8  \n",
       "6          10.417384         10.416043  input_len=1024, output_len=128, tp=8  \n",
       "7           0.000000          0.000000  input_len=1024, output_len=128, tp=8  \n",
       "8           8.477709          8.480544  input_len=2048, output_len=128, tp=8  \n",
       "9           0.000000          0.000000  input_len=2048, output_len=128, tp=8  \n",
       "10          9.212005          9.210489  input_len=4096, output_len=128, tp=8  \n",
       "11          0.000000          0.000000  input_len=4096, output_len=128, tp=8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of configurations to test\n",
    "configs = [\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 128, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 256, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 512, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 1024, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 2048, 'output_len': 128, 'tp': 8},\n",
    "    {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'input_len': 4096, 'output_len': 128, 'tp': 8},\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Run benchmarks for each configuration\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    df_result = benchmark_model(**config)\n",
    "    if df_result is not None:\n",
    "        # Add a column for the configuration\n",
    "        df_result['Config'] = f\"input_len={config['input_len']}, output_len={config['output_len']}, tp={config['tp']}\"\n",
    "        all_results = pd.concat([all_results, df_result], ignore_index=True)\n",
    "\n",
    "# Display all results\n",
    "display(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vllm]",
   "language": "python",
   "name": "conda-env-vllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
