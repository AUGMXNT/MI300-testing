{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ece1d8-2533-4921-864d-55a67e02871d",
   "metadata": {},
   "source": [
    "# Mixtral - Runpod\n",
    "https://blog.runpod.io/amd-mi300x-vs-nvidia-h100-sxm-performance-comparison-on-mixtral-8x7b-inference/\n",
    "Jul 1, 2024 \n",
    "\n",
    "Mixtral in:128 out:128\n",
    "\n",
    "Tensorwave\n",
    "https://tensorwave.com/blog/amds-mi300x-outperforms-nvidias-h100-for-llm-inference\n",
    "128 in/out\n",
    "\n",
    "256 in/out\n",
    "https://www.nscale.com/blog/nscale-benchmarks-amd-mi300x-gpus-with-gemm-tuning-improves-throughput-and-latency-by-up-to-7-2x\n",
    "https://www.nscale.com/blog/nscale-benchmarks-updated-amd-mi300x-gpus-with-gemm-tuning-using-gradlib-and-textprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc3925d-b598-4394-9bf7-be36e3f9aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase File handles\n",
    "!ulimit -n 131072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c42ce2d-99f1-4c03-98aa-684187b0ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-23 03:31:01 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=16, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:31:11 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:31:11 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:31:11 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
      "WARNING 10-23 03:31:11 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:31:11 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:31:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:31:16 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:31:16 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:31:16 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7160b7ebd5d0>, local_subscribe_port=58715, remote_subscribe_port=None)\n",
      "INFO 10-23 03:31:16 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:16 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:31:16 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m WARNING 10-23 03:31:16 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:31:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:31:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/19 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 1/19 [00:01<00:20,  1.16s/it]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 2/19 [00:02<00:20,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 3/19 [00:03<00:19,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 4/19 [00:04<00:17,  1.20s/it]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 5/19 [00:06<00:16,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 6/19 [00:07<00:15,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 7/19 [00:08<00:14,  1.22s/it]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 8/19 [00:09<00:13,  1.24s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 9/19 [00:10<00:12,  1.24s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 10/19 [00:12<00:11,  1.25s/it]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 11/19 [00:13<00:10,  1.26s/it]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 12/19 [00:14<00:08,  1.27s/it]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 13/19 [00:16<00:07,  1.28s/it]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 14/19 [00:17<00:06,  1.28s/it]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 15/19 [00:18<00:05,  1.26s/it]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 16/19 [00:19<00:03,  1.27s/it]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 17/19 [00:21<00:02,  1.26s/it]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 18/19 [00:22<00:01,  1.24s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 19/19 [00:23<00:00,  1.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 19/19 [00:23<00:00,  1.24s/it]\n",
      "\n",
      "INFO 10-23 03:31:41 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:41 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:31:56 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:31:56 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:31:59 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:31:59 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:32:00 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:32:00 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:32:00 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:32:00 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:32:20 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:32:20 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "Processed prompts: 100%|█| 16/16 [00:03<00:00,  4.68it/s, est. speed input: 603.\n",
      "INFO 10-23 03:32:24 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=618485)\u001b[0;0m INFO 10-23 03:32:24 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 4.66 requests/s, 1192.41 tokens/s\n",
      "[rank0]:[W1023 03:32:25.663506162 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!VLLM_USE_TRITON_FLASH_ATTN=0 python benchmark_throughput.py --backend vllm --model mistralai/Mixtral-8x7B-Instruct-v0.1 --input-len=128 --output-len=128 --num-prompts=16 --tensor-parallel-size 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76af292-9328-4731-9e6f-8a9c65029e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-23 03:34:53 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=8, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:35:03 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:35:03 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "/home/hotaisle/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
      "INFO 10-23 03:35:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:35:03 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:35:03 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:35:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:35:04 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/19 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 1/19 [00:01<00:33,  1.85s/it]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 2/19 [00:03<00:34,  2.02s/it]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 3/19 [00:06<00:33,  2.08s/it]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 4/19 [00:08<00:31,  2.10s/it]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 5/19 [00:10<00:29,  2.14s/it]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 6/19 [00:12<00:28,  2.16s/it]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 7/19 [00:14<00:26,  2.17s/it]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 8/19 [00:17<00:23,  2.16s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 9/19 [00:19<00:21,  2.18s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 10/19 [00:21<00:19,  2.17s/it]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 11/19 [00:23<00:17,  2.16s/it]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 12/19 [00:25<00:15,  2.17s/it]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 13/19 [00:27<00:12,  2.16s/it]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 14/19 [00:30<00:10,  2.16s/it]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 15/19 [00:32<00:08,  2.18s/it]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 16/19 [00:34<00:06,  2.19s/it]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 17/19 [00:36<00:04,  2.18s/it]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 18/19 [00:38<00:02,  2.17s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 19/19 [00:40<00:00,  2.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 19/19 [00:40<00:00,  2.14s/it]\n",
      "\n",
      "INFO 10-23 03:35:45 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 03:35:57 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:36:00 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 03:36:00 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 03:36:01 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:36:01 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:36:13 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Processed prompts: 100%|█| 8/8 [00:04<00:00,  1.74it/s, est. speed input: 224.85\n",
      "Throughput: 1.74 requests/s, 445.01 tokens/s\n",
      "[rank0]:[W1023 03:36:18.515738333 ProcessGroupNCCL.cpp:1326] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!VLLM_USE_TRITON_FLASH_ATTN=0 python benchmark_throughput.py --backend vllm --model mistralai/Mixtral-8x7B-Instruct-v0.1 --input-len=128 --output-len=128 --num-prompts=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6feae24b-3ee0-4197-a2b6-16aa60ce96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark with num_prompt=1\n",
      "STDOUT:\n",
      "WARNING 10-23 03:37:14 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=1, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:37:24 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:37:24 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:37:24 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:37:24 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:37:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:37:24 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:29 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:29 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 10-23 03:37:29 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:29 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:37:29 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:29 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:37:29 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7791583e0c50>, local_subscribe_port=36455, remote_subscribe_port=None)\n",
      "INFO 10-23 03:37:29 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:37:29 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:29 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m WARNING 10-23 03:37:29 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:37:29 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:29 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:37:30 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:30 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:37:56 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:37:56 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:38:09 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:38:09 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:38:12 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:38:12 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:38:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:38:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:38:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:38:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:38:33 model_runner.py:1530] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:38:33 model_runner.py:1530] Graph capturing finished in 20 secs.\n",
      "INFO 10-23 03:38:36 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=629613)\u001b[0;0m INFO 10-23 03:38:36 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 0.49 requests/s, 125.48 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=1\n",
      "\n",
      "Running benchmark with num_prompt=2\n",
      "STDOUT:\n",
      "WARNING 10-23 03:38:45 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=2, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:38:55 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:38:55 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:38:55 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:38:55 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:38:55 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:38:55 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:38:59 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:38:59 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:00 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:39:00 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:00 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:39:00 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:39:00 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ee9bea78ad0>, local_subscribe_port=58479, remote_subscribe_port=None)\n",
      "INFO 10-23 03:39:00 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:39:00 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:00 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m WARNING 10-23 03:39:00 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:39:00 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:00 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:39:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:39:26 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:26 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:39 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:39:39 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:39:43 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:39:43 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:39:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:39:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:39:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:40:03 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:40:03 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:40:07 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=633525)\u001b[0;0m INFO 10-23 03:40:07 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 0.68 requests/s, 173.65 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=2\n",
      "\n",
      "Running benchmark with num_prompt=4\n",
      "STDOUT:\n",
      "WARNING 10-23 03:40:16 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=4, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:40:26 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:40:26 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:40:26 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:40:26 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:40:26 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:40:26 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:30 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:30 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:31 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:40:31 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:31 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:40:31 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:40:31 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7396baa8c090>, local_subscribe_port=49663, remote_subscribe_port=None)\n",
      "INFO 10-23 03:40:31 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:40:31 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:31 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m WARNING 10-23 03:40:31 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:40:31 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:31 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:40:32 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:32 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:40:57 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:40:57 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:41:10 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:41:10 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:41:14 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:41:14 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:41:15 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:41:15 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:41:15 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:41:15 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:41:34 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:41:34 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:41:39 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=637433)\u001b[0;0m INFO 10-23 03:41:39 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 1.19 requests/s, 303.74 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=4\n",
      "\n",
      "Running benchmark with num_prompt=8\n",
      "STDOUT:\n",
      "WARNING 10-23 03:41:47 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=8, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:41:57 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:41:57 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:41:57 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:41:58 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:41:58 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:41:58 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:02 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:42:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:42:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:42:03 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x79d825e98590>, local_subscribe_port=44651, remote_subscribe_port=None)\n",
      "INFO 10-23 03:42:03 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:42:03 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:03 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m WARNING 10-23 03:42:03 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:42:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:42:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:42:29 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:29 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:42:42 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:42 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:42:45 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:42:45 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:42:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:42:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:42:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:43:06 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:43:06 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:43:09 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=641345)\u001b[0;0m INFO 10-23 03:43:09 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 3.07 requests/s, 785.25 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=8\n",
      "\n",
      "Running benchmark with num_prompt=16\n",
      "STDOUT:\n",
      "WARNING 10-23 03:43:18 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=16, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:43:27 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:43:27 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:43:27 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:43:28 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:43:28 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:43:28 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:32 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:32 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:32 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:32 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:43:32 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:43:32 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:43:33 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7b80a1491950>, local_subscribe_port=46105, remote_subscribe_port=None)\n",
      "INFO 10-23 03:43:33 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:43:33 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:33 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m WARNING 10-23 03:43:33 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:43:33 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:33 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:43:33 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:33 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:43:57 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:43:59 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:44:11 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:44:12 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:44:15 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:44:15 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "INFO 10-23 03:44:16 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:44:16 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:44:16 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:44:16 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:44:34 model_runner.py:1530] Graph capturing finished in 18 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:44:34 model_runner.py:1530] Graph capturing finished in 18 secs.\n",
      "INFO 10-23 03:44:39 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=645114)\u001b[0;0m INFO 10-23 03:44:39 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 4.74 requests/s, 1212.26 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=16\n",
      "\n",
      "Running benchmark with num_prompt=32\n",
      "STDOUT:\n",
      "WARNING 10-23 03:44:48 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=32, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:44:57 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:44:57 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:44:57 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:44:57 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:44:57 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:44:58 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:02 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:45:02 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:45:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:45:03 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x79d90a4568d0>, local_subscribe_port=35083, remote_subscribe_port=None)\n",
      "INFO 10-23 03:45:03 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:45:03 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:03 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m WARNING 10-23 03:45:03 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:45:03 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:45:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:29 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:45:29 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:42 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:45:42 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:45:45 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:45:45 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "INFO 10-23 03:45:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:45:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:45:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:46:06 model_runner.py:1530] Graph capturing finished in 20 secs.\n",
      "INFO 10-23 03:46:06 model_runner.py:1530] Graph capturing finished in 20 secs.\n",
      "INFO 10-23 03:46:10 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=648924)\u001b[0;0m INFO 10-23 03:46:10 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 10.27 requests/s, 2628.69 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=32\n",
      "\n",
      "Running benchmark with num_prompt=64\n",
      "STDOUT:\n",
      "WARNING 10-23 03:46:19 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=64, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:46:28 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:46:28 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:46:28 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:46:28 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:46:28 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:46:29 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:46:33 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:46:33 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 10-23 03:46:33 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:46:33 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:46:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:46:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:46:34 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7e296daf5190>, local_subscribe_port=57213, remote_subscribe_port=None)\n",
      "INFO 10-23 03:46:34 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:46:34 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:46:34 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m WARNING 10-23 03:46:34 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:46:34 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:46:34 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:46:34 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:46:34 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:47:00 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:47:00 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:47:13 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:47:13 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:47:16 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:47:16 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:47:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:47:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:47:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:47:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:47:36 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:47:36 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:47:41 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=652739)\u001b[0;0m INFO 10-23 03:47:41 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 16.59 requests/s, 4246.45 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=64\n",
      "\n",
      "Running benchmark with num_prompt=128\n",
      "STDOUT:\n",
      "WARNING 10-23 03:47:50 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=128, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:48:00 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:48:00 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:48:00 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:48:00 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:48:00 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:48:00 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:05 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:05 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:05 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:48:05 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:48:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:48:05 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7980b9e0d710>, local_subscribe_port=48299, remote_subscribe_port=None)\n",
      "INFO 10-23 03:48:05 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:48:05 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:05 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m WARNING 10-23 03:48:05 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:48:05 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:05 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:48:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:48:31 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:32 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:48:44 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:44 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:48:48 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:48:48 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:49 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:48:49 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:48:49 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:48:49 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:49:08 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:49:08 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:49:14 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=656556)\u001b[0;0m INFO 10-23 03:49:14 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 25.17 requests/s, 6444.18 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=128\n",
      "\n",
      "Running benchmark with num_prompt=256\n",
      "STDOUT:\n",
      "WARNING 10-23 03:49:23 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=256, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:49:33 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:49:33 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:49:33 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:49:33 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:49:33 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:49:33 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:49:38 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:49:38 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "INFO 10-23 03:49:38 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:49:38 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:49:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:49:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:49:38 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7a7c40cb9fd0>, local_subscribe_port=43599, remote_subscribe_port=None)\n",
      "INFO 10-23 03:49:38 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:49:38 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:49:38 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m WARNING 10-23 03:49:38 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:49:39 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:49:39 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:49:39 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:49:39 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:50:04 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:50:04 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:50:17 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:50:17 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:50:21 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:50:21 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "INFO 10-23 03:50:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:50:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:50:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:50:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:50:41 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:50:41 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:50:52 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=660368)\u001b[0;0m INFO 10-23 03:50:52 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 27.10 requests/s, 6936.75 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=256\n",
      "\n",
      "Running benchmark with num_prompt=512\n",
      "STDOUT:\n",
      "WARNING 10-23 03:51:01 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=512, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:51:10 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:51:10 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:51:10 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:51:10 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:51:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:51:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:15 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:15 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:15 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:15 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:51:15 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:51:15 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:51:16 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f8685ec0bd0>, local_subscribe_port=58187, remote_subscribe_port=None)\n",
      "INFO 10-23 03:51:16 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:51:16 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:16 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m WARNING 10-23 03:51:16 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:51:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:16 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:51:16 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:16 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:51:41 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:42 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:55 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:51:55 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:51:58 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:51:58 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:59 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:51:59 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:51:59 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:51:59 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:52:19 model_runner.py:1530] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:52:19 model_runner.py:1530] Graph capturing finished in 20 secs.\n",
      "INFO 10-23 03:52:37 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=664513)\u001b[0;0m INFO 10-23 03:52:37 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 29.30 requests/s, 7501.97 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=512\n",
      "\n",
      "Running benchmark with num_prompt=1024\n",
      "STDOUT:\n",
      "WARNING 10-23 03:52:46 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=2, n=1, num_prompts=1024, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:52:55 config.py:887] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 03:52:55 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:52:55 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 03:52:56 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 03:52:56 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-23 03:52:56 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:00 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:00 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:00 utils.py:1008] Found nccl from library librccl.so.1\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:00 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:53:00 utils.py:1008] Found nccl from library librccl.so.1\n",
      "INFO 10-23 03:53:00 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 03:53:01 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x79af37240dd0>, local_subscribe_port=55423, remote_subscribe_port=None)\n",
      "INFO 10-23 03:53:01 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:53:01 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:01 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m WARNING 10-23 03:53:01 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:53:01 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:01 selector.py:120] Using ROCmFlashAttention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:53:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:27 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:53:27 model_runner.py:1071] Loading model weights took 43.5064 GB\n",
      "INFO 10-23 03:53:40 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:40 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:53:43 distributed_gpu_executor.py:57] # GPU blocks: 124518, # CPU blocks: 4096\n",
      "INFO 10-23 03:53:43 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 60.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:53:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:53:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:53:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:54:04 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:54:04 model_runner.py:1530] Graph capturing finished in 19 secs.\n",
      "INFO 10-23 03:54:38 multiproc_worker_utils.py:134] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=668827)\u001b[0;0m INFO 10-23 03:54:38 multiproc_worker_utils.py:242] Worker exiting\n",
      "Throughput: 31.02 requests/s, 7942.34 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=1024\n",
      "\n",
      "All benchmarks completed.\n"
     ]
    }
   ],
   "source": [
    "# 80 tokens\n",
    "import subprocess\n",
    "\n",
    "num_prompts = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "base_command = \"\"\"VLLM_USE_TRITON_FLASH_ATTN=0 python benchmark_throughput.py --backend vllm --model mistralai/Mixtral-8x7B-Instruct-v0.1 --input-len=128 --output-len=128 --tensor-parallel-size 2  --num-prompts={num_prompt}\"\"\"\n",
    "\n",
    "for num_prompt in num_prompts:\n",
    "    command = base_command.format(num_prompt=num_prompt)\n",
    "    print(f\"Running benchmark with num_prompt={num_prompt}\")\n",
    "    \n",
    "    # Run the command and capture output\n",
    "    result = subprocess.run(command, shell=True, text=True, capture_output=True)\n",
    "    \n",
    "    # Print stdout and stderr\n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    # print(\"STDERR:\")\n",
    "    # print(result.stderr)\n",
    "    \n",
    "    print(f\"Finished benchmark with num_prompt={num_prompt}\\n\")\n",
    "\n",
    "print(\"All benchmarks completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e997df-fe00-4ef9-9439-fe7b0609e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark with num_prompt=1\n",
      "STDOUT:\n",
      "WARNING 10-23 03:56:52 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=1, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:57:02 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:57:02 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 03:57:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:57:02 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:57:02 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:57:02 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:57:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:57:48 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 03:58:01 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:58:04 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 03:58:04 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 03:58:05 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:58:05 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:58:15 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 0.47 requests/s, 119.40 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=1\n",
      "\n",
      "Running benchmark with num_prompt=2\n",
      "STDOUT:\n",
      "WARNING 10-23 03:58:26 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=2, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 03:58:35 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 03:58:35 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 03:58:36 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:58:36 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 03:58:36 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 03:58:36 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 03:58:36 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 03:59:21 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 03:59:34 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 03:59:37 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 03:59:37 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 03:59:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 03:59:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 03:59:49 model_runner.py:1530] Graph capturing finished in 12 secs.\n",
      "Throughput: 0.62 requests/s, 157.93 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=2\n",
      "\n",
      "Running benchmark with num_prompt=4\n",
      "STDOUT:\n",
      "WARNING 10-23 04:00:01 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=4, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:00:11 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:00:11 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:00:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:00:11 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:00:11 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:00:11 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:00:11 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:00:57 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:01:09 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:01:12 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:01:12 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:01:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:01:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:01:24 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 1.04 requests/s, 265.47 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=4\n",
      "\n",
      "Running benchmark with num_prompt=8\n",
      "STDOUT:\n",
      "WARNING 10-23 04:01:37 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=8, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:01:47 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:01:47 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:01:47 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:01:47 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:01:47 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:01:47 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:01:48 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:02:32 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:02:45 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:02:48 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:02:48 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:02:49 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:02:49 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:03:00 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 2.02 requests/s, 516.66 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=8\n",
      "\n",
      "Running benchmark with num_prompt=16\n",
      "STDOUT:\n",
      "WARNING 10-23 04:03:13 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=16, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:03:22 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:03:22 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:03:23 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:03:23 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:03:23 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:03:23 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:03:23 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:04:08 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:04:21 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:04:24 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:04:24 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:04:24 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:04:24 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:04:36 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 3.19 requests/s, 817.24 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=16\n",
      "\n",
      "Running benchmark with num_prompt=32\n",
      "STDOUT:\n",
      "WARNING 10-23 04:04:49 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=32, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:04:59 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:04:59 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:04:59 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:04:59 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:04:59 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:05:00 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:05:00 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:05:45 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:05:58 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:06:01 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:06:01 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:06:02 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:06:02 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:06:13 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 6.63 requests/s, 1696.81 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=32\n",
      "\n",
      "Running benchmark with num_prompt=64\n",
      "STDOUT:\n",
      "WARNING 10-23 04:06:26 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=64, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:06:36 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:06:36 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:06:36 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:06:36 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:06:36 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:06:37 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:06:37 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:07:22 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:07:35 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:07:38 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:07:38 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:07:39 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:07:39 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:07:50 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 12.45 requests/s, 3188.45 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=64\n",
      "\n",
      "Running benchmark with num_prompt=128\n",
      "STDOUT:\n",
      "WARNING 10-23 04:08:04 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=128, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:08:14 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:08:14 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:08:14 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:08:15 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:08:15 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:08:15 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:08:15 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:09:00 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:09:12 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:09:15 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:09:15 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:09:16 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:09:16 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:09:28 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 18.30 requests/s, 4684.59 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=128\n",
      "\n",
      "Running benchmark with num_prompt=256\n",
      "STDOUT:\n",
      "WARNING 10-23 04:09:43 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=256, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:09:53 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:09:53 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:09:54 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:09:54 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:09:54 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:09:54 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:09:54 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:10:39 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:10:52 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:10:55 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:10:55 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:10:56 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:10:56 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:11:07 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 22.61 requests/s, 5789.11 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=256\n",
      "\n",
      "Running benchmark with num_prompt=512\n",
      "STDOUT:\n",
      "WARNING 10-23 04:11:27 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "Namespace(backend='vllm', dataset=None, input_len=128, output_len=128, model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', quantization=None, tensor_parallel_size=1, n=1, num_prompts=512, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)\n",
      "INFO 10-23 04:11:36 config.py:916] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\n",
      "INFO 10-23 04:11:36 llm_engine.py:237] Initializing an LLM engine (v0.6.4.dev9+g5d264f4a) with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mixtral-8x7B-Instruct-v0.1, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 04:11:37 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:11:37 model_runner.py:1060] Starting to load model mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
      "WARNING 10-23 04:11:37 registry.py:244] Model architecture 'MixtralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 10-23 04:11:37 selector.py:120] Using ROCmFlashAttention backend.\n",
      "INFO 10-23 04:11:37 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-23 04:12:22 model_runner.py:1071] Loading model weights took 86.9985 GB\n",
      "INFO 10-23 04:12:35 fused_moe.py:316] Using configuration from /mnt/nvme1n1p1/miniforge3/envs/vllm/lib/python3.11/site-packages/vllm-0.6.4.dev9+g5d264f4a.rocm624-py3.11-linux-x86_64.egg/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json for MoE layer.\n",
      "INFO 10-23 04:12:38 gpu_executor.py:122] # GPU blocks: 39212, # CPU blocks: 2048\n",
      "INFO 10-23 04:12:38 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 19.15x\n",
      "INFO 10-23 04:12:39 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 04:12:39 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 04:12:50 model_runner.py:1530] Graph capturing finished in 11 secs.\n",
      "Throughput: 24.62 requests/s, 6302.27 tokens/s\n",
      "\n",
      "Finished benchmark with num_prompt=512\n",
      "\n",
      "All benchmarks completed.\n"
     ]
    }
   ],
   "source": [
    "# 80 tokens\n",
    "import subprocess\n",
    "\n",
    "num_prompts = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "base_command = \"VLLM_USE_TRITON_FLASH_ATTN=0 python benchmark_throughput.py --backend vllm --model mistralai/Mixtral-8x7B-Instruct-v0.1 --input-len=128 --output-len=128 --num-prompts={num_prompt}\"\n",
    "\n",
    "\n",
    "for num_prompt in num_prompts:\n",
    "    command = base_command.format(num_prompt=num_prompt)\n",
    "    print(f\"Running benchmark with num_prompt={num_prompt}\")\n",
    "    \n",
    "    # Run the command and capture output\n",
    "    result = subprocess.run(command, shell=True, text=True, capture_output=True)\n",
    "    \n",
    "    # Print stdout and stderr\n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    # print(\"STDERR:\")\n",
    "    # print(result.stderr)\n",
    "    \n",
    "    print(f\"Finished benchmark with num_prompt={num_prompt}\\n\")\n",
    "\n",
    "print(\"All benchmarks completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dfa2b-56d7-4e89-8e04-7e8b997fe5b1",
   "metadata": {},
   "source": [
    "## Parse results\n",
    "Now that we've gotten our results, here's a whole mess of code to both get that into a data frame, and also to pull the raw results from dstack's testing into data frames as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb090840-0881-43f4-b0fb-410e859c3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark parsing code\n",
    "\n",
    "from IPython.display import display\n",
    "import nbformat\n",
    "import ipynbname\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_benchmark_results(text):\n",
    "    \"\"\"\n",
    "    Parse the benchmark results from text into a structured DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text containing benchmark results.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the parsed benchmark results.\n",
    "    \"\"\"\n",
    "    # Define the metrics to extract with their regex patterns\n",
    "    metrics_patterns = {\n",
    "        'num_prompt': r\"Running benchmark with num_prompt=(\\d+)\",\n",
    "        'throughput': r\"Throughput:\\s*([\\d.]+)\\s*requests/s,\\s*([\\d.]+)\\s*tokens/s\"\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    current_result = {}\n",
    "    current_prompt = None\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        # Check for num_prompt\n",
    "        num_prompt_match = re.search(metrics_patterns['num_prompt'], line)\n",
    "        if num_prompt_match:\n",
    "            current_prompt = int(num_prompt_match.group(1))\n",
    "            current_result = {'num_prompt': current_prompt}\n",
    "            continue  # Move to the next line\n",
    "\n",
    "        # Extract throughput\n",
    "        throughput_match = re.search(metrics_patterns['throughput'], line)\n",
    "        if throughput_match:\n",
    "            current_result['request_throughput_req_s'] = float(throughput_match.group(1))\n",
    "            current_result['output_token_throughput_tok_s'] = float(throughput_match.group(2))\n",
    "            results.append(current_result)\n",
    "            current_prompt = None\n",
    "            current_result = {}\n",
    "\n",
    "    # Convert the list of results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_cell_output(cell_output):\n",
    "    \"\"\"\n",
    "    Parse the output of a code cell containing benchmark results.\n",
    "\n",
    "    Parameters:\n",
    "    - cell_output (str): The output text from the code cell.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the parsed benchmark results.\n",
    "    \"\"\"\n",
    "    return parse_benchmark_results(cell_output)\n",
    "\n",
    "def parse_markdown_cell(cell_content):\n",
    "    \"\"\"\n",
    "    Parse the content of a cell (e.g., Markdown) containing benchmark results.\n",
    "\n",
    "    Parameters:\n",
    "    - cell_content (str): The content text from the cell.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the parsed benchmark results.\n",
    "    \"\"\"\n",
    "    return parse_cell_content(cell_content)\n",
    "\n",
    "def parse_cell_content(cell_content):\n",
    "    \"\"\"\n",
    "    Parse the cell content to extract benchmark results into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - cell_content (str): The content of the cell to parse.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the parsed benchmark results.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract num_prompts\n",
    "    num_prompts_pattern = r\"num_prompts:\\s*([\\d,\\s]+)\"\n",
    "    num_prompts_match = re.search(num_prompts_pattern, cell_content, re.IGNORECASE)\n",
    "    \n",
    "    if not num_prompts_match:\n",
    "        print(\"No 'num_prompts' found in cell content.\")\n",
    "        return None\n",
    "    \n",
    "    num_prompts_str = num_prompts_match.group(1)\n",
    "    num_prompts = [int(x.strip()) for x in num_prompts_str.split(\",\")]\n",
    "    \n",
    "    # Step 2: Split the content into benchmark result blocks\n",
    "    # Adjusted the regex to handle variations in the separator lines\n",
    "    benchmark_block_pattern = r\"=========== Serving Benchmark Result ============\\n(.*?)==================================================\"\n",
    "    blocks = re.findall(benchmark_block_pattern, cell_content, re.DOTALL)\n",
    "    \n",
    "    if not blocks:\n",
    "        print(\"No benchmark result blocks found in cell content.\")\n",
    "        return None\n",
    "    \n",
    "    # Check if the number of blocks matches the number of num_prompts\n",
    "    if len(blocks) != len(num_prompts):\n",
    "        print(\"Warning: Number of benchmark result blocks does not match number of num_prompts.\")\n",
    "        print(f\"Number of blocks: {len(blocks)}, Number of num_prompts: {len(num_prompts)}\")\n",
    "        # Proceeding, but mapping may be incorrect\n",
    "    \n",
    "    # Define all the metrics to extract\n",
    "    metrics_patterns = {\n",
    "        'successful_requests': r\"Successful requests:\\s+(\\d+)\",\n",
    "        'benchmark_duration_s': r\"Benchmark duration \\(s\\):\\s+([\\d.]+)\",\n",
    "        'total_input_tokens': r\"Total input tokens:\\s+([\\d.]+)\",\n",
    "        'total_generated_tokens': r\"Total generated tokens:\\s+([\\d.]+)\",\n",
    "        'request_throughput_req_s': r\"Request throughput \\(req/s\\):\\s+([\\d.]+)\",\n",
    "        'output_token_throughput_tok_s': r\"Output token throughput \\(tok/s\\):\\s+([\\d.]+)\",\n",
    "        'total_token_throughput_tok_s': r\"Total Token throughput \\(tok/s\\):\\s+([\\d.]+)\",\n",
    "        'mean_ttft_ms': r\"Mean TTFT \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'median_ttft_ms': r\"Median TTFT \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'p99_ttft_ms': r\"P99 TTFT \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'mean_tpot_ms': r\"Mean TPOT \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'median_tpot_ms': r\"Median TPOT \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'p99_tpot_ms': r\"P99 TPOT \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'mean_itl_ms': r\"Mean ITL \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'median_itl_ms': r\"Median ITL \\(ms\\):\\s+([\\d.]+)\",\n",
    "        'p99_itl_ms': r\"P99 ITL \\(ms\\):\\s+([\\d.]+)\"\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, block in enumerate(blocks):\n",
    "        # Initialize a dictionary for each result\n",
    "        result = {}\n",
    "        result['num_prompt'] = num_prompts[idx] if idx < len(num_prompts) else None\n",
    "        \n",
    "        # Extract each metric using its regex pattern\n",
    "        for metric, pattern in metrics_patterns.items():\n",
    "            match = re.search(pattern, block, re.IGNORECASE)\n",
    "            if match:\n",
    "                # Convert numerical values to float or int as appropriate\n",
    "                value_str = match.group(1)\n",
    "                if '.' in value_str:\n",
    "                    value = float(value_str)\n",
    "                else:\n",
    "                    value = int(value_str)\n",
    "                result[metric] = value\n",
    "            else:\n",
    "                result[metric] = None  # Assign None if the metric is not found\n",
    "        \n",
    "        # Append the result to the list\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb1cfb3d-4639-4fb4-8af0-8111f417e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from cells...\n",
    "\n",
    "import nbformat\n",
    "import ipynbname\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_notebook_cells():\n",
    "    try:\n",
    "        notebook_path = ipynbname.path()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not determine the notebook's path.\")\n",
    "        return []\n",
    "    \n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    return notebook.cells\n",
    "\n",
    "def get_cell_content_by_position(position):\n",
    "    \"\"\"Get the content of a cell by its position in the notebook (0-based index).\"\"\"\n",
    "    cells = get_notebook_cells()\n",
    "    if position < len(cells):\n",
    "        cell = cells[position]\n",
    "        return cell.source\n",
    "    else:\n",
    "        print(f\"No cell found at position {position}.\")\n",
    "        return None\n",
    "\n",
    "def get_cell_outputs_by_position(position):\n",
    "    \"\"\"Get the outputs of a code cell by its position in the notebook (0-based index).\"\"\"\n",
    "    cells = get_notebook_cells()\n",
    "    if position < len(cells):\n",
    "        cell = cells[position]\n",
    "        if cell.cell_type == 'code':\n",
    "            outputs = cell.get('outputs', [])\n",
    "            # Concatenate all output texts\n",
    "            output_texts = []\n",
    "            for output in outputs:\n",
    "                if 'text' in output:\n",
    "                    output_texts.append(''.join(output['text']))\n",
    "                elif 'data' in output and 'text/plain' in output['data']:\n",
    "                    output_texts.append(''.join(output['data']['text/plain']))\n",
    "                elif 'ename' in output and 'evalue' in output:\n",
    "                    # Capture error messages\n",
    "                    output_texts.append(f\"{output['ename']}: {output['evalue']}\")\n",
    "            return '\\n'.join(output_texts)\n",
    "        else:\n",
    "            print(f\"Cell at position {position} is not a code cell.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No cell found at position {position}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6be4d62-2b7a-42c4-96a7-8f29fdc550a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_prompt</th>\n",
       "      <th>request_throughput_req_s</th>\n",
       "      <th>output_token_throughput_tok_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>125.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.68</td>\n",
       "      <td>173.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1.19</td>\n",
       "      <td>303.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>3.07</td>\n",
       "      <td>785.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>4.74</td>\n",
       "      <td>1212.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>10.27</td>\n",
       "      <td>2628.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>16.59</td>\n",
       "      <td>4246.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>25.17</td>\n",
       "      <td>6444.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>27.10</td>\n",
       "      <td>6936.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>512</td>\n",
       "      <td>29.30</td>\n",
       "      <td>7501.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1024</td>\n",
       "      <td>31.02</td>\n",
       "      <td>7942.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_prompt  request_throughput_req_s  output_token_throughput_tok_s\n",
       "0            1                      0.49                         125.48\n",
       "1            2                      0.68                         173.65\n",
       "2            4                      1.19                         303.74\n",
       "3            8                      3.07                         785.25\n",
       "4           16                      4.74                        1212.26\n",
       "5           32                     10.27                        2628.69\n",
       "6           64                     16.59                        4246.45\n",
       "7          128                     25.17                        6444.18\n",
       "8          256                     27.10                        6936.75\n",
       "9          512                     29.30                        7501.97\n",
       "10        1024                     31.02                        7942.34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_prompt</th>\n",
       "      <th>request_throughput_req_s</th>\n",
       "      <th>output_token_throughput_tok_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.47</td>\n",
       "      <td>119.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.62</td>\n",
       "      <td>157.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1.04</td>\n",
       "      <td>265.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2.02</td>\n",
       "      <td>516.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>3.19</td>\n",
       "      <td>817.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1696.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>12.45</td>\n",
       "      <td>3188.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>18.30</td>\n",
       "      <td>4684.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>22.61</td>\n",
       "      <td>5789.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>512</td>\n",
       "      <td>24.62</td>\n",
       "      <td>6302.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_prompt  request_throughput_req_s  output_token_throughput_tok_s\n",
       "0           1                      0.47                         119.40\n",
       "1           2                      0.62                         157.93\n",
       "2           4                      1.04                         265.47\n",
       "3           8                      2.02                         516.66\n",
       "4          16                      3.19                         817.24\n",
       "5          32                      6.63                        1696.81\n",
       "6          64                     12.45                        3188.45\n",
       "7         128                     18.30                        4684.59\n",
       "8         256                     22.61                        5789.11\n",
       "9         512                     24.62                        6302.27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### My results from the output\n",
    "\n",
    "# Get output\n",
    "cell_position = 4  # 0-based index (e.g., 4 corresponds to the 5th cell)\n",
    "\n",
    "# Retrieve cell content\n",
    "cell_content = get_cell_content_by_position(cell_position)\n",
    "\n",
    "# Retrieve cell output (only for code cells)\n",
    "cell_output = get_cell_outputs_by_position(cell_position)\n",
    "\n",
    "# Parsing the output if available\n",
    "if cell_output:\n",
    "    df_myvllm_tp2 = parse_cell_output(cell_output)\n",
    "    print(\"TP2\")\n",
    "    display(df_myvllm_tp2)\n",
    "\n",
    "\n",
    "# Get output\n",
    "cell_position = 5  # 0-based index (e.g., 4 corresponds to the 5th cell)\n",
    "\n",
    "# Retrieve cell content\n",
    "cell_content = get_cell_content_by_position(cell_position)\n",
    "\n",
    "# Retrieve cell output (only for code cells)\n",
    "cell_output = get_cell_outputs_by_position(cell_position)\n",
    "\n",
    "if cell_output:\n",
    "    print(\"TP1\")\n",
    "    df_myvllm_tp1 = parse_cell_output(cell_output)\n",
    "    display(df_myvllm_tp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "159def82-6b7a-4b9b-889b-0f0f2537e6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_prompt</th>\n",
       "      <th>tp2_tok_s</th>\n",
       "      <th>2x_tp1_tok_s</th>\n",
       "      <th>percent_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>125.48</td>\n",
       "      <td>119.40</td>\n",
       "      <td>-4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>173.65</td>\n",
       "      <td>238.80</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>303.74</td>\n",
       "      <td>315.86</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>785.25</td>\n",
       "      <td>530.94</td>\n",
       "      <td>-32.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1212.26</td>\n",
       "      <td>1033.32</td>\n",
       "      <td>-14.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>2628.69</td>\n",
       "      <td>1634.48</td>\n",
       "      <td>-37.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>4246.45</td>\n",
       "      <td>3393.62</td>\n",
       "      <td>-20.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>6444.18</td>\n",
       "      <td>6376.90</td>\n",
       "      <td>-1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>6936.75</td>\n",
       "      <td>9369.18</td>\n",
       "      <td>35.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>512</td>\n",
       "      <td>7501.97</td>\n",
       "      <td>11578.22</td>\n",
       "      <td>54.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1024</td>\n",
       "      <td>7942.34</td>\n",
       "      <td>12604.54</td>\n",
       "      <td>58.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_prompt  tp2_tok_s  2x_tp1_tok_s  percent_diff\n",
       "0            1     125.48        119.40         -4.85\n",
       "1            2     173.65        238.80         37.52\n",
       "2            4     303.74        315.86          3.99\n",
       "3            8     785.25        530.94        -32.39\n",
       "4           16    1212.26       1033.32        -14.76\n",
       "5           32    2628.69       1634.48        -37.82\n",
       "6           64    4246.45       3393.62        -20.08\n",
       "7          128    6444.18       6376.90         -1.04\n",
       "8          256    6936.75       9369.18         35.07\n",
       "9          512    7501.97      11578.22         54.34\n",
       "10        1024    7942.34      12604.54         58.70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_myvllm_tp2 and df_myvllm_tp1 are already defined as per your data\n",
    "\n",
    "# Step 1: Create a copy of df_myvllm_tp1\n",
    "df_tp1_scaled = df_myvllm_tp1.copy()\n",
    "\n",
    "# Step 2: Initialize '2x_tp1_tok_s' with the 'output_token_throughput_tok_s' column\n",
    "df_tp1_scaled['2x_tp1_tok_s'] = df_tp1_scaled['output_token_throughput_tok_s'].copy()\n",
    "\n",
    "# Step 3: Compute '2x_tp1_tok_s' by doubling the previous 'output_token_throughput_tok_s' value\n",
    "for i in range(1, len(df_tp1_scaled)):\n",
    "    df_tp1_scaled.loc[i, '2x_tp1_tok_s'] = df_tp1_scaled.loc[i-1, 'output_token_throughput_tok_s'] * 2\n",
    "\n",
    "# Step 4: Append a new row with num_prompt=1024 and doubled last 'output_token_throughput_tok_s'\n",
    "new_num_prompt = 1024\n",
    "new_2x_tp1_tok_s = df_tp1_scaled['output_token_throughput_tok_s'].iloc[-1] * 2\n",
    "\n",
    "new_row = pd.DataFrame({\n",
    "    'num_prompt': [new_num_prompt],\n",
    "    'request_throughput_req_s': [np.nan],\n",
    "    'output_token_throughput_tok_s': [np.nan],\n",
    "    '2x_tp1_tok_s': [new_2x_tp1_tok_s]\n",
    "})\n",
    "\n",
    "df_tp1_scaled = pd.concat([df_tp1_scaled, new_row], ignore_index=True)\n",
    "\n",
    "# display(df_tp1_scaled)\n",
    "\n",
    "# Rename the columns for clarity\n",
    "df_myvllm_tp2.rename(columns={'output_token_throughput_tok_s': 'tp2_tok_s'}, inplace=True)\n",
    "\n",
    "\n",
    "# Step 5: Merge the two DataFrames on 'num_prompt'\n",
    "df = pd.merge(\n",
    "    df_myvllm_tp2[['num_prompt', 'tp2_tok_s']],\n",
    "    df_tp1_scaled[['num_prompt', '2x_tp1_tok_s']],\n",
    "    on='num_prompt',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 6: Calculate 'percent_diff'\n",
    "df['percent_diff'] = ((df['2x_tp1_tok_s'] - df['tp2_tok_s']) / df['tp2_tok_s']) * 100\n",
    "\n",
    "# Round 'percent_diff' to two decimal places\n",
    "df['percent_diff'] = df['percent_diff'].round(2)\n",
    "\n",
    "# Display the final DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e7737fb-e2a1-4ecb-b92f-6947a3c56bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Runpod_2GPU_tok_s</th>\n",
       "      <th>My_2GPU_tok_s</th>\n",
       "      <th>percent_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>122.25</td>\n",
       "      <td>119.40</td>\n",
       "      <td>-2.331288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>244.50</td>\n",
       "      <td>238.80</td>\n",
       "      <td>-2.331288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>376.96</td>\n",
       "      <td>315.86</td>\n",
       "      <td>-16.208616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>550.64</td>\n",
       "      <td>530.94</td>\n",
       "      <td>-3.577655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1078.04</td>\n",
       "      <td>1033.32</td>\n",
       "      <td>-4.148269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>1756.56</td>\n",
       "      <td>1634.48</td>\n",
       "      <td>-6.949948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>3236.30</td>\n",
       "      <td>3393.62</td>\n",
       "      <td>4.861107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>5043.48</td>\n",
       "      <td>6376.90</td>\n",
       "      <td>26.438491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>7207.96</td>\n",
       "      <td>9369.18</td>\n",
       "      <td>29.983796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>512</td>\n",
       "      <td>7989.14</td>\n",
       "      <td>11578.22</td>\n",
       "      <td>44.924485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1024</td>\n",
       "      <td>8801.66</td>\n",
       "      <td>12604.54</td>\n",
       "      <td>43.206395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Batch_size  Runpod_2GPU_tok_s  My_2GPU_tok_s  percent_diff\n",
       "0            1             122.25         119.40     -2.331288\n",
       "1            2             244.50         238.80     -2.331288\n",
       "2            4             376.96         315.86    -16.208616\n",
       "3            8             550.64         530.94     -3.577655\n",
       "4           16            1078.04        1033.32     -4.148269\n",
       "5           32            1756.56        1634.48     -6.949948\n",
       "6           64            3236.30        3393.62      4.861107\n",
       "7          128            5043.48        6376.90     26.438491\n",
       "8          256            7207.96        9369.18     29.983796\n",
       "9          512            7989.14       11578.22     44.924485\n",
       "10        1024            8801.66       12604.54     43.206395"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Runpod Results\n",
    "data = {'Batch_size': [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024], 'Runpod_2GPU_tok_s': [122.25, 244.5, 376.96, 550.64, 1078.04, 1756.56, 3236.3, 5043.48, 7207.96, 7989.14, 8801.66]}\n",
    "df_runpod = pd.DataFrame(data)\n",
    "\n",
    "df_runpod['My_2GPU_tok_s'] = df['2x_tp1_tok_s']\n",
    "df_runpod['percent_diff'] = (df_runpod['My_2GPU_tok_s'] - df_runpod['Runpod_2GPU_tok_s']) / df_runpod['Runpod_2GPU_tok_s'] * 100\n",
    "\n",
    "display(df_runpod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72f691e6-410d-4cb7-9c7c-8644473c1f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>MK1_2GPU_tok_s</th>\n",
       "      <th>My_2GPU_tok_s</th>\n",
       "      <th>percent_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>119.40</td>\n",
       "      <td>-15.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>238.80</td>\n",
       "      <td>-14.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>466</td>\n",
       "      <td>315.86</td>\n",
       "      <td>-32.218884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>804</td>\n",
       "      <td>530.94</td>\n",
       "      <td>-33.962687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1452</td>\n",
       "      <td>1033.32</td>\n",
       "      <td>-28.834711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>2662</td>\n",
       "      <td>1634.48</td>\n",
       "      <td>-38.599549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>4442</td>\n",
       "      <td>3393.62</td>\n",
       "      <td>-23.601531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>6348</td>\n",
       "      <td>6376.90</td>\n",
       "      <td>0.455261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>6292</td>\n",
       "      <td>9369.18</td>\n",
       "      <td>48.906230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>512</td>\n",
       "      <td>6292</td>\n",
       "      <td>11578.22</td>\n",
       "      <td>84.014940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1024</td>\n",
       "      <td>6288</td>\n",
       "      <td>12604.54</td>\n",
       "      <td>100.453880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Batch_size  MK1_2GPU_tok_s  My_2GPU_tok_s  percent_diff\n",
       "0            1             142         119.40    -15.915493\n",
       "1            2             280         238.80    -14.714286\n",
       "2            4             466         315.86    -32.218884\n",
       "3            8             804         530.94    -33.962687\n",
       "4           16            1452        1033.32    -28.834711\n",
       "5           32            2662        1634.48    -38.599549\n",
       "6           64            4442        3393.62    -23.601531\n",
       "7          128            6348        6376.90      0.455261\n",
       "8          256            6292        9369.18     48.906230\n",
       "9          512            6292       11578.22     84.014940\n",
       "10        1024            6288       12604.54    100.453880"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorwave Results\n",
    "data = {'Batch_size': [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024], 'MK1_2GPU_tok_s': [142,280,466,804,1452,2662,4442,6348,6292,6292,6288]}\n",
    "df_mk1 = pd.DataFrame(data)\n",
    "\n",
    "df_mk1['My_2GPU_tok_s'] = df['2x_tp1_tok_s']\n",
    "\n",
    "df_mk1['percent_diff'] = (df_mk1['My_2GPU_tok_s'] - df_mk1['MK1_2GPU_tok_s']) / df_mk1['MK1_2GPU_tok_s'] * 100\n",
    "\n",
    "display(df_mk1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vllm]",
   "language": "python",
   "name": "conda-env-vllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
