{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2823cbc5-5a57-438f-b5c5-85f269609cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmamba create -n torchtune python=3.12\\nmamba activate torchtune\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2\\npip install ipykernel nbformat\\npython -m ipykernel install --user --name=torchtune\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get the kernel working...\n",
    "\"\"\"\n",
    "mamba create -n torchtune python=3.12\n",
    "mamba activate torchtune\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2\n",
    "pip install torchao\n",
    "pip install torchtune\n",
    "pip install ipykernel nbformat\n",
    "python -m ipykernel install --user --name=torchtune\n",
    "\"\"\"\n",
    "# Select in the top right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc08abf-4ac3-4965-a1df-06601c427711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: none\n",
      "Fetching 17 files: 100%|█████████████████████| 17/17 [00:00<00:00, 23045.63it/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/hf_model_0001_0.pt\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/model-00004-of-00004.safetensors\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/model-00001-of-00004.safetensors\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/model-00002-of-00004.safetensors\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/special_tokens_map.json\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/original\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/tokenizer.json\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/config.json\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/model-00003-of-00004.safetensors\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/generation_config.json\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/USE_POLICY.md\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/hf_model_0002_0.pt\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/LICENSE\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/.cache\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/tokenizer_config.json\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/.gitattributes\n",
      "/mnt/nvme2n1p1/Meta-Llama-3.1-8B-Instruct/README.md\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/torchtune?tab=readme-ov-file#get-started\n",
    "!tune download meta-llama/Meta-Llama-3.1-8B-Instruct --ignore-patterns none --output-dir /tune/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e53bfb7-041a-43d9-a12d-23ce65924fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         phi3/mini_full                          \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         phi3/mini_lora                          \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/generation_v2           \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/evaluation              \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "knowledge_distillation_single_device     qwen2/knowledge_distillation_single_device\n",
      "                                         llama3_2/knowledge_distillation_single_device\n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a0d7b-45a8-4d34-b873-8636ccf6592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tune/Meta-Llama-3.1-8B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /tune/Meta-Llama-3.1-8B-Instruct/\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "epochs: 3\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: false\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tune/full-llama3.1-finetune\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.llama3_1_8b\n",
      "optimizer:\n",
      "  _component_: bitsandbytes.optim.PagedAdamW8bit\n",
      "  lr: 2.0e-05\n",
      "optimizer_in_bwd: true\n",
      "output_dir: /tune/full-llama3.1-finetune\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 1\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tune/full-llama3.1-finetune/profiling_outputs\n",
      "  profile_memory: true\n",
      "  record_shapes: true\n",
      "  wait_steps: 1\n",
      "  warmup_steps: 2\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /tune/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 1391141744. Local seed is seed + rank = 1391141744 + 0\n",
      "Writing logs to /tune/full-llama3.1-finetune/log_1729471053.txt\n",
      "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.02 GiB\n",
      "\tGPU peak memory reserved: 15.14 GiB\n",
      "\tGPU peak memory active: 15.02 GiB\n",
      "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "INFO:torchtune.utils._logging:In-backward optimizers are set up.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|3829|Loss: 0.854850172996521:  15%|▎ | 3829/26001 [1:18:40<7:57:24,  1.29s/it]"
     ]
    }
   ],
   "source": [
    "!time tune run full_finetune_single_device --config tt-llama3_1/8B_full_single_device.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12acbf-f21f-4bcf-8dbd-b899ec5d0f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchtune]",
   "language": "python",
   "name": "conda-env-torchtune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
